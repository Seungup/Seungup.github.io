{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_url = 'https://storage.googleapis.com/kaggle-competitions-data/kaggle/3004/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1528608739&Signature=KYTkqJ%2B2XjGrrc7%2FglFccTd6LLAyKniT1UISKjowXHRUWpTThU42i5ZL%2BFfiYcDPrQRSZHS8bB5nz86iOcax%2BkS97E1GGfZkRgQn0nC6EnotbqXEgwHBrMxFR9bQfk1i%2FXghulKH%2BuJdqJMXFhocuP2usO7h%2B5D8gzl14rZvOiBminBYwN9JTRTxmN6ugxfN14jm5M%2B%2FZk%2BL4odOiD%2BZ%2BmmmNBfPw%2F9%2FKa6UJLSRWKZA4SrRw0enNxahmAKbgvRV0Kc7YOMIzp0glckFGi%2Fhe2EkPIOibA0EajIKfrnU%2FQrhS8RJy3tHkjfBogy8mNOPf%2BJkrZCp3O3OK56UnmTFYQ%3D%3D'\n",
    "test_url = 'https://storage.googleapis.com/kaggle-competitions-data/kaggle/3004/test.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1528608707&Signature=swcilvB6CABwyekMsW0oc84NSIy0TGarWrObvLfmpwOPi%2FkFsbeih9AP%2FNbBzDXw6ltx615WYUt%2FZhq8qdovyIN6amKz%2BLbb3Ky2NrIJ5ys6uf2Hu7Qe4mBGV3Q4iDaZ1zsW9E8QCQDWaaabFSl8erMCdHe56PLXLx8CgRWBNbnIOMtUDk0t6lwiXYKFisLyo8KSIyJAsxNF%2ByZXquI%2FYuBXCxFjIzAH2OHoHHt3S1ZK4qSBmx9PhpKA%2B6HH%2BckShvLbGLp%2FHaJ%2FNPWZNYEr%2FHeAQp%2BbCHC5P9XQSB%2Fs7uI%2BzcbO4BiEdU%2Bn93daVlqq2uE%2BWZaTVVPbbFAF06lSPw%3D%3D'\n",
    "\n",
    "train_df = pd.read_csv(train_url) \n",
    "test_df = pd.read_csv(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [10, 10]\n",
      "train accuracy: 90.22%, test accuracy: 87.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "hidden_layer_size: [10, 20, 10]\n",
      "train accuracy: 65.79%, test accuracy: 64.71%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 80, 10]\n",
      "train accuracy: 77.44%, test accuracy: 76.98%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 40, 80, 40, 20, 10]\n",
      "train accuracy: 93.27%, test accuracy: 90.69%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 40, 80, 160, 80, 40, 20, 10]\n",
      "train accuracy: 95.58%, test accuracy: 93.10%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 960, 240, 80, 10]\n"
     ]
    }
   ],
   "source": [
    "def print_accuracy(clf):\n",
    "    X = train_df[train_df.columns[1:]]\n",
    "    Y = train_df['label']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\n",
    "    train_sample_size = len(x_train)\n",
    "    x_train = x_train[x_train.columns][:train_sample_size]\n",
    "    y_train = y_train[:train_sample_size]\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"train accuracy: {:.2%}, test accuracy: {:.2%}\".format(\n",
    "        accuracy_score(y_train, clf.predict(x_train)),\n",
    "        accuracy_score(y_test, clf.predict(x_test))\n",
    "    ))\n",
    "hidden_layer_size_lst = [\n",
    "    [10, 10], \n",
    "    [10, 20, 10],\n",
    "    [10, 80, 240, 80, 10],\n",
    "    [10, 20, 40, 80, 40, 20, 10],\n",
    "    [10, 20, 40, 80, 160, 80, 40, 20, 10],\n",
    "    [10, 80, 240, 960, 240, 80, 10],\n",
    "    [10, 80, 240, 480, 960, 480, 240, 80, 10],\n",
    "]\n",
    "for h_l_s in hidden_layer_size_lst:\n",
    "    print(\"hidden_layer_size: {}\".format(h_l_s))\n",
    "    print_accuracy( MLPClassifier(hidden_layer_sizes=h_l_s, activation='relu', solver='adam') )\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11s - loss: 0.5017 - categorical_accuracy: 0.8496\n",
      "Epoch 2/20\n",
      "10s - loss: 0.1454 - categorical_accuracy: 0.9573\n",
      "Epoch 3/20\n",
      "9s - loss: 0.0880 - categorical_accuracy: 0.9742\n",
      "Epoch 4/20\n",
      "9s - loss: 0.0604 - categorical_accuracy: 0.9822\n",
      "Epoch 5/20\n",
      "9s - loss: 0.0405 - categorical_accuracy: 0.9873\n",
      "Epoch 6/20\n",
      "9s - loss: 0.0294 - categorical_accuracy: 0.9910\n",
      "Epoch 7/20\n",
      "9s - loss: 0.0240 - categorical_accuracy: 0.9920\n",
      "Epoch 8/20\n",
      "9s - loss: 0.0163 - categorical_accuracy: 0.9949\n",
      "Epoch 9/20\n",
      "9s - loss: 0.0148 - categorical_accuracy: 0.9951\n",
      "Epoch 10/20\n",
      "9s - loss: 0.0161 - categorical_accuracy: 0.9949\n",
      "Epoch 11/20\n",
      "10s - loss: 0.0122 - categorical_accuracy: 0.9959\n",
      "Epoch 12/20\n",
      "11s - loss: 0.0083 - categorical_accuracy: 0.9975\n",
      "Epoch 13/20\n",
      "12s - loss: 0.0069 - categorical_accuracy: 0.9978\n",
      "Epoch 14/20\n",
      "11s - loss: 0.0027 - categorical_accuracy: 0.9992\n",
      "Epoch 15/20\n",
      "12s - loss: 0.0086 - categorical_accuracy: 0.9972\n",
      "Epoch 16/20\n",
      "11s - loss: 0.0170 - categorical_accuracy: 0.9949\n",
      "Epoch 17/20\n",
      "9s - loss: 0.0115 - categorical_accuracy: 0.9961\n",
      "Epoch 18/20\n",
      "11s - loss: 0.0076 - categorical_accuracy: 0.9976\n",
      "Epoch 19/20\n",
      "12s - loss: 0.0094 - categorical_accuracy: 0.9967\n",
      "Epoch 20/20\n",
      "15s - loss: 0.0085 - categorical_accuracy: 0.9974\n",
      "train, loss and metric: [0.0034123742508624369, 0.99895833333333328]\n",
      "test, loss and metric: [0.10712113406642207, 0.97761904739198235]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- keras에서는 Y를 one_hot vector로 바꾸어 사용해야 함\n",
    "- X의 값들이 0과 256 사이에 분포해있으므로, 이를 0과 1.0 사이로 움직임 \n",
    "- train, test set으로 구분하여 진행\n",
    "\"\"\"\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.values.astype(np.float64)/256.0, \n",
    "                                                    Y.values.astype(np.float32), \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import metrics\n",
    "\"\"\"\n",
    "- 원래 데이터가 784이므로, 비슷한 1024로 두고 이를 감소시키는 뉴럴넷 설계\n",
    "- 총 10개로 클래스를 구분하므로, 마지막 Dense는 10개의 노드를 가지고 있어야함\n",
    "- 그리고, 마지막은 softmax\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(1024, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(512),\n",
    "    Activation('relu'),\n",
    "    Dense(256),\n",
    "    Activation('relu'),\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dense(32),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\"\"\"\n",
    "- multi classification이므로 loss는 'categorical_crossentropy'\n",
    "- metric에는 내가 추적할 지표들이 들어감. 최적화는 loss에 따라 되는데, epoch 마다 평가될 지표들이 metric에 들어감. \n",
    "\"\"\"\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              #optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\"\"\"\n",
    "- one epoch = one forward pass and one backward pass of all the training examples\n",
    "- batch size = the number of training examples in one forward/backward pass. \n",
    "\n",
    "- 즉, epoch을 증가한다는 것은 전체 데이터 셋을 몇 번 돌릴 것이냐 를 결정하는 것이고, \n",
    "- batch_size는 backpropagation을 몇 개의 size로 돌릴 것이냐? 를 결정하는 이야기다.\n",
    "\"\"\"\n",
    "train_history = model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=2)\n",
    "train_history = train_history.history # epoch마다 변화한 loss, metric\n",
    "\n",
    "# 아래 세 줄은 필요없는 코드인데, 그래도 이후에 사용될 수 있어서 일단 넣어둠. \n",
    "y_predict = model.predict_classes(x_train, verbose=0)\n",
    "y_true = [ np.argmax(y) for y in y_train]\n",
    "accuracy = np.sum([y_comp[0]==y_comp[1] for y_comp in zip(y_predict, y_true)])\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc69820c79a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mY_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n\u001b[0m\u001b[1;32m     13\u001b[0m                                                     test_size = 0.2, random_state=42)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- keras에서는 Y를 one_hot vector로 바꾸어 사용해야 함\n",
    "- X의 값들이 0과 256 사이에 분포해있으므로, 이를 0과 1.0 사이로 움직임 \n",
    "- train, test set으로 구분하여 진행\n",
    "\"\"\"\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(64, (5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 504s - loss: 0.5980 - categorical_accuracy: 0.8100   \n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 481s - loss: 0.0938 - categorical_accuracy: 0.9699   \n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 462s - loss: 0.0650 - categorical_accuracy: 0.9795   \n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 478s - loss: 0.0479 - categorical_accuracy: 0.9854   \n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 462s - loss: 0.0354 - categorical_accuracy: 0.9884   \n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 438s - loss: 0.0271 - categorical_accuracy: 0.9913   \n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 416s - loss: 0.0263 - categorical_accuracy: 0.9912   \n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 442s - loss: 0.0209 - categorical_accuracy: 0.9929   \n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 474s - loss: 0.0223 - categorical_accuracy: 0.9922   \n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 521s - loss: 0.0130 - categorical_accuracy: 0.9958   \n",
      "train, loss and metric: [0.010848363778620427, 0.99648809523809523]\n",
      "test, loss and metric: [0.042763131727420148, 0.98869047630400886]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33600/33600 [==============================] - 526s - loss: 0.5985 - categorical_accuracy: 0.8046   \n",
      "Epoch 2/20\n",
      "33600/33600 [==============================] - 482s - loss: 0.0885 - categorical_accuracy: 0.9729   \n",
      "Epoch 3/20\n",
      "33600/33600 [==============================] - 10337s - loss: 0.0556 - categorical_accuracy: 0.9826  \n",
      "Epoch 4/20\n",
      "33600/33600 [==============================] - 2295s - loss: 0.0458 - categorical_accuracy: 0.9856  \n",
      "Epoch 5/20\n",
      "33600/33600 [==============================] - 396s - loss: 0.0366 - categorical_accuracy: 0.9883   \n",
      "Epoch 6/20\n",
      "33600/33600 [==============================] - 384s - loss: 0.0277 - categorical_accuracy: 0.9914   \n",
      "Epoch 7/20\n",
      "33600/33600 [==============================] - 383s - loss: 0.0215 - categorical_accuracy: 0.9934   \n",
      "Epoch 8/20\n",
      "33600/33600 [==============================] - 379s - loss: 0.0216 - categorical_accuracy: 0.9925   \n",
      "Epoch 9/20\n",
      "33600/33600 [==============================] - 363s - loss: 0.0192 - categorical_accuracy: 0.9940   \n",
      "Epoch 10/20\n",
      "33600/33600 [==============================] - 371s - loss: 0.0147 - categorical_accuracy: 0.9947   \n",
      "Epoch 11/20\n",
      "33600/33600 [==============================] - 370s - loss: 0.0129 - categorical_accuracy: 0.9964   \n",
      "Epoch 12/20\n",
      "33600/33600 [==============================] - 377s - loss: 0.0110 - categorical_accuracy: 0.9965   \n",
      "Epoch 13/20\n",
      "33600/33600 [==============================] - 416s - loss: 0.0126 - categorical_accuracy: 0.9963   \n",
      "Epoch 14/20\n",
      "33600/33600 [==============================] - 391s - loss: 0.0080 - categorical_accuracy: 0.9976   \n",
      "Epoch 15/20\n",
      "33600/33600 [==============================] - 384s - loss: 0.0070 - categorical_accuracy: 0.9977   \n",
      "Epoch 16/20\n",
      "33600/33600 [==============================] - 386s - loss: 0.0077 - categorical_accuracy: 0.9974   \n",
      "Epoch 17/20\n",
      "33600/33600 [==============================] - 375s - loss: 0.0080 - categorical_accuracy: 0.9975   \n",
      "Epoch 18/20\n",
      "33600/33600 [==============================] - 387s - loss: 0.0057 - categorical_accuracy: 0.9981   \n",
      "Epoch 19/20\n",
      "33600/33600 [==============================] - 374s - loss: 0.0059 - categorical_accuracy: 0.9980   \n",
      "Epoch 20/20\n",
      "33600/33600 [==============================] - 381s - loss: 0.0065 - categorical_accuracy: 0.9978   \n",
      "train, loss and metric: [0.010528954193425652, 0.9966666666666667]\n",
      "test, loss and metric: [0.047946910524873862, 0.98821428594135108]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33600/33600 [==============================] - 470s - loss: 0.9917 - categorical_accuracy: 0.6965   \n",
      "Epoch 2/20\n",
      "33600/33600 [==============================] - 433s - loss: 0.1283 - categorical_accuracy: 0.9590   \n",
      "Epoch 3/20\n",
      "33600/33600 [==============================] - 423s - loss: 0.0739 - categorical_accuracy: 0.9766   \n",
      "Epoch 4/20\n",
      "33600/33600 [==============================] - 424s - loss: 0.0603 - categorical_accuracy: 0.9813   \n",
      "Epoch 5/20\n",
      "33600/33600 [==============================] - 417s - loss: 0.0408 - categorical_accuracy: 0.9873   \n",
      "Epoch 6/20\n",
      "33600/33600 [==============================] - 421s - loss: 0.0317 - categorical_accuracy: 0.9900   \n",
      "Epoch 7/20\n",
      "33600/33600 [==============================] - 415s - loss: 0.0342 - categorical_accuracy: 0.9884   \n",
      "Epoch 8/20\n",
      "33600/33600 [==============================] - 427s - loss: 0.0281 - categorical_accuracy: 0.9906   \n",
      "Epoch 9/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0219 - categorical_accuracy: 0.9930   \n",
      "Epoch 10/20\n",
      "33600/33600 [==============================] - 407s - loss: 0.0171 - categorical_accuracy: 0.9949   \n",
      "Epoch 11/20\n",
      "33600/33600 [==============================] - 414s - loss: 0.0154 - categorical_accuracy: 0.9950   \n",
      "Epoch 12/20\n",
      "33600/33600 [==============================] - 408s - loss: 0.0155 - categorical_accuracy: 0.9948   \n",
      "Epoch 13/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0168 - categorical_accuracy: 0.9941   \n",
      "Epoch 14/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0177 - categorical_accuracy: 0.9938   \n",
      "Epoch 15/20\n",
      "33600/33600 [==============================] - 409s - loss: 0.0106 - categorical_accuracy: 0.9960   \n",
      "Epoch 16/20\n",
      "33600/33600 [==============================] - 415s - loss: 0.0199 - categorical_accuracy: 0.9936   \n",
      "Epoch 17/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0167 - categorical_accuracy: 0.9948   \n",
      "Epoch 18/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0072 - categorical_accuracy: 0.9976   \n",
      "Epoch 19/20\n",
      "33600/33600 [==============================] - 413s - loss: 0.0183 - categorical_accuracy: 0.9940   \n",
      "Epoch 20/20\n",
      "33600/33600 [==============================] - 408s - loss: 0.0085 - categorical_accuracy: 0.9975   \n",
      "train, loss and metric: [0.0082378579511229576, 0.99711309523809522]\n",
      "test, loss and metric: [0.043590596066787841, 0.98916666689373201]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "33600/33600 [==============================] - 476s - loss: 2.0983 - categorical_accuracy: 0.1969   \n",
      "Epoch 2/40\n",
      "33600/33600 [==============================] - 439s - loss: 0.9060 - categorical_accuracy: 0.6513   \n",
      "Epoch 3/40\n",
      "33600/33600 [==============================] - 444s - loss: 0.3827 - categorical_accuracy: 0.8892   \n",
      "Epoch 4/40\n",
      "33600/33600 [==============================] - 470s - loss: 0.1997 - categorical_accuracy: 0.9456   \n",
      "Epoch 5/40\n",
      "33600/33600 [==============================] - 456s - loss: 0.1300 - categorical_accuracy: 0.9657   \n",
      "Epoch 6/40\n",
      "33600/33600 [==============================] - 436s - loss: 0.1100 - categorical_accuracy: 0.9712   \n",
      "Epoch 7/40\n",
      "33600/33600 [==============================] - 468s - loss: 0.0870 - categorical_accuracy: 0.9782   \n",
      "Epoch 8/40\n",
      "33600/33600 [==============================] - 423s - loss: 0.0764 - categorical_accuracy: 0.9806   \n",
      "Epoch 9/40\n",
      "33600/33600 [==============================] - 444s - loss: 0.0764 - categorical_accuracy: 0.9802   \n",
      "Epoch 10/40\n",
      "33600/33600 [==============================] - 430s - loss: 0.0604 - categorical_accuracy: 0.9842   \n",
      "Epoch 11/40\n",
      "33600/33600 [==============================] - 438s - loss: 0.0561 - categorical_accuracy: 0.9849   \n",
      "Epoch 12/40\n",
      "33600/33600 [==============================] - 436s - loss: 0.0518 - categorical_accuracy: 0.9867   \n",
      "Epoch 13/40\n",
      "33600/33600 [==============================] - 433s - loss: 0.0517 - categorical_accuracy: 0.9869   \n",
      "Epoch 14/40\n",
      "33600/33600 [==============================] - 441s - loss: 0.0477 - categorical_accuracy: 0.9876   \n",
      "Epoch 15/40\n",
      "33600/33600 [==============================] - 429s - loss: 0.0484 - categorical_accuracy: 0.9876   \n",
      "Epoch 16/40\n",
      "33600/33600 [==============================] - 429s - loss: 0.0448 - categorical_accuracy: 0.9884   \n",
      "Epoch 17/40\n",
      "33600/33600 [==============================] - 455s - loss: 0.0433 - categorical_accuracy: 0.9887   \n",
      "Epoch 18/40\n",
      "33600/33600 [==============================] - 432s - loss: 0.0442 - categorical_accuracy: 0.9884   \n",
      "Epoch 19/40\n",
      "33600/33600 [==============================] - 443s - loss: 0.0392 - categorical_accuracy: 0.9903   \n",
      "Epoch 20/40\n",
      "33600/33600 [==============================] - 437s - loss: 0.0381 - categorical_accuracy: 0.9897   \n",
      "Epoch 21/40\n",
      "33600/33600 [==============================] - 442s - loss: 0.0370 - categorical_accuracy: 0.9904   \n",
      "Epoch 22/40\n",
      "33600/33600 [==============================] - 440s - loss: 0.0320 - categorical_accuracy: 0.9919   \n",
      "Epoch 23/40\n",
      "33600/33600 [==============================] - 443s - loss: 0.0362 - categorical_accuracy: 0.9904   \n",
      "Epoch 24/40\n",
      "33600/33600 [==============================] - 424s - loss: 0.0296 - categorical_accuracy: 0.9925   \n",
      "Epoch 25/40\n",
      "33600/33600 [==============================] - 423s - loss: 0.0268 - categorical_accuracy: 0.9929   \n",
      "Epoch 26/40\n",
      "33600/33600 [==============================] - 434s - loss: 0.0275 - categorical_accuracy: 0.9926   \n",
      "Epoch 27/40\n",
      "33600/33600 [==============================] - 423s - loss: 0.0295 - categorical_accuracy: 0.9925   \n",
      "Epoch 28/40\n",
      "33600/33600 [==============================] - 434s - loss: 0.0301 - categorical_accuracy: 0.9920   \n",
      "Epoch 29/40\n",
      "33600/33600 [==============================] - 441s - loss: 0.0344 - categorical_accuracy: 0.9907   \n",
      "Epoch 30/40\n",
      "33600/33600 [==============================] - 441s - loss: 0.0297 - categorical_accuracy: 0.9922   \n",
      "Epoch 31/40\n",
      "33600/33600 [==============================] - 435s - loss: 0.0252 - categorical_accuracy: 0.9933   \n",
      "Epoch 32/40\n",
      "33600/33600 [==============================] - 421s - loss: 0.0273 - categorical_accuracy: 0.9929   \n",
      "Epoch 33/40\n",
      "33600/33600 [==============================] - 424s - loss: 0.0244 - categorical_accuracy: 0.9939   \n",
      "Epoch 34/40\n",
      "33600/33600 [==============================] - 427s - loss: 0.0236 - categorical_accuracy: 0.9939   \n",
      "Epoch 35/40\n",
      "33600/33600 [==============================] - 435s - loss: 0.0257 - categorical_accuracy: 0.9936   \n",
      "Epoch 36/40\n",
      "33600/33600 [==============================] - 432s - loss: 0.0238 - categorical_accuracy: 0.9937   \n",
      "Epoch 37/40\n",
      "33600/33600 [==============================] - 445s - loss: 0.0241 - categorical_accuracy: 0.9938   \n",
      "Epoch 38/40\n",
      "33600/33600 [==============================] - 463s - loss: 0.0263 - categorical_accuracy: 0.9929   \n",
      "Epoch 39/40\n",
      "33600/33600 [==============================] - 448s - loss: 0.0250 - categorical_accuracy: 0.9934   \n",
      "Epoch 40/40\n",
      "33600/33600 [==============================] - 453s - loss: 0.0223 - categorical_accuracy: 0.9941   \n",
      "train, loss and metric: [0.010496536339217398, 0.997172619047619]\n",
      "test, loss and metric: [0.040449898484096462, 0.99154761916115175]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    Dense(128, activation = \"relu\"),\n",
    "    Dense(32, activation = \"relu\"),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=40, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "33600/33600 [==============================] - 463s - loss: 2.2920 - categorical_accuracy: 0.1304   \n",
      "Epoch 2/60\n",
      "33600/33600 [==============================] - 446s - loss: 1.8574 - categorical_accuracy: 0.2776   \n",
      "Epoch 3/60\n",
      "33600/33600 [==============================] - 446s - loss: 0.7728 - categorical_accuracy: 0.7252   \n",
      "Epoch 4/60\n",
      "33600/33600 [==============================] - 431s - loss: 0.3059 - categorical_accuracy: 0.9269   \n",
      "Epoch 5/60\n",
      "33600/33600 [==============================] - 439s - loss: 0.2131 - categorical_accuracy: 0.9551   \n",
      "Epoch 6/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.1614 - categorical_accuracy: 0.9657   \n",
      "Epoch 7/60\n",
      "33600/33600 [==============================] - 438s - loss: 0.1254 - categorical_accuracy: 0.9727   \n",
      "Epoch 8/60\n",
      "33600/33600 [==============================] - 440s - loss: 0.1074 - categorical_accuracy: 0.9790   \n",
      "Epoch 9/60\n",
      "33600/33600 [==============================] - 431s - loss: 0.1039 - categorical_accuracy: 0.9772   \n",
      "Epoch 10/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0926 - categorical_accuracy: 0.9812   \n",
      "Epoch 11/60\n",
      "33600/33600 [==============================] - 430s - loss: 0.0786 - categorical_accuracy: 0.9831   \n",
      "Epoch 12/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0829 - categorical_accuracy: 0.9827   \n",
      "Epoch 13/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0811 - categorical_accuracy: 0.9840   \n",
      "Epoch 14/60\n",
      "33600/33600 [==============================] - 429s - loss: 0.0647 - categorical_accuracy: 0.9871   \n",
      "Epoch 15/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0676 - categorical_accuracy: 0.9864   \n",
      "Epoch 16/60\n",
      "33600/33600 [==============================] - 444s - loss: 0.0622 - categorical_accuracy: 0.9869   \n",
      "Epoch 17/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0713 - categorical_accuracy: 0.9851   \n",
      "Epoch 18/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0545 - categorical_accuracy: 0.9885   \n",
      "Epoch 19/60\n",
      "33600/33600 [==============================] - 431s - loss: 0.0476 - categorical_accuracy: 0.9902   \n",
      "Epoch 20/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0478 - categorical_accuracy: 0.9903   \n",
      "Epoch 21/60\n",
      "33600/33600 [==============================] - 430s - loss: 0.0506 - categorical_accuracy: 0.9894   \n",
      "Epoch 22/60\n",
      "33600/33600 [==============================] - 431s - loss: 0.0474 - categorical_accuracy: 0.9907   \n",
      "Epoch 23/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0449 - categorical_accuracy: 0.9907   \n",
      "Epoch 24/60\n",
      "33600/33600 [==============================] - 435s - loss: 0.0483 - categorical_accuracy: 0.9896   \n",
      "Epoch 25/60\n",
      "33600/33600 [==============================] - 444s - loss: 0.0360 - categorical_accuracy: 0.9924   \n",
      "Epoch 26/60\n",
      "33600/33600 [==============================] - 442s - loss: 0.0473 - categorical_accuracy: 0.9901   \n",
      "Epoch 27/60\n",
      "33600/33600 [==============================] - 447s - loss: 0.0421 - categorical_accuracy: 0.9910   \n",
      "Epoch 28/60\n",
      "33600/33600 [==============================] - 445s - loss: 0.0382 - categorical_accuracy: 0.9916   \n",
      "Epoch 29/60\n",
      "33600/33600 [==============================] - 460s - loss: 0.0444 - categorical_accuracy: 0.9905   \n",
      "Epoch 30/60\n",
      "33600/33600 [==============================] - 429s - loss: 0.0385 - categorical_accuracy: 0.9914   \n",
      "Epoch 31/60\n",
      "33600/33600 [==============================] - 431s - loss: 0.0334 - categorical_accuracy: 0.9922   \n",
      "Epoch 32/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0356 - categorical_accuracy: 0.9925   \n",
      "Epoch 33/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0344 - categorical_accuracy: 0.9921   \n",
      "Epoch 34/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0511 - categorical_accuracy: 0.9890   \n",
      "Epoch 35/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0332 - categorical_accuracy: 0.9928   \n",
      "Epoch 36/60\n",
      "33600/33600 [==============================] - 431s - loss: 0.0367 - categorical_accuracy: 0.9925   \n",
      "Epoch 37/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0384 - categorical_accuracy: 0.9922   \n",
      "Epoch 38/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0316 - categorical_accuracy: 0.9936   \n",
      "Epoch 39/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0324 - categorical_accuracy: 0.9932   \n",
      "Epoch 40/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0333 - categorical_accuracy: 0.9933   \n",
      "Epoch 41/60\n",
      "33600/33600 [==============================] - 430s - loss: 0.0287 - categorical_accuracy: 0.9940   \n",
      "Epoch 42/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0326 - categorical_accuracy: 0.9936   \n",
      "Epoch 43/60\n",
      "33600/33600 [==============================] - 430s - loss: 0.0283 - categorical_accuracy: 0.9941   \n",
      "Epoch 44/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0285 - categorical_accuracy: 0.9943   \n",
      "Epoch 45/60\n",
      "33600/33600 [==============================] - 429s - loss: 0.0256 - categorical_accuracy: 0.9946   \n",
      "Epoch 46/60\n",
      "33600/33600 [==============================] - 434s - loss: 0.0249 - categorical_accuracy: 0.9950   \n",
      "Epoch 47/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0314 - categorical_accuracy: 0.9936   \n",
      "Epoch 48/60\n",
      "33600/33600 [==============================] - 429s - loss: 0.0278 - categorical_accuracy: 0.9943   \n",
      "Epoch 49/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0278 - categorical_accuracy: 0.9941   \n",
      "Epoch 50/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0258 - categorical_accuracy: 0.9949   \n",
      "Epoch 51/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0316 - categorical_accuracy: 0.9938   \n",
      "Epoch 52/60\n",
      "33600/33600 [==============================] - 428s - loss: 0.0282 - categorical_accuracy: 0.9944   \n",
      "Epoch 53/60\n",
      "33600/33600 [==============================] - 431s - loss: 0.0366 - categorical_accuracy: 0.9930   \n",
      "Epoch 54/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0360 - categorical_accuracy: 0.9928   \n",
      "Epoch 55/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0372 - categorical_accuracy: 0.9923   \n",
      "Epoch 56/60\n",
      "33600/33600 [==============================] - 436s - loss: 0.0336 - categorical_accuracy: 0.9929   \n",
      "Epoch 57/60\n",
      "33600/33600 [==============================] - 434s - loss: 0.0366 - categorical_accuracy: 0.9930   \n",
      "Epoch 58/60\n",
      "33600/33600 [==============================] - 435s - loss: 0.0302 - categorical_accuracy: 0.9938   \n",
      "Epoch 59/60\n",
      "33600/33600 [==============================] - 433s - loss: 0.0234 - categorical_accuracy: 0.9950   \n",
      "Epoch 60/60\n",
      "33600/33600 [==============================] - 432s - loss: 0.0282 - categorical_accuracy: 0.9941   \n",
      "train, loss and metric: [0.005773499012014305, 0.99863095238095234]\n",
      "test, loss and metric: [0.036450345025153923, 0.99321428594135108]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation = \"relu\"),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=60, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 106s   \n"
     ]
    }
   ],
   "source": [
    "test_X_values = (test_df.values.astype(np.float64)/256.0).reshape(len(test_df), 28, 28, 1)\n",
    "test_y_pred = model.predict_classes(test_X_values)\n",
    "\n",
    "submit_df = pd.DataFrame({\"ImageId\":range(1, 1+len(test_y_pred)), \"Label\":test_y_pred})\n",
    "submit_df.to_csv('test_mnist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
