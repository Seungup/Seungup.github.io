---
title: Maximal likelihood estimation. 
category: machine-learning
tags: data machine-learning MLE logistic-regression
---

## MLE: maximal likelihood estimation

- 요즘 MLE를 좀 공부해야 할 필요성이 이어서 정리를 하고 있습니다. 
- 늘 말하지만, 모든 예측 모델은 결국 y = ax +b에서 a, b를 찾는 것뿐이죠. 그리고 뉴럴넷이 등장한 이후, 모든 에측 모델은 'gradient descent algorithm'을 사용해서 이 parameter들을 추정합니다. 즉 점진적으로 값을 업데이트하면서 찾아나가는 방식이죠. 
- 우리는 보통, 모델을 '함수'적으로 생각합니다. 제가 표현한 것처럼, y = ax+b 와 같은 식으로 생각하는 것이 일종의 모델을 함수적으로 생각하는 것이죠. 
- 그런데, 이걸 좀 다르게 생각할 수도 있습니다. y, x가 각각 확률적인 분포에 의해서 결정되는 값이라고 생각할 수 있다는 거죠. 가령, y를 몸무게, x를 키로 가정할 경우, 각각은 특정한 확률 분포를 가집니다. 그리고, x에 따라서, y의 확률 분포도 달라지죠. 예를 들어서, x(키)가 150일 때의 몸무게 분포와, 키가 170일 때의 몸무게 분포는 달라집니다. 즉, P(Y|X)같은 거죠. 
- 그리고, 동시에 우리는 이 확률 분포를 의미하는 모수(paramter)도 알지 못합니다. 만약 키가 normal distribution을 따른다고 합시다. normal distribution을 정의하는 parameter는 평균과 분산이죠. 이 값을 알면, 해당 값의 확률 분포를 모델링할 수 있죠. 다만, 우리는 여기에서, 어떤 평균과 어떤 분산이 가장 적합한지에 대해서는 알지 못합니다. 
- 우리에게 데이터가 있다고 하죠. 사람들의 키 에 대한 데이터가 있고, 우리는 이 데이터가 '정규분포'를 따른다고 '가정'합니다. 그런데, 이제 '평균'과 '분산'이 어떤지 모르죠. 즉, 이제 가장 그럴듯한, 데이터를 잘 설명할 수 있는, '평균', '분산'을 찾아야 합니다. 정규분포에는 식이 있고, 이를 사용해서, 다양한 평균과 분산에 대해서 마치 grid search처럼 막 찾아볼 수도 있겠죠. 그리고 그 결과로 가장 그럴듯한, 지금의 데이터를 만들어낼 수 있는 것으로 가장 유력해 보이는(likely) 놈을 찾습니다. 
- 이런걸 MLE, maximal likelihood estimation, 최대 우도 추정, 최대 가능도 추정이라고들 합니다. 다시 말하지만, '현재 데이터를 가장 잘 설명해주는 모수를 찾아주는 것이죠'. 
- 여기서, 고등학교 수학 때 배웠던 '최대화', '최소화'문제를 기억하시나요. 즉, 우리에게 주어진 likehood 함수가 있구요 여기서, [평균과, 분산에 대해서 각각 알아서 미분을 해주면 바로 likelihood를 최대화하는 평균과 분산을 가져올 수 있습니다](https://daijiang.name/en/2014/10/08/mle-normal-distribution/?source=post_page-----791153818030----------------------).
- 단, 당연하지만, '미분가능'해야죠. 미분이 불가능할 경우에는 이를 점진적으로 업데이트하면서 parameter를 찾는 gradient descent 방법을 쓰게 됩니다.

## Maximum a Posteriori Estimation (MAP)

- [이 블로그에서 지적한 바에 따르면](http://sanghyukchun.github.io/58/), MLE의 경우 모수(쎄타)가 주어졌을 때, X가 나올 수 있는 likelihood를 찾아가면서 가장 적합한 쎼타를 찾습니다. 즉, X를 가장 잘 설명할 수 있는 쎄타를 찾는 것이죠. 
- 하지만, 이렇게 할 경우, 데이터에 과적합될 수 있습니다. 참고한 링크에서는 'n 번 연속으로 동전이 앞면이 나왔을 때'와 같은 예외적인 경우를 말하며, 이러한 샘플 데이터에 내재된 특성이 모델의 파라미터에 영향을 미친다는 식으로 이야기를 하는데, 아니, 데이터 자체가 이미 잘못되어 있는거 아닌가 싶지만 아무튼 MAP에서는 그렇지 않다고 합니다. 뭐 그런가 보죠 뭐.
- 다시 정리하자면, MLE의 경우 maximize L(x | theta)인 theta를 찾죠(어떤 세타가 주어졌을 때, X가 나올 확률이 가장 높은가), 반대로 MAP의 경우 maximize f(theta | x), 즉, x가 주어졌을 때 x에 대해서 가장 확률이 높은 theta를 찾는다는데, 이게 무슨 개소리인지 모르겠네요 후. 헷갈리니까 수식을 봅시다. 
- 베이지안 정리를 사용해서 다음으로 수식을 전개할 수 있죠.
    - f( theta | x)
    - = f(x | theta) \* f(theta) / f(x)
- 이제 다시 봅시다. 하나 씩 정리를 해볼게요. 
    - f(x) : 우리는 theta를 찾고 있는데, 이 값은 theta와 관계가 없죠. 무시합니다. 
- f(theta) : theta에 대한 가정을 의미한다고 보면 됩니다. 사실 이게 제일 중요한데, MLE에서는 theta에 대한 아무런 가정을 하지 않았습니다. 가정이라는 말이 이상하지만, 가령 '평균이 10일 확률이 0.5'와 같은 정보가 없이, 모든 theta에 대해서 uniform한 distribution을 가진다고 가정하죠. 
- 하지만, 사실 우리는 데이터를 보고 어느 정도 '선입견'을 가질 수 있습니다. 이 데이터와 다르게 우리가 어떤 '일반성'을 학습 과정에 주입할 수 있는 것이죠. 즉, theta에 대해서 어떤 확률 분포를 주는 것, 이런 것을 보통 MAP라고 합니다. 당연하지만, 이런 선입견은 잘 잡히면 효율적이지만, 잘못 잡히면 영 이상한 모델을 찾을 수도 있죠.

## wrap-up

- MLE를 구현한 코드는 많이 있는데, 아직 MAP를 구현한 코드는 아직 찾지 못했습니다. 이후, 찾으면 업데이트할게요.

## reference 

- <https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/>
- <https://ratsgo.github.io/statistics/2017/09/23/MLE/>
- <http://sanghyukchun.github.io/58/>
- <https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/>