{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지난번에 코딩할때, 제대로 모듈화도 하지 않고, 너무 막 코딩하여 이 노트북에서는 좀 정리를 해놓으려고 합니다. \n",
    "    - 아직 추가로 하지 못한 부분이 많아서, 해야할게 많거든요. 아무튼. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop 2830 row\n",
      "빈도 시트 완료\n",
      "키워드 전체 네트워크 centrality 완료\n",
      "키워드 centrality 연도별 순위 변화 완료 \n",
      "excel complete\n",
      "save all figure\n",
      "ppt complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "required library \n",
    "\"\"\"\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from inflection import singularize \n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"\n",
    "desc: centrality를 계산하는 함수들입니다. \n",
    "input: graph(undirected)\n",
    "output: dictionary\n",
    "\"\"\"\n",
    "def return_weighted_degree_centrality(input_g, normalized=True):\n",
    "    w_d_centrality = {n:0.0 for n in input_g.nodes()}\n",
    "    for u, v, d in input_g.edges(data=True):\n",
    "        w_d_centrality[u]+=d['weight']\n",
    "        w_d_centrality[v]+=d['weight']\n",
    "    if normalized==True:\n",
    "        weighted_sum = sum(w_d_centrality.values())\n",
    "        return {k:v/weighted_sum for k, v in w_d_centrality.items()}\n",
    "    else:\n",
    "        return w_d_centrality\n",
    "def return_closeness_centrality(input_g):\n",
    "    new_g_with_distance = input_g.copy()\n",
    "    for u,v,d in new_g_with_distance.edges(data=True):\n",
    "        if 'distance' not in d:\n",
    "            d['distance'] = 1.0/d['weight']\n",
    "    return nx.closeness_centrality(new_g_with_distance, distance='distance')\n",
    "def return_betweenness_centrality(input_g):\n",
    "    return nx.betweenness_centrality(input_g, weight='weight')\n",
    "def return_pagerank(input_g):\n",
    "    return nx.pagerank(input_g, weight='weight')\n",
    "\n",
    "\"\"\"\n",
    "desc: pd.DataFrame의 특정 칼럼(each element is list)의 모든 리스트를 합친 다음 Counter => pd.DataFrame\n",
    "input: pd.DataFrame\n",
    "output: pd.DataFrame\n",
    "\"\"\"\n",
    "def total_count(input_df, column_name='Author Keywords'):\n",
    "    # 'Author Keywords' or 'Noun Phrases'\n",
    "    r = itertools.chain.from_iterable(input_df[column_name])\n",
    "    r = Counter(r).most_common()\n",
    "    return pd.DataFrame(r, columns=[column_name, 'count'])\n",
    "\"\"\"\n",
    "desc: 전체적으로 listfmf filtering하여 리턴한다. \n",
    "input: pd.DataFrame(each element is list of kwd)\n",
    "output: pd.DataFrame\n",
    "\"\"\"\n",
    "def filtering_auth_kwds(input_df,column_name='Author Keywords', above_n=3):\n",
    "    \"\"\"\n",
    "    개별 node가 전체에서 1번 밖에 등장하지 않는 경우도 많은데, 이를 모두 고려해서 분석을 하면, 효율적이지 못한 계산이 된다. \n",
    "    따라서, 빈도가 일정 이상을 넘는 경우에 대해서만 고려하여 new_df를 수정하는 것이 필요하다. \n",
    "    \"\"\"\n",
    "    # singularize \n",
    "    input_df[column_name] = input_df[column_name].apply(lambda ks: [singularize(k).strip().lower() for k in ks])\n",
    "    # drop low count kwd \n",
    "    filtered_kwds = total_count(input_df, column_name=column_name)\n",
    "    filtered_kwds = set(filtered_kwds[filtered_kwds['count']>=above_n][column_name])\n",
    "    input_df[column_name] = input_df[column_name].apply(lambda ks: list(filter(lambda k: True if k in filtered_kwds else False, ks)))\n",
    "    # edge를 만들때 중복을 방지하기 위해서 sorting해둔다. \n",
    "    input_df[column_name] = input_df[column_name].apply(lambda l: sorted(list(set(l))))\n",
    "    \"\"\"\n",
    "    word embeddingd 등 다른 데이터 전처리가 필요하다면 여기서 처리하는 것이 좋음. \n",
    "    \"\"\"\n",
    "    return input_df# 사실 굳이 return을 쓸 필요가 없음. 이미 내부에서 다 바꿔줌. \n",
    "\"\"\"\n",
    "desc: 연도별로 상위 빈도 키워드를 50개씩 칼럼별로 보여줌\n",
    "input: pd.DataFrame\n",
    "output: pd.DataFrame(column 이름은 각 연도)\n",
    "\"\"\"\n",
    "def yearly_count_rank(input_df, column_name='Author Keywords', until_rank_n=50):\n",
    "    r_dict = {}\n",
    "    for year, year_df in input_df.groupby('Year'):\n",
    "        r_dict[year] = list(total_count(year_df, column_name=column_name)[column_name])[:until_rank_n]\n",
    "        if len(r_dict[year])<until_rank_n:\n",
    "            for i in range(0, until_rank_n - len(r_dict[year])):\n",
    "                r_dict[year].append(\"\")\n",
    "    return pd.DataFrame(r_dict)\n",
    "\n",
    "def make_centrality_df(inputG, cent_func):\n",
    "    deg_cent = cent_func(inputG)\n",
    "    deg_cent = sorted([(k, v) for k, v in deg_cent.items()], key=lambda x: x[1], reverse=True)\n",
    "    return pd.DataFrame(deg_cent, columns=['node', 'centrality'])\n",
    "    \n",
    "def yearly_centrality_rank(input_df, cent_func, column_name='Author Keywords', until_rank_n=50):\n",
    "    r_dict = {}\n",
    "    for year, year_df in input_df.groupby('Year'):\n",
    "        yearG = make_graph(year_df)\n",
    "        \"\"\"\n",
    "        node_weight_lst = [n[1]['weight'] for n in yearG.nodes(data=True)]\n",
    "        if len(node_weight_lst)>100:\n",
    "            node_weight_threshold = node_weight_lst[100]\n",
    "        else:\n",
    "            node_weight_threshold = node_weight_lst[-1]\n",
    "        yearG = drop_low_weighted_node(yearG, node_weight_threshold)\n",
    "        \"\"\"\n",
    "        r_dict[year] = list(make_centrality_df(yearG, cent_func)['node'])[:50]\n",
    "        if len(r_dict[year])<until_rank_n:\n",
    "            r_dict[year]+=[\"\" for i in range(0, until_rank_n - len(r_dict[year]))]\n",
    "    return pd.DataFrame(r_dict)\n",
    "\"\"\"\n",
    "desc: df로부터 그래프를 생성하는 함수\n",
    "input: pd.DataFrame\n",
    "output: nx.Graph\n",
    "\"\"\"\n",
    "def make_graph(input_df, column_name='Author Keywords'):\n",
    "    # make edges: edge가 중복으로 생기지 않게 하려면, \n",
    "    def make_edges_from_lst(lst):\n",
    "        if len(lst)>1:\n",
    "            return [(lst[i], lst[j]) for i in range(0, len(lst)-1) for j in range(i+1, len(lst))]\n",
    "        else:\n",
    "            return []\n",
    "    nodes = total_count(input_df)\n",
    "    new_nodes = []\n",
    "    for i in range(0, len(nodes)):\n",
    "        name = nodes[column_name].iloc()[i]\n",
    "        w = nodes['count'].iloc()[i]\n",
    "        new_nodes.append( (name, {'weight':w}) )\n",
    "    nodes = new_nodes\n",
    "    edges = itertools.chain.from_iterable(input_df[column_name].apply(make_edges_from_lst))\n",
    "    edges = ((uv[0], uv[1], w) for uv, w in Counter(edges).most_common())\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_weighted_edges_from(edges)\n",
    "    # graph에 대한 데이터 필터링이 필요할 수 있는데. 여기서. \n",
    "    return G\n",
    "\"\"\"\n",
    "desc: graph에서 일정 수 이하의 weight를 가진 edgef를 삭제한 그래프를 리턴한다. \n",
    "만약 edge가 없어져서, node가 isolatede될 경우 해당 node도 함께 없애준다. \n",
    "input: graph\n",
    "output: graph \n",
    "\"\"\"\n",
    "def drop_low_weighted_edge(inputG, above_weight=3):\n",
    "    rG = nx.Graph()\n",
    "    rG.add_nodes_from(inputG.nodes(data=True))\n",
    "    edges = filter(lambda e: True if e[2]['weight']>=above_weight else False, inputG.edges(data=True))\n",
    "    rG.add_edges_from(edges)\n",
    "    for n in inputG.nodes(): # neighbor가 없는 isolated node를 모두 지운다. \n",
    "        if len(list(nx.all_neighbors(rG, n)))==0:\n",
    "            rG.remove_node(n)\n",
    "    return rG\n",
    "\"\"\"\n",
    "desc: graph에서 일정 수 이하의 weight를 가진 node를 삭제한 그래프를 리턴한다. \n",
    "input: graph\n",
    "output: graph \n",
    "\"\"\"\n",
    "def drop_low_weighted_node(inputG, above_weight=3):\n",
    "    rG = nx.Graph()\n",
    "    rG.add_nodes_from(inputG.nodes(data=True))\n",
    "    rG.add_edges_from(inputG.edges(data=True))\n",
    "    for n in inputG.nodes(data=True):\n",
    "        if n[1]['weight'] <= above_weight:\n",
    "            rG.remove_node(n[0])\n",
    "    rrG = rG.copy()\n",
    "    for n in rG.nodes():# 만약 node가 isolated 면 삭제한다. \n",
    "        if len(list(nx.all_neighbors(rG, n)))==0:\n",
    "            rrG.remove_node(n)\n",
    "    return rrG\n",
    "\"\"\"\n",
    "desc: 이 함수에서는 따로 node, edge를 filtering하지 않고 그래프를 그대로 그려줍니다. \n",
    "input: graph\n",
    "output: none(plt.show() or plt.savefig())\n",
    "\"\"\"\n",
    "def draw_whole_graph(inputG, outPicName):\n",
    "    plt.close('all')\n",
    "    f = plt.figure(figsize=(16, 10))\n",
    "    plt.margins(x=0.05, y=0.05) # text 가 잘리는 경우가 있어서, margins을 넣음\n",
    "    pos = nx.spring_layout(inputG)\n",
    "    \"\"\"\n",
    "    - weight에 따라서 값을 0.1에서 1.0으로 스케일링 하는데, 그냥 minmax scaling 하는 것은 적합하지 않을 것 같고 \n",
    "    - 해당 데이터들이 특정한 분포를 가지고 있다고 가정하고, 그 분포에 의거해서 그림을 그려주는 게 좋을 것 같다는 생각이 드는데. \n",
    "    - 흐음. \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    한번씩 input_lst가 비어있을때가 있는데 왜 그런지 확인 필요.\n",
    "    \"\"\"\n",
    "    def return_log_scaled_lst(input_lst):\n",
    "        r_lst = map(np.log, input_lst)\n",
    "        try:\n",
    "            max_v = max(map(np.log, input_lst))\n",
    "            min_v = min(map(np.log, input_lst))\n",
    "            return map(lambda v: v/max_v, r_lst)\n",
    "        except:\n",
    "            print(input_lst)\n",
    "    node_weight_lst = return_log_scaled_lst([n[1]['weight'] for n in inputG.nodes(data=True)])\n",
    "    edge_weight_lst = return_log_scaled_lst([e[2]['weight'] for e in inputG.edges(data=True)])\n",
    "    nx.draw_networkx_nodes(inputG, pos, \n",
    "                           node_size = list(map(lambda x: x*2000, node_weight_lst)),\n",
    "                     #node_size = [ n[1]['weight']*1000 for n in inputG.nodes(data=True)],\n",
    "                     alpha=1.0 )\n",
    "    # label의 경우는 특정 node만 그릴 수 없음. 그리면 모두 그려야함. \n",
    "    nx.draw_networkx_labels(inputG, pos, font_weight='bold', \n",
    "                            font_family='sans-serif', \n",
    "                            font_color='black', font_size=15\n",
    "                           )\n",
    "    nx.draw_networkx_edges(inputG, pos, \n",
    "                           width = list(map(lambda x: 5**(x+1), edge_weight_lst)), \n",
    "                           edge_color='b', alpha=0.5\n",
    "                          )\n",
    "    plt.axis('off')\n",
    "    plt.savefig('../../assets/images/markdown_img/'+outPicName)\n",
    "    #plt.show()\n",
    "\"\"\"\n",
    "일종의 main 함수입니다. \n",
    "raw_df를 넘기는데, 가능하면 해당 argument에서 복사해서 넘겨주는 게 좋을 것 같습니다. 혹시나 싶어서요. \n",
    "\"\"\"\n",
    "def scopus_analysis(raw_df, outExcelname, outPPTname):\n",
    "    # 모든 키워드에 대한 centrality를 계산할 필요가 없으므로 날려준다 .\n",
    "    def make_excel():\n",
    "        writer = pd.ExcelWriter(outExcelname)\n",
    "        total_count(r_df, column_name='Author Keywords').to_excel(writer, '01. 전체 저자 키워드 빈도 상위 키워드')\n",
    "        total_count(r_df, column_name='Noun Phrases').to_excel(writer, '02. 전체 noun phrase 빈도 상위')\n",
    "        yearly_count_rank(r_df, column_name='Author Keywords').to_excel(writer, '03. 연도별 저자 키워드 순위 변화')\n",
    "        yearly_count_rank(r_df, column_name='Noun Phrases').to_excel(writer, '04. 연도별 noun phrase 순위 변화')\n",
    "        print(\"빈도 시트 완료\")\n",
    "\n",
    "        make_centrality_df(kwdG, return_weighted_degree_centrality).to_excel(writer, '05. 키워드 전체 네트워크 w.deg cent')\n",
    "        make_centrality_df(kwdG, return_closeness_centrality).to_excel(writer, '06. 키워드 전체 네트워크 close cent')\n",
    "        make_centrality_df(kwdG, return_betweenness_centrality).to_excel(writer, '07. 키워드 전체 네트워크 bet. cent')\n",
    "        print(\"키워드 전체 네트워크 centrality 완료\")\n",
    "\n",
    "        yearly_centrality_rank(r_df, return_weighted_degree_centrality).to_excel(writer, '08. w.deg cent 키워드 연도별 순위 변화')\n",
    "        yearly_centrality_rank(r_df, return_closeness_centrality).to_excel(writer, '09. close cent 키워드 연도별 순위 변화')\n",
    "        yearly_centrality_rank(r_df, return_betweenness_centrality).to_excel(writer, '10. between cent 키워드 연도별 순위 변화')\n",
    "        yearly_centrality_rank(r_df, nx.pagerank).to_excel(writer, '11. page rank 키워드 연도별 순위 변화')\n",
    "        print('키워드 centrality 연도별 순위 변화 완료 ')\n",
    "        writer.save()\n",
    "        \n",
    "    def save_figure_and_return_content(ego_n=10):\n",
    "        # draw entire network \n",
    "        content_lst = []\n",
    "        above_weight = total_count(r_df)['count'].iloc()[15]\n",
    "        entireKwdG = drop_low_weighted_node(kwdG, above_weight = above_weight)\n",
    "        entire_pic_title = 'entire_kwd_network_20180510.png'\n",
    "        draw_whole_graph(entireKwdG, entire_pic_title)\n",
    "        content_lst.append( (\"entire network for SME\", \"\", entire_pic_title) )\n",
    "        # drwa ego network \n",
    "        for kwd in list(total_count(r_df)['Author Keywords'])[:ego_n]:\n",
    "            egoG = nx.ego_graph(kwdG, kwd)\n",
    "            max_node_len = 15\n",
    "            if len(egoG.nodes())>max_node_len:\n",
    "                above_weight = sorted((n[1]['weight'] for n in egoG.nodes(data=True)), reverse=True)[max_node_len]\n",
    "                egoG = drop_low_weighted_node(egoG, above_weight=above_weight)\n",
    "            egoGG = egoG.copy()\n",
    "            for e in egoG.edges():\n",
    "                if e[0]==kwd or e[1]==kwd:\n",
    "                    continue\n",
    "                else:\n",
    "                    egoGG.remove_edge(e[0], e[1])\n",
    "            #above_weight = sorted([n[1]['weight'] for n in egoG.nodes(data=True)], reverse=True)[10]\n",
    "            ego_network_pic_title = \"ego_network-{} 20180509.png\".format(kwd)\n",
    "            draw_whole_graph(egoGG, ego_network_pic_title)\n",
    "            content_lst.append((\"ego network of '{}'\".format(kwd),\"\",ego_network_pic_title))\n",
    "        return content_lst\n",
    "    \n",
    "    def make_ppt(content_lst):\n",
    "        this_prs = Presentation()\n",
    "        slide_layout = this_prs.slide_layouts[1]\n",
    "        for title, content, img_file_name in content_lst:\n",
    "            this_slide = this_prs.slides.add_slide(slide_layout)\n",
    "            shapes = this_slide.shapes\n",
    "            shapes.title.text = title\n",
    "            shapes.placeholders[1].text = content\n",
    "            img_path='../../assets/images/markdown_img/'\n",
    "            shapes.add_picture( img_path+img_file_name, Inches(0.5), Inches(1.5), \n",
    "                               height=Inches(6), width=Inches(9) )\n",
    "        this_prs.save(outPPTname)\n",
    "    r_df = raw_df.dropna()\n",
    "    print(\"drop {} row\".format(len(raw_df) - len(r_df)))\n",
    "    r_df['Author Keywords'] = r_df['Author Keywords'].apply(lambda s: s.strip().split(\";\"))\n",
    "    r_df['Noun Phrases'] = r_df['Abstract'].apply(lambda s: TextBlob(s).noun_phrases)\n",
    "    r_df = filtering_auth_kwds(r_df, 'Author Keywords', 10)\n",
    "    r_df = filtering_auth_kwds(r_df, 'Noun Phrases', 10)\n",
    "    \n",
    "    \"\"\"\n",
    "    여기서부터는 엑셀에 시트별로 내용을 넣어주는 부분 \n",
    "    \"\"\"\n",
    "    kwdG = make_graph(r_df, 'Author Keywords')\n",
    "    kwdG = drop_low_weighted_node(kwdG, 10)\n",
    "    \n",
    "    make_excel()\n",
    "    print(\"excel complete\")\n",
    "    content_lst = save_figure_and_return_content()\n",
    "    print(\"save all figure\")\n",
    "    make_ppt(content_lst)\n",
    "    print(\"ppt complete\")\n",
    "    print('all complete')\n",
    "\n",
    "excel_path_and_filename = \"../../../Downloads/SMEs_Scopus_2013-2017.xlsx\"\n",
    "df = pd.read_excel(excel_path_and_filename)\n",
    "df = df[['Author Keywords', 'Year', 'Abstract']]\n",
    "\n",
    "scopus_analysis(df.copy(), 'simple_report_for_SME.xlsx', 'simple_figure_for_SME.pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
