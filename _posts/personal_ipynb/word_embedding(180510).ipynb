{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ==> datum\n",
      "people ==> person\n",
      "cs ==> c\n"
     ]
    }
   ],
   "source": [
    "## singularize\n",
    "from inflection import singularize\n",
    "for w in ['data', 'people', 'cs']:\n",
    "    print(\"{} ==> {}\".format(w, singularize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   am  am boys  are  are girl  boy  boy or  boys  boys boy  girl  he  he is  \\\n",
      "0   1        1    0         0    1       0     1         1     0   0      0   \n",
      "1   0        0    1         1    0       0     0         0     1   0      0   \n",
      "2   0        0    0         0    1       1     0         0     1   1      1   \n",
      "\n",
      "   is  is the  or  or girl  the  the boy  you  you are  \n",
      "0   0       0   0        0    0        0    0        0  \n",
      "1   0       0   0        0    0        0    1        1  \n",
      "2   1       1   1        1    1        1    0        0  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"ngram_range를 넣으면 앞 뒤 window를 고려하여 확장된 형태로 제시해주지만, range가 커질 경우, 계산의 복잡도를 생각해야 함.\n",
    "늘어나는 ngram_range일 수록 일정 값 이하의 워드 gram threshold를 함께 제시해주는 것이 필요함. \n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def make_count_vector_df(sent_lst):\n",
    "    \"\"\"\n",
    "    관사 삭제: the a 등은 자동으로 삭제됨\n",
    "    단수 복수는?: 삭제되지 않는다. \n",
    "    비교적 간단하게 여기서 count vector를 만들 수 있기는 한데, 이정도는 나도 금방 코딩해서 만들 수 있는 정도기는 함. \n",
    "    뭐 그래도 경우에 \n",
    "    \"\"\"\n",
    "    CV_model = CountVectorizer(\n",
    "        ngram_range=(1,2), # 앞 뒤 window를 고려하여 확장된 형태로 제시해줌. phrase를 뽑아낼 수 있는 강점이 있기는 할듯. \n",
    "        min_df = 1, # document freqeuency 가 1 이상은 되는 키워드만으로 vocabulary를 구성\n",
    "        max_df = 2, # document frequency가 2 이하인 키워드만으로 vocabulary를 구성 \n",
    "        binary = False # binary이면 있다 없다 구조로 변경됨\n",
    "    )\n",
    "    cv_result = CV_model.fit_transform(sent_lst)\n",
    "    # print(\"CV_model.vocabulary_: {}\".format(CV_model.vocabulary_))\n",
    "    return pd.DataFrame(cv_result.toarray(),\n",
    "             columns = [it[0] for it in sorted(CV_model.vocabulary_.items(), key=lambda x: x[1])])\n",
    "sent_lst = [\"I am a boys, boy\", 'You are a girl', \"he is the a boy or girl\"]\n",
    "print(make_count_vector_df(sent_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         am       boy       dog      girl\n",
      "0  0.508542  0.861037  0.000000  0.000000\n",
      "1  0.508542  0.000000  0.000000  0.861037\n",
      "2  0.508542  0.000000  0.861037  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def make_tfidf_df(sent_lst):\n",
    "    TFIDFmodel = TfidfVectorizer(\n",
    "        ngram_range=(1,1), # 앞 뒤 window를 고려하여 확장된 형태로 제시해줌. phrase를 뽑아낼 수 있는 강점이 있기는 할듯. \n",
    "        min_df = 1, # document freqeuency 가 1 이상은 되는 키워드만으로 vocabulary를 구성\n",
    "        max_df = 10, # document frequency가 2 이하인 키워드만으로 vocabulary를 구성 \n",
    "        binary = False # binary이면 있다 없다 구조로 변경됨\n",
    "    )\n",
    "    TFIDFmodel.fit(sent_lst)\n",
    "    #print(TFIDFmodel.vocabulary_)\n",
    "    return pd.DataFrame(TFIDFmodel.transform(sent_lst).toarray(),\n",
    "             columns = [it[0] for it in sorted(TFIDFmodel.vocabulary_.items(), key=lambda x: x[1])])\n",
    "sent_lst = [\"I am a boy\", \"I am a girl\", \"I am a dog\"]\n",
    "print( make_tfidf_df(sent_lst) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA60AAAD8CAYAAACcn0IQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n63q6osgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2ff16e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "for w in model.wv.index2entity:\n",
    "    print(\"{} ==> {}\".format(w, model.wv.get_vector(w)))\n",
    "\n",
    "print(model.wv.most_similar('boy'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# using tsne and visualization \n",
    "\"\"\"\n",
    "# actor + she - actress \n",
    "print(model.wv.most_similar(positive=['actor', 'she'], negative='actress', topn=1))\n",
    "# actress + he - actor\n",
    "print(model.wv.most_similar(positive=['actress', 'he'], negative='actor', topn=1))\n",
    "\"\"\"\n",
    "\n",
    "sent_lst = [\"I am a boy\", \"I am a girl\", \"I am a dog\"]*10\n",
    "sent_split_lst = map(lambda s: list(s.lower().split(\" \")), sent_lst)\n",
    "\n",
    "model = Word2Vec(list(sent_split_lst), \n",
    "                 size=2, # word2vec은 뉴럴넷을 활용해 학습하는 형태인데, 최종 layer size, 벡터 사이즈\n",
    "                 window = 3, # 앞뒤 문 맥을 몇 window size까지 파악할 것인가? \n",
    "                 min_count=1 # vocab는 최소한 count 몇 이상인 워드들로 구성할 것인가? \n",
    "                )\n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "xy_text = [(model.wv.get_vector(w)[0], model.wv.get_vector(w)[0], w) for w in model.wv.index2entity]\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.margins(0.2, 0.2)# \n",
    "for x, y, ind in xy_text:\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x, y, ind, fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"vector of {}:\".format('man'))\n",
    "print(model.wv['man'])\n",
    "for w1, w2 in [('actor', 'actress'), ('man', 'woman')]:\n",
    "    print(\"similarity of {} and {}:\".format(w1, w2))\n",
    "    print(model.wv.similarity(w1, w2))\n",
    "\n",
    "print()\n",
    "print(model.wv.most_similar('man'))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.'], ['one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.'], ['what', \"'\", 's', 'the', 'deal', '?'], ['watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.'], ['.'], ['.'], ['critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.'], ['which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.'], ['they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.']]\n",
      "vector of man:\n",
      "[-0.05991558 -0.03086592  0.01189669  0.102423    0.08505771 -0.09862742\n",
      "  0.08142797  0.02029772 -0.17479017  0.01900225  0.05792401  0.06180907\n",
      " -0.04836149 -0.10937159  0.14327788 -0.04057971 -0.1174344  -0.04258196\n",
      "  0.18731837  0.151831    0.10687945  0.08146088 -0.08685417  0.10933898\n",
      " -0.16440198  0.0373562   0.01310256  0.03336507 -0.01018305 -0.0933388\n",
      " -0.10338249  0.04603217 -0.07799796 -0.024062    0.05609193  0.0566499\n",
      "  0.0074096   0.33555806 -0.1898278  -0.11308021 -0.07655233  0.11526188\n",
      "  0.12217306  0.01184229  0.09759069 -0.11515123 -0.15305457  0.10993052\n",
      " -0.00248098  0.02767029  0.04375774 -0.00440128  0.04237483 -0.04811569\n",
      " -0.19767506 -0.10172944 -0.07304381  0.04213926  0.05408062  0.00856169\n",
      "  0.15958184  0.08053605  0.09385015 -0.1240105  -0.11469473 -0.05808977\n",
      " -0.0507145  -0.06650107 -0.06885882 -0.04288544 -0.18494043 -0.2141173\n",
      " -0.0578146  -0.03582449 -0.07636473  0.01424812  0.06276877  0.10046591\n",
      " -0.12056368  0.0165755   0.11693306  0.16448683 -0.02465955 -0.01681185\n",
      " -0.00900315 -0.01431582 -0.01640091  0.11476148 -0.07439734 -0.01545318\n",
      "  0.0801242   0.12226445  0.14416789 -0.1167293   0.03779126 -0.12788197\n",
      "  0.03959465  0.16347627 -0.1416588   0.0379013 ]\n",
      "similarity of actor and actress:\n",
      "0.88108045572\n",
      "similarity of man and woman:\n",
      "0.902238936279\n",
      "\n",
      "[('woman', 0.9022389650344849), ('girl', 0.8441144824028015), ('boy', 0.8249863386154175), ('child', 0.8178112506866455), ('killer', 0.7937303781509399), ('doctor', 0.7489098310470581), ('kid', 0.7231124639511108), ('lady', 0.7107384204864502), ('guy', 0.709437370300293), ('secret', 0.7061775326728821)]\n",
      "\n",
      "[('he', 0.3141002058982849)]\n",
      "[('she', 0.38161590695381165)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]\n",
    "\n",
    "print(sentences[:10])\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences, size=100) \n",
    "# 여기서 사이즈는 vector의 크기를 말합니다. nn의 최종 layer size라고 생각해도 됩니다. \n",
    "# 뉴럴넷의 사이즈를 좀 깊게 만들어보고 싶은데, gensim에서 자동으로 해주는지 모르겠네요. \n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 \n",
    "\n",
    "print(\"vector of {}:\".format('man'))\n",
    "print(model.wv['man'])\n",
    "for w1, w2 in [('actor', 'actress'), ('man', 'woman')]:\n",
    "    print(\"similarity of {} and {}:\".format(w1, w2))\n",
    "    print(model.wv.similarity(w1, w2))\n",
    "\n",
    "print()\n",
    "print(model.wv.most_similar('man'))\n",
    "print()\n",
    "# actor + she - actress \n",
    "print(model.wv.most_similar(positive=['actor', 'she'], negative='actress', topn=1))\n",
    "# actress + he - actor\n",
    "print(model.wv.most_similar(positive=['actress', 'he'], negative='actor', topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbipartite graph를 구성해서 거기서부터, similarity를 뽑아낼 수 있지 않을까? 사실 그게 일종의 워드 임베딩 아닌가. \\n원래 워드 임베딩은 개별 단어를 중심으로 하는 건데, 여기는 지금 그렇게 구성할 수 가 없을 것 같음. \\n\\nauthor kwd ==> index kwd \\nauthor kwd ==> abstrac noun or verb (or both)\\n\\n의 형태로 bipartite하게 구성할 수 있는데 이렇게 구성하여, structure equivalence를 만들 수 있지 않을까? \\n\\n아무튼 일단 워드 임베딩을 조금만 더 파서 정리는 마저 하는 것이 필요함. \\n'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bipartite graph를 구성해서 거기서부터, similarity를 뽑아낼 수 있지 않을까? 사실 그게 일종의 워드 임베딩 아닌가. \n",
    "원래 워드 임베딩은 개별 단어를 중심으로 하는 건데, 여기는 지금 그렇게 구성할 수 가 없을 것 같음. \n",
    "\n",
    "author kwd ==> index kwd \n",
    "author kwd ==> abstrac noun or verb (or both)\n",
    "\n",
    "의 형태로 bipartite하게 구성할 수 있는데 이렇게 구성하여, structure equivalence를 만들 수 있지 않을까? \n",
    "\n",
    "아무튼 일단 워드 임베딩을 조금만 더 파서 정리는 마저 하는 것이 필요함. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-fb019c283242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'am'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'good'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m bigram_transformer = gensim.models.phrases.Phraser([\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_transformer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, phrases_model)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'threshold'"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "ss = ['I', 'am', 'a', 'good', 'boy'] \n",
    "bigram_transformer = gensim.models.phrases.Phraser([\n",
    "    ss\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'small and medium enterprises' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-015b962efa43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 학습 완료 후, 필요없는 메모리 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"small and medium enterprises\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'small and medium enterprises' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(abs_lst)\n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 \n",
    "#model.most_similar(\"small and medium enterprises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_counter = Counter(itertools.chain.from_iterable(auth_kwd_lst)).most_common()\n",
    "count1_w = list(filter(lambda x: True if x[1]==5 else False, auth_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proximity\n",
      "[('trust,', 0.7394723892211914), ('experience,', 0.7393851280212402), ('formality', 0.7343860864639282), ('embeddedness', 0.7326754927635193), (\"owner's\", 0.72593092918396), ('human,', 0.7258467674255371), ('income,', 0.720426619052887), ('structure,', 0.7192338109016418), ('knowledge)', 0.7186062932014465), ('loyalty', 0.717450737953186)]\n",
      "\n",
      "fuzzy comprehensive evaluation\n",
      "[('prosthetics', 0.9823917150497437), ('information systems security', 0.9795824289321899), ('wastewater treatment', 0.9777668118476868), ('natural language processing', 0.9777394533157349), ('credit evaluations', 0.977641761302948), ('agile methods', 0.9775588512420654), ('power cables', 0.9772118926048279), ('cost saving', 0.9771889448165894), ('drug blood level', 0.9763650298118591), ('drug bioavailability', 0.9759867787361145)]\n",
      "\n",
      "spillovers\n",
      "[('portfolios', 0.7346036434173584), ('appropriability', 0.7307323217391968), ('cross-border', 0.7283568978309631), ('breadth', 0.7144168615341187), ('mind-set', 0.712887704372406), ('tenure', 0.7062869668006897), ('inflows', 0.701845109462738), ('reallocation', 0.6988070607185364), ('equity,', 0.6935837268829346), ('involvement.', 0.6921118497848511)]\n",
      "\n",
      "sensitivity analysis\n",
      "[('software testing', 0.992296576499939), ('data acquisition', 0.9920706748962402), ('user interfaces', 0.9906312823295593), ('computer software selection and evaluation', 0.9906026721000671), ('embedded systems', 0.9890177845954895), ('petroleum engineering', 0.9889569878578186), ('best practices', 0.9889198541641235), ('energy management systems', 0.9884681701660156), ('neural networks', 0.9881827235221863), ('computer simulation', 0.9881125092506409)]\n",
      "\n",
      "waste\n",
      "[('material', 0.7799482345581055), ('traffic', 0.7637003660202026), ('equipment', 0.7552963495254517), ('electricity', 0.7420525550842285), ('transport', 0.7340760231018066), ('raw', 0.7266721725463867), ('water', 0.723881185054779), ('air', 0.7221243381500244), ('charge', 0.7171646952629089), ('energy,', 0.7137024402618408)]\n",
      "\n",
      "technological innovation capabilities\n",
      "[('medium enterprises', 0.8217032551765442), ('wood processing', 0.7786175012588501), ('industrial enterprise', 0.7584035396575928), ('financial system', 0.7444849610328674), ('corporate social responsibilities (csr)', 0.744378387928009), ('firm size', 0.7370509505271912), ('industrial investment', 0.7339873909950256), ('business development', 0.7292322516441345), ('software and hardwares', 0.7284433245658875), ('brazil', 0.7282372117042542)]\n",
      "\n",
      "enablers\n",
      "[('drivers', 0.8133851885795593), ('barriers', 0.7590547204017639), ('facilitators', 0.7429195642471313), ('obstacles', 0.722565770149231), ('motivations', 0.6505460143089294), ('weaknesses', 0.650393009185791), ('tqm', 0.6491608619689941), ('motives', 0.6373159289360046), ('autonomy', 0.6284334659576416), ('oi', 0.6266810894012451)]\n",
      "\n",
      "vikor\n",
      "[('topsis', 0.9251444339752197), ('dematel', 0.896966278553009), ('ism', 0.8885839581489563), ('mcdm', 0.8861225247383118), ('information mining', 0.8844143152236938), ('colony', 0.8795072436332703), ('togaf', 0.8790115714073181), ('pso', 0.877009391784668), ('neuro-fuzzy', 0.8761899471282959), ('industrial electronics', 0.870215117931366)]\n",
      "\n",
      "renewable energy sources\n",
      "[('superconducting fault current limiters (sfcls)', 0.9842095375061035), ('reactive power compensation', 0.9841621518135071), ('high-temperature superconductor (hts)-cables', 0.984147846698761), ('superconducting faultcurrent limiters (sfcl)', 0.9800307750701904), ('time delay', 0.9798702001571655), ('flywheel energy storage system (fess)', 0.9796547889709473), ('superconducting magnetic energies', 0.9795759320259094), ('supercapacitor', 0.9793630242347717), ('laser treatment', 0.9785982966423035), ('petroleum reservoir evaluation', 0.9783592820167542)]\n",
      "\n",
      "energy consumption\n",
      "[('environmental pollutions', 0.9723266363143921), ('industrie 4.0', 0.9721337556838989), ('manufacturing execution system', 0.9709111452102661), ('remote monitoring', 0.9705083966255188), ('fleet operations', 0.9696674942970276), ('model and simulation', 0.9694508910179138), ('service platforms', 0.9688398241996765), ('programmable logic controllers', 0.9687053561210632), ('particle swarm optimization', 0.9684544205665588), ('industrial robotics', 0.968034565448761)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for w in count1_w[:10]:\n",
    "    print(w[0])\n",
    "    try:\n",
    "        print(model.most_similar(w[0]))\n",
    "    except:\n",
    "        continue\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plot',\n",
       "  ':',\n",
       "  'two',\n",
       "  'teen',\n",
       "  'couples',\n",
       "  'go',\n",
       "  'to',\n",
       "  'a',\n",
       "  'church',\n",
       "  'party',\n",
       "  ',',\n",
       "  'drink',\n",
       "  'and',\n",
       "  'then',\n",
       "  'drive',\n",
       "  '.'],\n",
       " ['they', 'get', 'into', 'an', 'accident', '.'],\n",
       " ['one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'guys',\n",
       "  'dies',\n",
       "  ',',\n",
       "  'but',\n",
       "  'his',\n",
       "  'girlfriend',\n",
       "  'continues',\n",
       "  'to',\n",
       "  'see',\n",
       "  'him',\n",
       "  'in',\n",
       "  'her',\n",
       "  'life',\n",
       "  ',',\n",
       "  'and',\n",
       "  'has',\n",
       "  'nightmares',\n",
       "  '.'],\n",
       " ['what', \"'\", 's', 'the', 'deal', '?'],\n",
       " ['watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.'],\n",
       " ['.'],\n",
       " ['.'],\n",
       " ['critique',\n",
       "  ':',\n",
       "  'a',\n",
       "  'mind',\n",
       "  '-',\n",
       "  'fuck',\n",
       "  'movie',\n",
       "  'for',\n",
       "  'the',\n",
       "  'teen',\n",
       "  'generation',\n",
       "  'that',\n",
       "  'touches',\n",
       "  'on',\n",
       "  'a',\n",
       "  'very',\n",
       "  'cool',\n",
       "  'idea',\n",
       "  ',',\n",
       "  'but',\n",
       "  'presents',\n",
       "  'it',\n",
       "  'in',\n",
       "  'a',\n",
       "  'very',\n",
       "  'bad',\n",
       "  'package',\n",
       "  '.'],\n",
       " ['which',\n",
       "  'is',\n",
       "  'what',\n",
       "  'makes',\n",
       "  'this',\n",
       "  'review',\n",
       "  'an',\n",
       "  'even',\n",
       "  'harder',\n",
       "  'one',\n",
       "  'to',\n",
       "  'write',\n",
       "  ',',\n",
       "  'since',\n",
       "  'i',\n",
       "  'generally',\n",
       "  'applaud',\n",
       "  'films',\n",
       "  'which',\n",
       "  'attempt',\n",
       "  'to',\n",
       "  'break',\n",
       "  'the',\n",
       "  'mold',\n",
       "  ',',\n",
       "  'mess',\n",
       "  'with',\n",
       "  'your',\n",
       "  'head',\n",
       "  'and',\n",
       "  'such',\n",
       "  '(',\n",
       "  'lost',\n",
       "  'highway',\n",
       "  '&',\n",
       "  'memento',\n",
       "  ')',\n",
       "  ',',\n",
       "  'but',\n",
       "  'there',\n",
       "  'are',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'ways',\n",
       "  'of',\n",
       "  'making',\n",
       "  'all',\n",
       "  'types',\n",
       "  'of',\n",
       "  'films',\n",
       "  ',',\n",
       "  'and',\n",
       "  'these',\n",
       "  'folks',\n",
       "  'just',\n",
       "  'didn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'snag',\n",
       "  'this',\n",
       "  'one',\n",
       "  'correctly',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'have',\n",
       "  'taken',\n",
       "  'this',\n",
       "  'pretty',\n",
       "  'neat',\n",
       "  'concept',\n",
       "  ',',\n",
       "  'but',\n",
       "  'executed',\n",
       "  'it',\n",
       "  'terribly',\n",
       "  '.']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences)\n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-5f9fcd98af16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m ]\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 학습 완료 후, 필요없는 메모리 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    [\"a\", 'is', 'not', 'b'], ['b', 'is', 'c']\n",
    "]\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences)\n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 \n",
    "model.most_similar('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
