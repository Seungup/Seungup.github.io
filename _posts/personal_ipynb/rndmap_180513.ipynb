{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from inflection import singularize \n",
    "import nltk\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excel_path_and_filename = \"../../../Downloads/SMEs_Scopus_2013-2017.xlsx\"\n",
    "rawDF = pd.read_excel(excel_path_and_filename)\n",
    "df = rawDF[['Author Keywords', 'Year', 'Abstract', 'Index Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for computation efficienty, cut down node got below weight, remaining node is 1077\n",
      "solving transivity\n",
      "keyword set size: 22883\n",
      "keyword set size: 22842\n",
      "2.0763649940490723\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "def basic_filter_Series(i_Series):\n",
    "    \"\"\"\n",
    "    주관적이지 않은 필터링, 아주 기본적인 filtering. \n",
    "    split 부터 keyword list of list로 변환하여 리턴\n",
    "    \"\"\"\n",
    "    r_Series = i_Series.copy().fillna(\"\").apply(lambda s: s.strip().lower().split(\";\"))\n",
    "    def replace_sp_chr(input_s):\n",
    "        return \"\".join(map(lambda c: c if 'a'<=c and c<='z' else c if '0'<=c and c<='9'else \" \", input_s)).strip()\n",
    "    def remove_double_space(input_s):\n",
    "        while \"  \" in input_s:\n",
    "            input_s = input_s.replace(\"  \", \" \")\n",
    "        return input_s.strip()\n",
    "    r_Series = r_Series.apply(\n",
    "        lambda ks: list(map(\n",
    "            lambda k: remove_double_space(replace_sp_chr(k)), ks)))\n",
    "    \n",
    "    all_kwd_set = set(itertools.chain.from_iterable(list(r_Series)))\n",
    "    to_singular_dict = {}\n",
    "    for kwd in all_kwd_set:\n",
    "        singularized_kwd = singularize(kwd)\n",
    "        if singularized_kwd !=kwd and singularized_kwd in all_kwd_set:\n",
    "            to_singular_dict[kwd] = singularized_kwd\n",
    "    \"\"\"remove blank string\"\"\"\n",
    "    r_Series = r_Series.apply(lambda ks:filter(lambda k: True if k!=\"\" else False, ks))\n",
    "    \"\"\"singularize \"\"\"\n",
    "    r_Series = r_Series.apply(\n",
    "        lambda ks: sorted(list(set(map(\n",
    "            lambda k: to_singular_dict[k].strip() if k in to_singular_dict.keys() else k.strip(), ks\n",
    "        )))))    \n",
    "    return r_Series\n",
    "def make_graph_from_series(i_Series):\n",
    "    rG = nx.Graph()\n",
    "    rG.add_nodes_from(\n",
    "        (n[0], {'weight':n[1]}) for n in Counter(itertools.chain.from_iterable(i_Series)).most_common())\n",
    "    edges = []\n",
    "    for x in i_Series:\n",
    "        if len(x)!=0:\n",
    "            edges += [(x[i], x[j]) for i in range(0, len(x)-1) for j in range(i+1, len(x))]\n",
    "    rG.add_edges_from(\n",
    "        [(e[0][0], e[0][1], {'weight':e[1]}) for e in Counter(edges).most_common()])\n",
    "    return rG\n",
    "def syntactical_simialrity_dict(i_Series, above_node_w=10, above_sim=0.9):\n",
    "    kwd_counter = itertools.chain.from_iterable(i_Series)\n",
    "    kwd_count_dct = {w:c for w, c in Counter(kwd_counter).most_common() if c >= above_node_w}\n",
    "    print(\"for computation efficienty, cut down node got below weight, remaining node is {}\".format(len(kwd_count_dct)))\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    kwd_changed_dct = {}\n",
    "    for w1 in sorted(kwd_count_dct.keys()):\n",
    "        for w2 in sorted(kwd_count_dct.keys()):\n",
    "            if w1 < w2 and w1[0]==w2[0] and \" \" in w1 and \" \" in w2:\n",
    "                \"\"\"중복을 피하고, 처음 캐릭터가 같고, 해당 단어가 복합어일 것 \n",
    "                \"\"\"\n",
    "                sim_v = difflib.SequenceMatcher(None,w1, w2).ratio()\n",
    "                if sim_v >= above_sim:\n",
    "                    if kwd_count_dct[w1] >= kwd_count_dct[w2]:\n",
    "                        kwd_changed_dct[w2]=w1\n",
    "                    else:\n",
    "                        kwd_changed_dct[w1]=w2\n",
    "    def make_non_transitive(input_dct):\n",
    "        print('solving transivity')\n",
    "        non_transvitiy_kwd_dict = {}\n",
    "        for k, v in input_dct.items():\n",
    "            while v in input_dct.keys():\n",
    "                v = input_dct[v]\n",
    "            non_transvitiy_kwd_dict[k] = v\n",
    "        return non_transvitiy_kwd_dict\n",
    "    return make_non_transitive(kwd_changed_dct)\n",
    "\n",
    "def transform_by_dict(i_Series, input_dct):\n",
    "    print('keyword set size: {}'.format(len(set(itertools.chain.from_iterable(i_Series)))))\n",
    "    r_S = i_Series.apply(lambda ks: list(set([input_dct[k] if k in input_dct.keys() else k for k in ks])))\n",
    "    print('keyword set size: {}'.format(len(set(itertools.chain.from_iterable(r_S)))))\n",
    "    return r_S\n",
    "temp = basic_filter_Series(rawDF['Author Keywords'])\n",
    "import time\n",
    "start = time.time()\n",
    "temp = transform_by_dict(temp, syntactical_simialrity_dict(temp, 6, 0.9))\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "temp.head()\n",
    "g = make_graph_from_series(temp)\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for computation efficienty, cut down node got below weight, remaining node is 1077\n",
      "solving transivity\n",
      "keyword set size: 22883\n",
      "keyword set size: 22842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for computation efficienty, cut down node got below weight, remaining node is 1077\n",
      "solving transivity\n",
      "keyword set size: 22883\n",
      "keyword set size: 22842\n",
      "the dimension of dataframe is 639\n",
      "top 10 closest pair: euclidean space::\n",
      "('binary mixture', 'metallacarboranes', 0.0)\n",
      "('binary mixture', 'flotation', 0.0)\n",
      "('metallacarboranes', 'flotation', 0.0)\n",
      "('binary mixture', 'carbapenemase', 0.0125)\n",
      "('carbapenemase', 'metallacarboranes', 0.0125)\n",
      "('carbapenemase', 'flotation', 0.0125)\n",
      "('transient stability', 'battery', 0.29829779253058464)\n",
      "('transient stability', 'carbapenemase', 0.3305048960211117)\n",
      "('transient stability', 'binary mixture', 0.3307411923149668)\n",
      "('transient stability', 'metallacarboranes', 0.3307411923149668)\n",
      "top 10 closest pair: jaccard space::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:616: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.double(np.bitwise_or(u != 0, v != 0).sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('exploration', 'exploitation', 0.29629629629629628)\n",
      "('pecking order theory', 'trade off theory', 0.45454545454545453)\n",
      "('microstructure', 'niti', 0.63636363636363635)\n",
      "('transient stability', 'power quality', 0.66666666666666663)\n",
      "('power quality', 'ac loss', 0.66666666666666663)\n",
      "('superelasticity', 'actuator', 0.66666666666666663)\n",
      "('superelasticity', 'nitinol', 0.66666666666666663)\n",
      "('industrial symbiosis', 'circular economy', 0.66666666666666663)\n",
      "('usability', 'requirements engineering', 0.69230769230769229)\n",
      "('ahp', 'analytic hierarchy process', 0.70588235294117652)\n"
     ]
    }
   ],
   "source": [
    "temp = basic_filter_Series(rawDF['Author Keywords'])\n",
    "temp = transform_by_dict(temp, syntactical_simialrity_dict(temp, 6, 0.9))\n",
    "g = make_graph_from_series(temp)\n",
    "\"\"\"node가 너무 많으면, 계산이 어려워져서 일정 이상의 weight를 가진 node만이 의미있는 node라고 가정했습니다. \n",
    "\"\"\"\n",
    "g.remove_nodes_from( [n[0] for n in g.copy().nodes(data=True) if n[1]['weight']<=8] )\n",
    "\n",
    "temp_df = pd.DataFrame(\n",
    "    nx.adjacency_matrix(g).toarray(), index=[n for n in g.nodes()], columns=[n for n in g.nodes()]\n",
    ")\n",
    "print(\"the dimension of dataframe is {}\".format(temp_df.shape[0]))\n",
    "\n",
    "###\n",
    "\n",
    "from scipy.spatial.distance import euclidean, jaccard\n",
    "def make_close_pair_lst(adj_df, dist_func):\n",
    "    r_lst = []\n",
    "    for i in range(0, len(adj_df)-1):\n",
    "        for j in range(i+1, len(adj_df)):\n",
    "            r_lst.append(\n",
    "                (adj_df.index[i], adj_df.index[j], dist_func(adj_df.iloc()[i], adj_df.iloc()[j]))\n",
    "            )\n",
    "    return sorted(r_lst, key=lambda x: x[2])\n",
    "        \n",
    "print(\"top 10 closest pair: euclidean space::\")\n",
    "\"\"\"euclidean distance의 경우는 표준화가 필요함. \n",
    "\"\"\"\n",
    "for p in make_close_pair_lst(temp_df.apply(lambda col: (col)/(col.max())).fillna(0), euclidean)[:10]:\n",
    "    print(p)\n",
    "print(\"top 10 closest pair: jaccard space::\")\n",
    "for p in make_close_pair_lst(temp_df, jaccard)[:10]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for computation efficienty, cut down node got below weight, remaining node is 1077\n",
      "solving transivity\n",
      "keyword set size: 22883\n",
      "keyword set size: 22842\n"
     ]
    }
   ],
   "source": [
    "temp_auth = basic_filter_Series(rawDF['Author Keywords'])\n",
    "temp_auth = transform_by_dict(temp_auth, syntactical_simialrity_dict(temp_auth, 6, 0.9))\n",
    "\n",
    "temp_ind = basic_filter_Series(rawDF['Index Keywords'])\n",
    "temp_ind = transform_by_dict(temp_ind, syntactical_simialrity_dict(temp_ind, 6, 0.9))\n",
    "\n",
    "temp_df = pd.DataFrame({'Author Keywrods':temp_auth, 'Index Keywords':temp_ind})\n",
    "temp_df.head()\n",
    "#temp = transform_by_dict(temp, syntactical_simialrity_dict(temp, 6, 0.9))\n",
    "def make_bigraph_from_series(iS, jS):\n",
    "    if len(iS)!=len(jS):\n",
    "        print(\"different length of Series\")\n",
    "        return None\n",
    "    rG = nx.Graph()\n",
    "    edges = []\n",
    "    def make_edges_from_bipartite_sets(setA, setB):\n",
    "        if len(setA)==[] or len(setB)==[]:\n",
    "            return []\n",
    "        else:\n",
    "            return [(n1, n2+\"(i)\") for n1 in setA for n2 in setB]\n",
    "    for i in range(0, len(iS)):\n",
    "        if i%100==0:\n",
    "            print(\"complete {}\".format(i))\n",
    "        edges+=make_edges_from_bipartite_sets(iS.iloc()[i], iS.iloc()[j])\n",
    "    rG.add_edges_from([(e[0][0], e[0][1], {'weight':e[1]}) for e in Counter(edges).most_common()])\n",
    "    return rG\n",
    "biG = make_bigraph_from_series(temp_auth, temp_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22883\n",
      "after try 51: remove top degree centrality doesn't matter\n",
      "True\n",
      "19363\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "가장 중요한 node를 삭제하고, 몇 가지의 그룹으로 나누어지는지 확인해보자. \n",
    "일종의 클러스터링 분석이라고 생각함. 만약 이 아이가 없어도 커넥션이 유지되면 이 그룹을 나름 긴밀한 구조라고 평가할 수도 있음 \n",
    "하나의 키워드만 삭제해도, 커넥션이 유지되지 못함.\n",
    "이런 식으로 빈도가 가장 높은 키워드(이걸 빈도로 하는게 적합한지는 모르겠는데, degree centrality로 하는게 더 적합할 수도 있고 \n",
    "하나를 지워도 여전히 네트워크에 아무런 타격이 없는 경우가, 해당 네트워크가 어느 정도 안정화되어 있다 라고 평가할 수 있지 않을까 싶음. \n",
    "그러나, 이걸 이론적으로 증명하려면 조금 어렵다는 생각이 들기는 하는ㄷ. \n",
    "\"\"\"\n",
    "print(len(g.nodes(data=True)))\n",
    "g1 = g.copy()\n",
    "for i in range(0, 100):\n",
    "    before = nx.is_connected(g1)\n",
    "    removed_k = max(nx.degree_centrality(g1).items(), key=lambda k: k[1])\n",
    "    g1.remove_node(removed_k[0])\n",
    "    after = nx.is_connected(g1)\n",
    "    if nx.is_connected(g1)==True:\n",
    "        print(\"after try {}: remove top degree centrality doesn't matter\".format(i))\n",
    "        break\n",
    "    else:\n",
    "        g1 = max(nx.connected_component_subgraphs(g1), key=lambda subG: len(subG.nodes()))\n",
    "print(nx.is_connected(g1))\n",
    "print(len(g1.nodes(data=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/message.c:4294)\n",
      "KeyboardInterrupt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try 0: remove top degree node: sme\n",
      "try 1: remove top degree node: small and medium sized enterprise\n",
      "try 2: remove top degree node: innovation\n",
      "try 3: remove top degree node: small and medium enterprise\n",
      "try 4: remove top degree node: entrepreneurship\n",
      "try 5: remove top degree node: small and medium sized enterprises sme\n",
      "try 6: remove top degree node: performance\n",
      "try 7: remove top degree node: small to medium sized enterprise\n",
      "try 8: remove top degree node: small and medium enterprises sme\n",
      "try 9: remove top degree node: cloud computing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def make_graph_dense(i_G):\n",
    "    \"\"\"graph에서 degree centrality가 가장 높은 node를 삭제해도, graph는 여전히 connected여야 함\n",
    "    그럴때마다 잘려나가는 node는 모두 삭제함. \n",
    "    \"\"\"\n",
    "    tempG = i_G.copy()\n",
    "    tempG = max(nx.connected_component_subgraphs(tempG), key=lambda subG: len(subG.nodes()))\n",
    "    removed_nodes = set(i_G.nodes())\n",
    "    for i in range(0, 10):\n",
    "        top_deg_node = max(nx.degree_centrality(tempG).items(), key=lambda k: k[1])[0]\n",
    "        print(\"try {}: remove top degree node: {}\".format(i, top_deg_node))\n",
    "        removed_nodes.remove(top_deg_node)# 이 node는 마지막에 다시 넣어줘야 함.\n",
    "        tempG.remove_nodes_from([top_deg_node])\n",
    "        if nx.is_connected(tempG)==True:\n",
    "            print(\"after try {}: remove top degree centrality doesn't matter\".format(i))\n",
    "            break\n",
    "        else:\n",
    "            tempG = max(nx.connected_component_subgraphs(tempG), key=lambda subG: len(subG.nodes()))\n",
    "    \"\"\"만약 100번을 넘어도 커넥션이 유지되지 않는다면 이를 메세지로 알려주는 것이 필요한데.\"\"\"\n",
    "    return tempG # 이걸 가지고 해보자 한번. \n",
    "    removed_nodes.difference_update(set(tempG.nodes()))\n",
    "    rG = i_G.copy()\n",
    "    rG.remove_nodes_from(removed_nodes)\n",
    "    \"\"\"단 이렇게 변형했을때, node의 weight attribute는 그대로 유지됨. 뭐 근데 별 의미없을 수 잇지만.\"\"\"\n",
    "    return rG\n",
    "denseG = make_graph_dense(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseG.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sme', 'innovation', 412.37846694511097),\n",
       " ('sme', 'superconducting magnetic energy storage', 397.28453279733907),\n",
       " ('sme', 'superconducting magnetic energy storage smes', 397.25306795542815),\n",
       " ('sme', 'shape memory effect', 397.25306795542815),\n",
       " ('sme', 'south africa', 393.39166234174309),\n",
       " ('sme', 'small medium enterprise', 392.96437497564585),\n",
       " ('sme', 'risk', 392.44744871128927),\n",
       " ('sme', 'sustainable development', 392.30982654019766),\n",
       " ('sme', 'trust', 391.97066216746373),\n",
       " ('sme', 'erp', 391.92218615434365),\n",
       " ('sme', 'survey', 391.59545451907383),\n",
       " ('sme', 'risk management', 391.22116507162542),\n",
       " ('sme', 'productivity', 391.18154353190027),\n",
       " ('sme', 'social media', 390.0487149062281),\n",
       " ('sme', 'indium', 389.61647809095547),\n",
       " ('sme', 'intellectual capital', 389.35459416834931),\n",
       " ('sme', 'export', 389.31606696872916),\n",
       " ('sme', 'e commerce', 389.28267364474368),\n",
       " ('sme', 'barrier', 389.06683230519661),\n",
       " ('sme', 'internationalisation', 388.93187064060459)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempG = denseG.copy()\n",
    "tempG.remove_nodes_from(\n",
    "    [n[0] for n in tempG.nodes(data=True) if n[1]['weight'] < 50]\n",
    ")\n",
    "print(len(tempG.nodes()))\n",
    "adj_df = pd.DataFrame(nx.adjacency_matrix(tempG).toarray(), index=tempG.nodes(), columns=tempG.nodes())\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "dist_lst = []\n",
    "for i in range(0, len(adj_df)-1):\n",
    "    for j in range(i+1, len(adj_df)):\n",
    "        dist_lst.append(\n",
    "            (adj_df.index[i], adj_df.index[j], euclidean(adj_df.iloc()[i], adj_df.iloc()[j]))\n",
    "        )\n",
    "sorted(dist_lst, key=lambda x: x[2], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"a\":[i for i in range(0, 10)]}).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
