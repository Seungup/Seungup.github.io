{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "\n",
    "train_df = pd.read_csv(\"https://storage.googleapis.com/kaggle-competitions-data/kaggle/3004/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1527922704&Signature=ABIBMpRUAsvD97%2FpOGe0Ch7%2F3ytIrGGgLrSIRkv6Q2X5%2BRaiueFBRSRuCUdiVIqeHa4sXdD0ML37qI0ybx%2FaS5Z3NnYWj9N2c6%2B6JYzjl12ebZzrqblAPXDjBiIVSNO6ygiV4i9GBwn0cQB1cZtysQhsWUtAqPmfgnXnepmbvcmDn2kG78qFxhkNm78Opva52vigptri%2F08byHP7DTgEB6778%2FKSAwdJw5nHH8nj9M7x263mqj01ct6D0o5PLpV%2FbGSQf8kZM3q5kGT1KhaIbh%2FAtz2wAN4brFtB%2Fvznyvbuqx%2BK%2FWR%2FGSvYnQ1AaRcWwHeMCRpCmIgT2dfCq29WLQ%3D%3D\") \n",
    "test_df = pd.read_csv(\"https://storage.googleapis.com/kaggle-competitions-data/kaggle/3004/test.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1527922522&Signature=R%2BCLwbq0LUF5tZ%2BowcWPnJOTfkCo5xNB%2BxQNFoAPlsrqyM8PanUsgU3%2FcfFrMv86U6oMHsWup2vuVjD61ekG8NphsiEEFamBb3whTaA%2BdEBVdL2cscyaES4sJPAIGTU1x%2BYcsvjbGuxdFR8y6oi31Ar42pAicoW%2B0lz1HLeQeo86lksOaj3fXjRqWKWD34rqNqKt04HoYepzOgSZqI4hAFHoAyoT%2F4oC7Nx%2BakA4mJTzOCs%2BmlH4z08SsevZ%2BBnvqZGrS3NfquqxhyFONfL6lPOH8cr1%2BxiaT1E5tTqs74Zo%2BKbQLJp6so1QGNI89UFl6NiVIZFPhULeXZ7%2BatpiZw%3D%3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [10, 10]\n",
      "train accuracy: 91.27%, test accuracy: 88.73%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 10]\n",
      "train accuracy: 94.61%, test accuracy: 91.24%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 80, 10]\n",
      "train accuracy: 96.34%, test accuracy: 92.65%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 40, 80, 40, 20, 10]\n",
      "train accuracy: 96.73%, test accuracy: 92.60%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 40, 80, 160, 80, 40, 20, 10]\n",
      "train accuracy: 96.98%, test accuracy: 93.29%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 960, 240, 80, 10]\n",
      "train accuracy: 96.83%, test accuracy: 92.52%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 480, 960, 480, 240, 80, 10]\n",
      "train accuracy: 93.98%, test accuracy: 90.95%\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def print_accuracy(clf):\n",
    "    X = train_df[train_df.columns[1:]]\n",
    "    Y = train_df['label']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\n",
    "    train_sample_size = len(x_train)\n",
    "    x_train = x_train[x_train.columns][:train_sample_size]\n",
    "    y_train = y_train[:train_sample_size]\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"train accuracy: {:.2%}, test accuracy: {:.2%}\".format(\n",
    "        accuracy_score(y_train, clf.predict(x_train)),\n",
    "        accuracy_score(y_test, clf.predict(x_test))\n",
    "    ))\n",
    "hidden_layer_size_lst = [\n",
    "    [10, 10], \n",
    "    [10, 20, 10],\n",
    "    [10, 80, 240, 80, 10],\n",
    "    [10, 20, 40, 80, 40, 20, 10],\n",
    "    [10, 20, 40, 80, 160, 80, 40, 20, 10],\n",
    "    [10, 80, 240, 960, 240, 80, 10],\n",
    "    [10, 80, 240, 480, 960, 480, 240, 80, 10],\n",
    "]\n",
    "for h_l_s in hidden_layer_size_lst:\n",
    "    print(\"hidden_layer_size: {}\".format(h_l_s))\n",
    "    print_accuracy( MLPClassifier(hidden_layer_sizes=h_l_s, activation='relu', solver='adam') )\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29400,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12s - loss: 0.5048 - categorical_accuracy: 0.8413\n",
      "Epoch 2/20\n",
      "10s - loss: 0.1401 - categorical_accuracy: 0.9580\n",
      "Epoch 3/20\n",
      "9s - loss: 0.0832 - categorical_accuracy: 0.9748\n",
      "Epoch 4/20\n",
      "9s - loss: 0.0586 - categorical_accuracy: 0.9816\n",
      "Epoch 5/20\n",
      "9s - loss: 0.0446 - categorical_accuracy: 0.9864\n",
      "Epoch 6/20\n",
      "9s - loss: 0.0327 - categorical_accuracy: 0.9898\n",
      "Epoch 7/20\n",
      "11s - loss: 0.0212 - categorical_accuracy: 0.9933\n",
      "Epoch 8/20\n",
      "11s - loss: 0.0141 - categorical_accuracy: 0.9956\n",
      "Epoch 9/20\n",
      "12s - loss: 0.0121 - categorical_accuracy: 0.9963\n",
      "Epoch 10/20\n",
      "11s - loss: 0.0167 - categorical_accuracy: 0.9943\n",
      "Epoch 11/20\n",
      "11s - loss: 0.0139 - categorical_accuracy: 0.9957\n",
      "Epoch 12/20\n",
      "11s - loss: 0.0084 - categorical_accuracy: 0.9976\n",
      "Epoch 13/20\n",
      "11s - loss: 0.0078 - categorical_accuracy: 0.9974\n",
      "Epoch 14/20\n",
      "10s - loss: 0.0108 - categorical_accuracy: 0.9966\n",
      "Epoch 15/20\n",
      "9s - loss: 0.0029 - categorical_accuracy: 0.9992\n",
      "Epoch 16/20\n",
      "10s - loss: 0.0082 - categorical_accuracy: 0.9976\n",
      "Epoch 17/20\n",
      "11s - loss: 0.0191 - categorical_accuracy: 0.9937\n",
      "Epoch 18/20\n",
      "12s - loss: 0.0104 - categorical_accuracy: 0.9963\n",
      "Epoch 19/20\n",
      "12s - loss: 0.0136 - categorical_accuracy: 0.9958\n",
      "Epoch 20/20\n",
      "11s - loss: 0.0080 - categorical_accuracy: 0.9976\n",
      "train, loss and metric: [0.011053406523133162, 0.99630952380952376]\n",
      "test, loss and metric: [0.13161088258205425, 0.97297619024912518]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- keras에서는 Y를 one_hot vector로 바꾸어 사용해야 함\n",
    "- X의 값들이 0과 256 사이에 분포해있으므로, 이를 0과 1.0 사이로 움직임 \n",
    "- train, test set으로 구분하여 진행\n",
    "\"\"\"\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.values.astype(np.float64)/256.0, \n",
    "                                                    Y.values.astype(np.float32), \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import metrics\n",
    "\"\"\"\n",
    "- 원래 데이터가 784이므로, 비슷한 1024로 두고 이를 감소시키는 뉴럴넷 설계\n",
    "- 총 10개로 클래스를 구분하므로, 마지막 Dense는 10개의 노드를 가지고 있어야함\n",
    "- 그리고, 마지막은 softmax\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(1024, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(512),\n",
    "    Activation('relu'),\n",
    "    Dense(256),\n",
    "    Activation('relu'),\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dense(32),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\"\"\"\n",
    "- multi classification이므로 loss는 'categorical_crossentropy'\n",
    "- metric에는 내가 추적할 지표들이 들어감. 최적화는 loss에 따라 되는데, epoch 마다 평가될 지표들이 metric에 들어감. \n",
    "\"\"\"\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              #optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\"\"\"\n",
    "- one epoch = one forward pass and one backward pass of all the training examples\n",
    "- batch size = the number of training examples in one forward/backward pass. \n",
    "\n",
    "- 즉, epoch을 증가한다는 것은 전체 데이터 셋을 몇 번 돌릴 것이냐 를 결정하는 것이고, \n",
    "- batch_size는 backpropagation을 몇 개의 size로 돌릴 것이냐? 를 결정하는 이야기다.\n",
    "\"\"\"\n",
    "train_history = model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=2)\n",
    "train_history = train_history.history # epoch마다 변화한 loss, metric\n",
    "\n",
    "# 아래 세 줄은 필요없는 코드인데, 그래도 이후에 사용될 수 있어서 일단 넣어둠. \n",
    "y_predict = model.predict_classes(x_train, verbose=0)\n",
    "y_true = [ np.argmax(y) for y in y_train]\n",
    "accuracy = np.sum([y_comp[0]==y_comp[1] for y_comp in zip(y_predict, y_true)])\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 67s - loss: 0.6176 - categorical_accuracy: 0.8340    \n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 56s - loss: 0.1475 - categorical_accuracy: 0.9559    \n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 53s - loss: 0.0870 - categorical_accuracy: 0.9741    \n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 51s - loss: 0.0662 - categorical_accuracy: 0.9798    \n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 51s - loss: 0.0509 - categorical_accuracy: 0.9848    \n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 43s - loss: 0.0450 - categorical_accuracy: 0.9862    \n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 59s - loss: 0.0397 - categorical_accuracy: 0.9879    \n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 50s - loss: 0.0351 - categorical_accuracy: 0.9890    \n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 47s - loss: 0.0300 - categorical_accuracy: 0.9910    \n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 49s - loss: 0.0290 - categorical_accuracy: 0.9908    \n",
      "train, loss and metric: [0.02223081667064911, 0.9933035714285714]\n",
      "test, loss and metric: [0.043031070519770898, 0.98821428594135108]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- keras에서는 Y를 one_hot vector로 바꾸어 사용해야 함\n",
    "- X의 값들이 0과 256 사이에 분포해있으므로, 이를 0과 1.0 사이로 움직임 \n",
    "- train, test set으로 구분하여 진행\n",
    "\"\"\"\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(64, (5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 504s - loss: 0.5980 - categorical_accuracy: 0.8100   \n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 481s - loss: 0.0938 - categorical_accuracy: 0.9699   \n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 462s - loss: 0.0650 - categorical_accuracy: 0.9795   \n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 478s - loss: 0.0479 - categorical_accuracy: 0.9854   \n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 462s - loss: 0.0354 - categorical_accuracy: 0.9884   \n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 438s - loss: 0.0271 - categorical_accuracy: 0.9913   \n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 416s - loss: 0.0263 - categorical_accuracy: 0.9912   \n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 442s - loss: 0.0209 - categorical_accuracy: 0.9929   \n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 474s - loss: 0.0223 - categorical_accuracy: 0.9922   \n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 521s - loss: 0.0130 - categorical_accuracy: 0.9958   \n",
      "train, loss and metric: [0.010848363778620427, 0.99648809523809523]\n",
      "test, loss and metric: [0.042763131727420148, 0.98869047630400886]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 141s   \n"
     ]
    }
   ],
   "source": [
    "test_X_values = (test_df.values.astype(np.float64)/256.0).reshape(len(test_df), 28, 28, 1)\n",
    "test_y_pred = model.predict_classes(test_X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.DataFrame({\"ImageId\":range(1, 1+len(test_y_pred)), \"Label\":test_y_pred})\n",
    "submit_df.to_csv('test_mnist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
