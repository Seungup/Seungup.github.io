{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "\n",
    "train_df = pd.read_csv(\"https://storage.googleapis.com/kaggle-competitions-data/kaggle/3004/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1527922704&Signature=ABIBMpRUAsvD97%2FpOGe0Ch7%2F3ytIrGGgLrSIRkv6Q2X5%2BRaiueFBRSRuCUdiVIqeHa4sXdD0ML37qI0ybx%2FaS5Z3NnYWj9N2c6%2B6JYzjl12ebZzrqblAPXDjBiIVSNO6ygiV4i9GBwn0cQB1cZtysQhsWUtAqPmfgnXnepmbvcmDn2kG78qFxhkNm78Opva52vigptri%2F08byHP7DTgEB6778%2FKSAwdJw5nHH8nj9M7x263mqj01ct6D0o5PLpV%2FbGSQf8kZM3q5kGT1KhaIbh%2FAtz2wAN4brFtB%2Fvznyvbuqx%2BK%2FWR%2FGSvYnQ1AaRcWwHeMCRpCmIgT2dfCq29WLQ%3D%3D\") \n",
    "test_df = pd.read_csv(\"https://storage.googleapis.com/kaggle-competitions-data/kaggle/3004/test.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1527922522&Signature=R%2BCLwbq0LUF5tZ%2BowcWPnJOTfkCo5xNB%2BxQNFoAPlsrqyM8PanUsgU3%2FcfFrMv86U6oMHsWup2vuVjD61ekG8NphsiEEFamBb3whTaA%2BdEBVdL2cscyaES4sJPAIGTU1x%2BYcsvjbGuxdFR8y6oi31Ar42pAicoW%2B0lz1HLeQeo86lksOaj3fXjRqWKWD34rqNqKt04HoYepzOgSZqI4hAFHoAyoT%2F4oC7Nx%2BakA4mJTzOCs%2BmlH4z08SsevZ%2BBnvqZGrS3NfquqxhyFONfL6lPOH8cr1%2BxiaT1E5tTqs74Zo%2BKbQLJp6so1QGNI89UFl6NiVIZFPhULeXZ7%2BatpiZw%3D%3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [10, 10]\n",
      "train accuracy: 91.27%, test accuracy: 88.73%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 10]\n",
      "train accuracy: 94.61%, test accuracy: 91.24%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 80, 10]\n",
      "train accuracy: 96.34%, test accuracy: 92.65%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 40, 80, 40, 20, 10]\n",
      "train accuracy: 96.73%, test accuracy: 92.60%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 20, 40, 80, 160, 80, 40, 20, 10]\n",
      "train accuracy: 96.98%, test accuracy: 93.29%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 960, 240, 80, 10]\n",
      "train accuracy: 96.83%, test accuracy: 92.52%\n",
      "-----------------\n",
      "hidden_layer_size: [10, 80, 240, 480, 960, 480, 240, 80, 10]\n",
      "train accuracy: 93.98%, test accuracy: 90.95%\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def print_accuracy(clf):\n",
    "    X = train_df[train_df.columns[1:]]\n",
    "    Y = train_df['label']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\n",
    "    train_sample_size = len(x_train)\n",
    "    x_train = x_train[x_train.columns][:train_sample_size]\n",
    "    y_train = y_train[:train_sample_size]\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"train accuracy: {:.2%}, test accuracy: {:.2%}\".format(\n",
    "        accuracy_score(y_train, clf.predict(x_train)),\n",
    "        accuracy_score(y_test, clf.predict(x_test))\n",
    "    ))\n",
    "hidden_layer_size_lst = [\n",
    "    [10, 10], \n",
    "    [10, 20, 10],\n",
    "    [10, 80, 240, 80, 10],\n",
    "    [10, 20, 40, 80, 40, 20, 10],\n",
    "    [10, 20, 40, 80, 160, 80, 40, 20, 10],\n",
    "    [10, 80, 240, 960, 240, 80, 10],\n",
    "    [10, 80, 240, 480, 960, 480, 240, 80, 10],\n",
    "]\n",
    "for h_l_s in hidden_layer_size_lst:\n",
    "    print(\"hidden_layer_size: {}\".format(h_l_s))\n",
    "    print_accuracy( MLPClassifier(hidden_layer_sizes=h_l_s, activation='relu', solver='adam') )\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29400,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12s - loss: 0.5048 - categorical_accuracy: 0.8413\n",
      "Epoch 2/20\n",
      "10s - loss: 0.1401 - categorical_accuracy: 0.9580\n",
      "Epoch 3/20\n",
      "9s - loss: 0.0832 - categorical_accuracy: 0.9748\n",
      "Epoch 4/20\n",
      "9s - loss: 0.0586 - categorical_accuracy: 0.9816\n",
      "Epoch 5/20\n",
      "9s - loss: 0.0446 - categorical_accuracy: 0.9864\n",
      "Epoch 6/20\n",
      "9s - loss: 0.0327 - categorical_accuracy: 0.9898\n",
      "Epoch 7/20\n",
      "11s - loss: 0.0212 - categorical_accuracy: 0.9933\n",
      "Epoch 8/20\n",
      "11s - loss: 0.0141 - categorical_accuracy: 0.9956\n",
      "Epoch 9/20\n",
      "12s - loss: 0.0121 - categorical_accuracy: 0.9963\n",
      "Epoch 10/20\n",
      "11s - loss: 0.0167 - categorical_accuracy: 0.9943\n",
      "Epoch 11/20\n",
      "11s - loss: 0.0139 - categorical_accuracy: 0.9957\n",
      "Epoch 12/20\n",
      "11s - loss: 0.0084 - categorical_accuracy: 0.9976\n",
      "Epoch 13/20\n",
      "11s - loss: 0.0078 - categorical_accuracy: 0.9974\n",
      "Epoch 14/20\n",
      "10s - loss: 0.0108 - categorical_accuracy: 0.9966\n",
      "Epoch 15/20\n",
      "9s - loss: 0.0029 - categorical_accuracy: 0.9992\n",
      "Epoch 16/20\n",
      "10s - loss: 0.0082 - categorical_accuracy: 0.9976\n",
      "Epoch 17/20\n",
      "11s - loss: 0.0191 - categorical_accuracy: 0.9937\n",
      "Epoch 18/20\n",
      "12s - loss: 0.0104 - categorical_accuracy: 0.9963\n",
      "Epoch 19/20\n",
      "12s - loss: 0.0136 - categorical_accuracy: 0.9958\n",
      "Epoch 20/20\n",
      "11s - loss: 0.0080 - categorical_accuracy: 0.9976\n",
      "train, loss and metric: [0.011053406523133162, 0.99630952380952376]\n",
      "test, loss and metric: [0.13161088258205425, 0.97297619024912518]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- keras에서는 Y를 one_hot vector로 바꾸어 사용해야 함\n",
    "- X의 값들이 0과 256 사이에 분포해있으므로, 이를 0과 1.0 사이로 움직임 \n",
    "- train, test set으로 구분하여 진행\n",
    "\"\"\"\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.values.astype(np.float64)/256.0, \n",
    "                                                    Y.values.astype(np.float32), \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import metrics\n",
    "\"\"\"\n",
    "- 원래 데이터가 784이므로, 비슷한 1024로 두고 이를 감소시키는 뉴럴넷 설계\n",
    "- 총 10개로 클래스를 구분하므로, 마지막 Dense는 10개의 노드를 가지고 있어야함\n",
    "- 그리고, 마지막은 softmax\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(1024, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(512),\n",
    "    Activation('relu'),\n",
    "    Dense(256),\n",
    "    Activation('relu'),\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dense(32),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\"\"\"\n",
    "- multi classification이므로 loss는 'categorical_crossentropy'\n",
    "- metric에는 내가 추적할 지표들이 들어감. 최적화는 loss에 따라 되는데, epoch 마다 평가될 지표들이 metric에 들어감. \n",
    "\"\"\"\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              #optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\"\"\"\n",
    "- one epoch = one forward pass and one backward pass of all the training examples\n",
    "- batch size = the number of training examples in one forward/backward pass. \n",
    "\n",
    "- 즉, epoch을 증가한다는 것은 전체 데이터 셋을 몇 번 돌릴 것이냐 를 결정하는 것이고, \n",
    "- batch_size는 backpropagation을 몇 개의 size로 돌릴 것이냐? 를 결정하는 이야기다.\n",
    "\"\"\"\n",
    "train_history = model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=2)\n",
    "train_history = train_history.history # epoch마다 변화한 loss, metric\n",
    "\n",
    "# 아래 세 줄은 필요없는 코드인데, 그래도 이후에 사용될 수 있어서 일단 넣어둠. \n",
    "y_predict = model.predict_classes(x_train, verbose=0)\n",
    "y_true = [ np.argmax(y) for y in y_train]\n",
    "accuracy = np.sum([y_comp[0]==y_comp[1] for y_comp in zip(y_predict, y_true)])\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 67s - loss: 0.6176 - categorical_accuracy: 0.8340    \n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 56s - loss: 0.1475 - categorical_accuracy: 0.9559    \n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 53s - loss: 0.0870 - categorical_accuracy: 0.9741    \n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 51s - loss: 0.0662 - categorical_accuracy: 0.9798    \n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 51s - loss: 0.0509 - categorical_accuracy: 0.9848    \n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 43s - loss: 0.0450 - categorical_accuracy: 0.9862    \n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 59s - loss: 0.0397 - categorical_accuracy: 0.9879    \n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 50s - loss: 0.0351 - categorical_accuracy: 0.9890    \n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 47s - loss: 0.0300 - categorical_accuracy: 0.9910    \n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 49s - loss: 0.0290 - categorical_accuracy: 0.9908    \n",
      "train, loss and metric: [0.02223081667064911, 0.9933035714285714]\n",
      "test, loss and metric: [0.043031070519770898, 0.98821428594135108]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- keras에서는 Y를 one_hot vector로 바꾸어 사용해야 함\n",
    "- X의 값들이 0과 256 사이에 분포해있으므로, 이를 0과 1.0 사이로 움직임 \n",
    "- train, test set으로 구분하여 진행\n",
    "\"\"\"\n",
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(64, (5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 504s - loss: 0.5980 - categorical_accuracy: 0.8100   \n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 481s - loss: 0.0938 - categorical_accuracy: 0.9699   \n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 462s - loss: 0.0650 - categorical_accuracy: 0.9795   \n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 478s - loss: 0.0479 - categorical_accuracy: 0.9854   \n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 462s - loss: 0.0354 - categorical_accuracy: 0.9884   \n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 438s - loss: 0.0271 - categorical_accuracy: 0.9913   \n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 416s - loss: 0.0263 - categorical_accuracy: 0.9912   \n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 442s - loss: 0.0209 - categorical_accuracy: 0.9929   \n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 474s - loss: 0.0223 - categorical_accuracy: 0.9922   \n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 521s - loss: 0.0130 - categorical_accuracy: 0.9958   \n",
      "train, loss and metric: [0.010848363778620427, 0.99648809523809523]\n",
      "test, loss and metric: [0.042763131727420148, 0.98869047630400886]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33600/33600 [==============================] - 526s - loss: 0.5985 - categorical_accuracy: 0.8046   \n",
      "Epoch 2/20\n",
      "33600/33600 [==============================] - 482s - loss: 0.0885 - categorical_accuracy: 0.9729   \n",
      "Epoch 3/20\n",
      "33600/33600 [==============================] - 10337s - loss: 0.0556 - categorical_accuracy: 0.9826  \n",
      "Epoch 4/20\n",
      "33600/33600 [==============================] - 2295s - loss: 0.0458 - categorical_accuracy: 0.9856  \n",
      "Epoch 5/20\n",
      "33600/33600 [==============================] - 396s - loss: 0.0366 - categorical_accuracy: 0.9883   \n",
      "Epoch 6/20\n",
      "33600/33600 [==============================] - 384s - loss: 0.0277 - categorical_accuracy: 0.9914   \n",
      "Epoch 7/20\n",
      "33600/33600 [==============================] - 383s - loss: 0.0215 - categorical_accuracy: 0.9934   \n",
      "Epoch 8/20\n",
      "33600/33600 [==============================] - 379s - loss: 0.0216 - categorical_accuracy: 0.9925   \n",
      "Epoch 9/20\n",
      "33600/33600 [==============================] - 363s - loss: 0.0192 - categorical_accuracy: 0.9940   \n",
      "Epoch 10/20\n",
      "33600/33600 [==============================] - 371s - loss: 0.0147 - categorical_accuracy: 0.9947   \n",
      "Epoch 11/20\n",
      "33600/33600 [==============================] - 370s - loss: 0.0129 - categorical_accuracy: 0.9964   \n",
      "Epoch 12/20\n",
      "33600/33600 [==============================] - 377s - loss: 0.0110 - categorical_accuracy: 0.9965   \n",
      "Epoch 13/20\n",
      "33600/33600 [==============================] - 416s - loss: 0.0126 - categorical_accuracy: 0.9963   \n",
      "Epoch 14/20\n",
      "33600/33600 [==============================] - 391s - loss: 0.0080 - categorical_accuracy: 0.9976   \n",
      "Epoch 15/20\n",
      "33600/33600 [==============================] - 384s - loss: 0.0070 - categorical_accuracy: 0.9977   \n",
      "Epoch 16/20\n",
      "33600/33600 [==============================] - 386s - loss: 0.0077 - categorical_accuracy: 0.9974   \n",
      "Epoch 17/20\n",
      "33600/33600 [==============================] - 375s - loss: 0.0080 - categorical_accuracy: 0.9975   \n",
      "Epoch 18/20\n",
      "33600/33600 [==============================] - 387s - loss: 0.0057 - categorical_accuracy: 0.9981   \n",
      "Epoch 19/20\n",
      "33600/33600 [==============================] - 374s - loss: 0.0059 - categorical_accuracy: 0.9980   \n",
      "Epoch 20/20\n",
      "33600/33600 [==============================] - 381s - loss: 0.0065 - categorical_accuracy: 0.9978   \n",
      "train, loss and metric: [0.010528954193425652, 0.9966666666666667]\n",
      "test, loss and metric: [0.047946910524873862, 0.98821428594135108]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33600/33600 [==============================] - 470s - loss: 0.9917 - categorical_accuracy: 0.6965   \n",
      "Epoch 2/20\n",
      "33600/33600 [==============================] - 433s - loss: 0.1283 - categorical_accuracy: 0.9590   \n",
      "Epoch 3/20\n",
      "33600/33600 [==============================] - 423s - loss: 0.0739 - categorical_accuracy: 0.9766   \n",
      "Epoch 4/20\n",
      "33600/33600 [==============================] - 424s - loss: 0.0603 - categorical_accuracy: 0.9813   \n",
      "Epoch 5/20\n",
      "33600/33600 [==============================] - 417s - loss: 0.0408 - categorical_accuracy: 0.9873   \n",
      "Epoch 6/20\n",
      "33600/33600 [==============================] - 421s - loss: 0.0317 - categorical_accuracy: 0.9900   \n",
      "Epoch 7/20\n",
      "33600/33600 [==============================] - 415s - loss: 0.0342 - categorical_accuracy: 0.9884   \n",
      "Epoch 8/20\n",
      "33600/33600 [==============================] - 427s - loss: 0.0281 - categorical_accuracy: 0.9906   \n",
      "Epoch 9/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0219 - categorical_accuracy: 0.9930   \n",
      "Epoch 10/20\n",
      "33600/33600 [==============================] - 407s - loss: 0.0171 - categorical_accuracy: 0.9949   \n",
      "Epoch 11/20\n",
      "33600/33600 [==============================] - 414s - loss: 0.0154 - categorical_accuracy: 0.9950   \n",
      "Epoch 12/20\n",
      "33600/33600 [==============================] - 408s - loss: 0.0155 - categorical_accuracy: 0.9948   \n",
      "Epoch 13/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0168 - categorical_accuracy: 0.9941   \n",
      "Epoch 14/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0177 - categorical_accuracy: 0.9938   \n",
      "Epoch 15/20\n",
      "33600/33600 [==============================] - 409s - loss: 0.0106 - categorical_accuracy: 0.9960   \n",
      "Epoch 16/20\n",
      "33600/33600 [==============================] - 415s - loss: 0.0199 - categorical_accuracy: 0.9936   \n",
      "Epoch 17/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0167 - categorical_accuracy: 0.9948   \n",
      "Epoch 18/20\n",
      "33600/33600 [==============================] - 406s - loss: 0.0072 - categorical_accuracy: 0.9976   \n",
      "Epoch 19/20\n",
      "33600/33600 [==============================] - 413s - loss: 0.0183 - categorical_accuracy: 0.9940   \n",
      "Epoch 20/20\n",
      "33600/33600 [==============================] - 408s - loss: 0.0085 - categorical_accuracy: 0.9975   \n",
      "train, loss and metric: [0.0082378579511229576, 0.99711309523809522]\n",
      "test, loss and metric: [0.043590596066787841, 0.98916666689373201]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "33600/33600 [==============================] - 490s - loss: 1.5498 - categorical_accuracy: 0.4420   \n",
      "Epoch 2/40\n",
      "33600/33600 [==============================] - 443s - loss: 0.3228 - categorical_accuracy: 0.9034   \n",
      "Epoch 3/40\n",
      "33600/33600 [==============================] - 464s - loss: 0.1749 - categorical_accuracy: 0.9479   \n",
      "Epoch 4/40\n",
      "33600/33600 [==============================] - 434s - loss: 0.1344 - categorical_accuracy: 0.9611   \n",
      "Epoch 5/40\n",
      "33600/33600 [==============================] - 442s - loss: 0.1097 - categorical_accuracy: 0.9669   \n",
      "Epoch 6/40\n",
      "33600/33600 [==============================] - 431s - loss: 0.0963 - categorical_accuracy: 0.9713   \n",
      "Epoch 7/40\n",
      "33600/33600 [==============================] - 438s - loss: 0.0802 - categorical_accuracy: 0.9760   \n",
      "Epoch 8/40\n",
      "33600/33600 [==============================] - 426s - loss: 0.0757 - categorical_accuracy: 0.9770   \n",
      "Epoch 9/40\n",
      "33600/33600 [==============================] - 426s - loss: 0.0649 - categorical_accuracy: 0.9802   \n",
      "Epoch 10/40\n",
      "33600/33600 [==============================] - 421s - loss: 0.0590 - categorical_accuracy: 0.9819   \n",
      "Epoch 11/40\n",
      "33600/33600 [==============================] - 420s - loss: 0.0633 - categorical_accuracy: 0.9815   \n",
      "Epoch 12/40\n",
      "33600/33600 [==============================] - 414s - loss: 0.0520 - categorical_accuracy: 0.9844   \n",
      "Epoch 13/40\n",
      "33600/33600 [==============================] - 416s - loss: 0.0527 - categorical_accuracy: 0.9847   \n",
      "Epoch 14/40\n",
      "33600/33600 [==============================] - 416s - loss: 0.0462 - categorical_accuracy: 0.9862   \n",
      "Epoch 15/40\n",
      "33600/33600 [==============================] - 431s - loss: 0.0425 - categorical_accuracy: 0.9878   \n",
      "Epoch 16/40\n",
      "33600/33600 [==============================] - 425s - loss: 0.0425 - categorical_accuracy: 0.9876   \n",
      "Epoch 17/40\n",
      "33600/33600 [==============================] - 434s - loss: 0.0398 - categorical_accuracy: 0.9877   \n",
      "Epoch 18/40\n",
      "33600/33600 [==============================] - 434s - loss: 0.0369 - categorical_accuracy: 0.9893   \n",
      "Epoch 19/40\n",
      "33600/33600 [==============================] - 419s - loss: 0.0339 - categorical_accuracy: 0.9898   \n",
      "Epoch 20/40\n",
      "33600/33600 [==============================] - 434s - loss: 0.0327 - categorical_accuracy: 0.9903   \n",
      "Epoch 21/40\n",
      "33600/33600 [==============================] - 431s - loss: 0.0333 - categorical_accuracy: 0.9900   \n",
      "Epoch 22/40\n",
      "33600/33600 [==============================] - 430s - loss: 0.0304 - categorical_accuracy: 0.9909   \n",
      "Epoch 23/40\n",
      "33600/33600 [==============================] - 426s - loss: 0.0326 - categorical_accuracy: 0.9903   \n",
      "Epoch 24/40\n",
      "33600/33600 [==============================] - 420s - loss: 0.0281 - categorical_accuracy: 0.9915   \n",
      "Epoch 25/40\n",
      "33600/33600 [==============================] - 422s - loss: 0.0270 - categorical_accuracy: 0.9917   \n",
      "Epoch 26/40\n",
      "33600/33600 [==============================] - 427s - loss: 0.0253 - categorical_accuracy: 0.9921   \n",
      "Epoch 27/40\n",
      "33600/33600 [==============================] - 431s - loss: 0.0318 - categorical_accuracy: 0.9902   \n",
      "Epoch 28/40\n",
      "33600/33600 [==============================] - 435s - loss: 0.0265 - categorical_accuracy: 0.9926   \n",
      "Epoch 29/40\n",
      "33600/33600 [==============================] - 426s - loss: 0.0286 - categorical_accuracy: 0.9918   \n",
      "Epoch 30/40\n",
      "33600/33600 [==============================] - 432s - loss: 0.0213 - categorical_accuracy: 0.9932   \n",
      "Epoch 31/40\n",
      "33600/33600 [==============================] - 437s - loss: 0.0228 - categorical_accuracy: 0.9931   \n",
      "Epoch 32/40\n",
      "33600/33600 [==============================] - 436s - loss: 0.0281 - categorical_accuracy: 0.9914   \n",
      "Epoch 33/40\n",
      "33600/33600 [==============================] - 443s - loss: 0.0208 - categorical_accuracy: 0.9944   \n",
      "Epoch 34/40\n",
      "33600/33600 [==============================] - 414s - loss: 0.0199 - categorical_accuracy: 0.9939   \n",
      "Epoch 35/40\n",
      "33600/33600 [==============================] - 421s - loss: 0.0227 - categorical_accuracy: 0.9937   \n",
      "Epoch 36/40\n",
      "33600/33600 [==============================] - 426s - loss: 0.0165 - categorical_accuracy: 0.9952   \n",
      "Epoch 37/40\n",
      "33600/33600 [==============================] - 428s - loss: 0.0196 - categorical_accuracy: 0.9938   \n",
      "Epoch 38/40\n",
      "33600/33600 [==============================] - 424s - loss: 0.0181 - categorical_accuracy: 0.9940   \n",
      "Epoch 39/40\n",
      "33600/33600 [==============================] - 420s - loss: 0.0195 - categorical_accuracy: 0.9941   \n",
      "Epoch 40/40\n",
      "33600/33600 [==============================] - 421s - loss: 0.0176 - categorical_accuracy: 0.9946   \n",
      "train, loss and metric: [0.0057180644724091206, 0.99821428571428572]\n",
      "test, loss and metric: [0.029539806916422787, 0.99190476201829458]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[1:]]\n",
    "Y = pd.get_dummies(train_df['label'])\n",
    "\n",
    "X_values = (X.values.astype(np.float64)/256.0).reshape(len(X_values), 28, 28, 1)\n",
    "Y_values = Y.values.astype(np.float32)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_values, Y_values, \n",
    "                                                    test_size = 0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation = \"relu\"),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    Dense(128, activation = \"relu\"),\n",
    "    Dense(32, activation = \"relu\"),\n",
    "    #Dropout(0.5),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), \n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "train_history = model.fit(x_train, y_train, epochs=40, batch_size=500, verbose=1)\n",
    "train_history = train_history.history\n",
    "\n",
    "loss_and_metric = model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 113s   \n"
     ]
    }
   ],
   "source": [
    "test_X_values = (test_df.values.astype(np.float64)/256.0).reshape(len(test_df), 28, 28, 1)\n",
    "test_y_pred = model.predict_classes(test_X_values)\n",
    "\n",
    "submit_df = pd.DataFrame({\"ImageId\":range(1, 1+len(test_y_pred)), \"Label\":test_y_pred})\n",
    "submit_df.to_csv('test_mnist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.DataFrame({\"ImageId\":range(1, 1+len(test_y_pred)), \"Label\":test_y_pred})\n",
    "submit_df.to_csv('test_mnist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
