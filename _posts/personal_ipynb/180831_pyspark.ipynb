{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 필요한 라이브러리를 불러옵니다. \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "## pyspark를 불러옵니다.\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미 spark context가 살아 있으면 죽이고, 있으면 그대로 사용합니다. \n",
    "## spark context는 한 번에 여러 개 돌리려면 세팅에 몇 개를 추가해야 하는것 같아요. \n",
    "## 또한 스파크 컨텍스트를 제대로 구현하려면 여기에 넘겨야 하는 값들이 많이 있습니다만, 저는 테스트만 하려고 해서 이걸 대충 세팅했습니다. \n",
    "if sc is None: \n",
    "    sc = SparkContext(master=\"local\", appName=\"first app\")\n",
    "else:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(master=\"local\", appName=\"first app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20099115371704102, 2.9489879608154297, 11.879252910614014, 25.1126070022583, 45.649672985076904, 70.58739018440247, 106.82262706756592, 158.06050515174866, 185.18903493881226, 243.1557366847992]\n",
      "[9.059906005859375e-06, 2.5483241081237793, 10.643134117126465, 25.348090887069702, 45.02895379066467, 72.11722803115845, 107.7942442893982, 145.35469794273376, 185.32872891426086, 232.6388590335846]\n",
      "[0.0002586841583251953, 0.0001728534698486328, 0.00018787384033203125, 0.00026297569274902344, 0.0004010200500488281, 0.0009829998016357422, 0.0017499923706054688, 0.026026010513305664, 0.0018031597137451172, 0.001260995864868164]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미 spark context가 살아 있으면 죽이고, 있으면 그대로 사용합니다. \n",
    "## spark context는 한 번에 여러 개 돌리려면 세팅에 몇 개를 추가해야 하는것 같아요. \n",
    "## 또한 스파크 컨텍스트를 제대로 구현하려면 여기에 넘겨야 하는 값들이 많이 있습니다만, 저는 테스트만 하려고 해서 이걸 대충 세팅했습니다. \n",
    "if sc is None: \n",
    "    sc = SparkContext(master=\"local\", appName=\"first app\")\n",
    "else:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(master=\"local\", appName=\"first app\")\n",
    "\n",
    "### peformance check \n",
    "spark_time_lst = []\n",
    "python_time_lst = []\n",
    "python_np_time_lst = []\n",
    "n_lst = [20000*i for i in range(0, 10)]\n",
    "for n in n_lst:\n",
    "    def each_k(k):\n",
    "        return 1/(16**k)*( 4/(8*k+1) - 2/(8*k+4) - 1/(8*k+5) - 1/(8*k+6))\n",
    "    ## with spark \n",
    "    start_time = time.time()\n",
    "    pi_approximated = sc.parallelize(range(0, n)).map(each_k).sum()\n",
    "    spark_time_lst.append(time.time()-start_time)\n",
    "    ## pure python\n",
    "    start_time = time.time()\n",
    "    pi_approximated = sum((each_k(k) for k in range(0, n)))\n",
    "    python_time_lst.append(time.time()-start_time)\n",
    "    ## with numpy \n",
    "    start_time = time.time()\n",
    "    pi_approximated = np.apply_along_axis(arr=np.array(range(0, 10)), func1d=each_k, axis=0).sum()\n",
    "    python_np_time_lst.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting     \n",
    "df = pd.DataFrame({\n",
    "    \"spark\":spark_time_lst.copy(), \n",
    "    \"pure python\":python_time_lst.copy(), \n",
    "    \"python with numpy\":python_np_time_lst.copy()\n",
    "}, index = [20000*i for i in range(0, 10)])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['spark'], 'ro-'), plt.plot(df['pure python'], 'bo-'), plt.plot(df['python with numpy'], 'go-')\n",
    "plt.legend(fontsize=25)\n",
    "plt.xticks([20000*i for i in range(0, 10)], [20000*i for i in range(0, 10)])\n",
    "plt.savefig('../../assets/images/markdown_img/180831_pyspark_performance_check.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
