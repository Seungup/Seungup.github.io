{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "excel_path_and_filename = \"../../../Downloads/SMEs_Scopus_2013-2017.xlsx\"\n",
    "df = pd.read_excel(excel_path_and_filename)\n",
    "df = df[['Author Keywords', 'Year', 'Abstract', 'Index Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_kwd_lst = list(df['Author Keywords'].dropna().apply(lambda s: s.strip().split(\";\")))\n",
    "auth_kwd_lst = map(lambda ks: [k.strip().lower() for k in ks], auth_kwd_lst)\n",
    "auth_kwd_lst = list(auth_kwd_lst)\n",
    "\n",
    "ind_kwd_lst = list(df['Index Keywords'].dropna().apply(lambda s: s.strip().split(\";\")))\n",
    "ind_kwd_lst = map(lambda ks: [k.strip().lower() for k in ks], ind_kwd_lst)\n",
    "ind_kwd_lst = list(ind_kwd_lst)\n",
    "\n",
    "abs_lst = list(df[df['Abstract']!=\"[No abstract available]\"]['Abstract'])\n",
    "abs_lst = map(lambda s: s.strip().split(\" \"), abs_lst)\n",
    "abs_lst = map(lambda ws: [k.strip().lower() for k in ws] , abs_lst)\n",
    "abs_lst = list(abs_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Author Keywords\n",
      "0  [csfs, factor analysis, indian manufacturing s...\n",
      "1  [energy recovery, muzzle arc, pulsed supply, r...\n",
      "2  [internationalization, market research, organi...\n",
      "3   [contractors, entrepreneurship, indonesia, smes]\n",
      "4  [bpgm-sme, improved ukf, multi-target tracking...\n",
      "original unique keyword len: 24200\n",
      "after filtering, unique keyword len: 1337\n"
     ]
    }
   ],
   "source": [
    "from inflection import singularize \n",
    "import nltk\n",
    "import difflib\n",
    "from collections import Counter\n",
    "import itertools \n",
    "\n",
    "def total_count(input_df, column_name='Author Keywords'):\n",
    "    # 'Author Keywords' or 'Noun Phrases'\n",
    "    r = itertools.chain.from_iterable(input_df[column_name])\n",
    "    r = Counter(r).most_common()\n",
    "    return pd.DataFrame(r, columns=[column_name, 'count'])\n",
    "def make_kwd_change_dict(l_of_l, non_app_keys):\n",
    "    kwd_counter = itertools.chain.from_iterable(list(auth_kwd_df['Author Keywords']))\n",
    "    kwd_counter = Counter(total_c).most_common()\n",
    "    kwd_counter = {w:c for w, c in kwd_counter}\n",
    "\n",
    "    kwd_changed_dict = {}\n",
    "    for w1 in sorted(kwd_counter.keys()):\n",
    "        for w2 in sorted(kwd_counter.keys()):\n",
    "            if w1 < w2:# 중복으로 계산하는 것을 피하기 위함\n",
    "                sim_v = difflib.SequenceMatcher(None,w1, w2).ratio()\n",
    "                if sim_v >= 0.90:\n",
    "                    if kwd_counter[w1] >= kwd_counter[w2]:\n",
    "                        kwd_changed_dict[w2]=w1\n",
    "                        #print(\"{} ==> {}\".format(w2, w1))\n",
    "                    else:\n",
    "                        #print(\"{} ==> {}\".format(w1, w2))\n",
    "                        kwd_changed_dict[w1]=w2\n",
    "    \"\"\"\n",
    "    적합하지 않은 key는 제외함\n",
    "    \"\"\"\n",
    "    new_kwd_changed_dict = filter(lambda k: True if k[0] not in non_app_keys else False, kwd_changed_dict.items())\n",
    "    new_kwd_changed_dict = {k:v for k, v in new_kwd_changed_dict}\n",
    "    \"\"\"\n",
    "    아래와 같은 상황이 발생할 수 있다. 결국 D 또한 B로 변환되면 되는데, 변환되기 위해서는 \n",
    "    D ==> C, C==>A, A==B 의 세번 의 과정을 거쳐야 하는 것. 이러한 transivity를 dictionary에서 제외해준다. \n",
    "\n",
    "    A: small medium enterprise\n",
    "    B: small and medium enterprise\n",
    "    C: small medium enterprise sme\n",
    "    D: small medium enterprises sme\n",
    "\n",
    "    A ==> B\n",
    "    C ==> A\n",
    "    D ==> C\n",
    "    \"\"\"\n",
    "    non_transvitiy_kwd_dict = {}\n",
    "    for k, v in new_kwd_changed_dict.items():\n",
    "        while v in new_kwd_changed_dict.keys():\n",
    "            v = new_kwd_changed_dict[v]\n",
    "        non_transvitiy_kwd_dict[k] = v\n",
    "    return non_transvitiy_kwd_dict\n",
    "def filtering_auth_kwds(input_df,column_name='Author Keywords', above_n=3):\n",
    "    print(\"original unique keyword len: {}\".format(\n",
    "        len(set(itertools.chain.from_iterable(list(input_df[column_name]))))\n",
    "    ))\n",
    "    input_df[column_name] = input_df[column_name].apply(lambda ks: [k.strip().lower() for k in ks])\n",
    "    # edge를 만들때 중복을 방지하기 위해서 sorting해둔다. \n",
    "    input_df[column_name] = input_df[column_name].apply(lambda l: sorted(list(set(l))))\n",
    "    \"\"\"\n",
    "    특수문자 삭제: 0-9 이거나, 'a' - 'z'가 아니면 다 삭제. \n",
    "    \"\"\"\n",
    "    def replace_sp_chr(input_s):\n",
    "        return \"\".join(map(lambda c: c if 'a'<=c and c<='z' else c if '0'<=c and c<='9'else \" \", input_s)).strip()\n",
    "    def remove_double_space(input_s):\n",
    "        while \"  \" in input_s:\n",
    "            input_s = input_s.replace(\"  \", \" \")\n",
    "        return input_s\n",
    "    input_df[column_name] = input_df[column_name].apply(\n",
    "        lambda ks: list(map(\n",
    "            lambda k: remove_double_space(replace_sp_chr(k)), ks)))\n",
    "    \"\"\"\n",
    "    단수 복수 처리: singularized 가 이미 키워드 세트에 포함되어 있을때에만 변형\n",
    "    \"\"\"\n",
    "    all_kwd_set = set(itertools.chain.from_iterable(list(input_df[column_name])))\n",
    "    to_singular_dict = {}\n",
    "    for kwd in all_kwd_set:\n",
    "        singularized_kwd = singularize(kwd)\n",
    "        if singularized_kwd !=kwd and singularized_kwd in all_kwd_set:\n",
    "            to_singular_dict[kwd] = singularized_kwd\n",
    "    input_df[column_name] = input_df[column_name].apply(\n",
    "        lambda ks: list(map(\n",
    "            lambda k: to_singular_dict[k] if k in to_singular_dict.keys() else k, ks\n",
    "        ))\n",
    "    )\n",
    "    \"\"\"\n",
    "    형태에 따른 키워드 유사도를 평가하여 변환한다. \n",
    "    적합하지 않은 키워드는 아래에서 직접 넣어주는 것이 필요함\n",
    "    \"\"\"\n",
    "    non_app_keys = ['lean production', 'coopetition'] ## 여기에 직접 넣어줌.\n",
    "    kwd_change_dict = make_kwd_change_dict(list(input_df[column_name]), non_app_keys)\n",
    "    input_df[column_name] = input_df[column_name].apply(\n",
    "        lambda ks: list(map(\n",
    "            lambda k: kwd_change_dict[k] if k in kwd_change_dict.keys() else k, ks\n",
    "        ))\n",
    "    )\n",
    "    \"\"\"\n",
    "    개별 node가 전체에서 1번 밖에 등장하지 않는 경우도 많은데, 이를 모두 고려해서 분석을 하면, 효율적이지 못한 계산이 된다. \n",
    "    따라서, 빈도가 일정 이상을 넘는 경우에 대해서만 고려하여 df를 수정하는 것이 필요하다. \n",
    "    \"\"\"\n",
    "    filtered_kwds = total_count(input_df, column_name=column_name)\n",
    "    filtered_kwds = set(filtered_kwds[filtered_kwds['count']>=above_n][column_name])\n",
    "    input_df[column_name] = input_df[column_name].apply(lambda ks: list(filter(lambda k: True if k in filtered_kwds else False, ks)))\n",
    "    \"\"\"\n",
    "    word embedding 등 다른 데이터 전처리가 필요하다면 여기서 처리하는 것이 좋음. \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    리스트 내부 중복 삭제\n",
    "    \"\"\"\n",
    "    input_df[column_name] = input_df[column_name].apply(\n",
    "        lambda ks: list(set(ks))\n",
    "    )\n",
    "    \"\"\"\n",
    "    검색 키워드의 중심에서 거리상으로 측정했을때, 가장 먼 shortest path를 제외하는 등으로의 방식으로도 수행할 수 있지 않을까? \n",
    "    \"\"\"\n",
    "    print(\"after filtering, unique keyword len: {}\".format(\n",
    "        len(set(itertools.chain.from_iterable(list(input_df[column_name]))))\n",
    "    ))\n",
    "    #print(input_df.head())\n",
    "    return input_df# 사실 굳이 return을 쓸 필요가 없음. 이미 내부에서 다 바꿔줌. \n",
    "auth_kwd_lst = list(df['Author Keywords'].dropna().apply(lambda s: s.strip().split(\";\")))\n",
    "auth_kwd_lst = map(lambda ks: [k.strip().lower() for k in ks], auth_kwd_lst)\n",
    "auth_kwd_lst = list(auth_kwd_lst)\n",
    "auth_kwd_df = pd.DataFrame({'Author Keywords':auth_kwd_lst})\n",
    "print(auth_kwd_df.head())\n",
    "auth_kwd_df = filtering_auth_kwds(auth_kwd_df, column_name='Author Keywords', above_n=5)\n",
    "#print(auth_kwd_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small medium enterprise sme ==> small medium enterprises sme\n",
      "organizational culture ==> organisational culture\n",
      "organizational performance ==> organisational performance\n",
      "small and medium sized enterprises sme ==> small and medium sized enterprise sme\n",
      "structural equation modeling ==> structural equation modelling\n",
      "small medium enterprises sme ==> small medium enterprise sme\n",
      "small and medium enterprise sme ==> small and medium enterprises sme\n",
      "small and medium sized enterprise ==> small and medium size enterprise\n",
      "smes performance ==> sme performance\n",
      "small and medium sized enterprise sme ==> small and medium sized enterprises sme\n",
      "organisational performance ==> organizational performance\n",
      "small and medium enterprises sme ==> small and medium enterprise sme\n",
      "structural equation modelling ==> structural equation modeling\n",
      "organisational culture ==> organizational culture\n",
      "small and medium size enterprise ==> small and medium sized enterprise\n",
      "sme performance ==> smes performance\n"
     ]
    }
   ],
   "source": [
    "w_set = set(itertools.chain.from_iterable(list(auth_kwd_df['Author Keywords'])))\n",
    "for w1 in w_set:\n",
    "    for w2 in w_set:\n",
    "        if w1 != w2 and len(w1) >=5 and len(w2) >=5:# 그냥 최소한 5개는 되어야 하지 않나 싶어서...\n",
    "            if difflib.SequenceMatcher(None, w1, w2).ratio() > 0.95:\n",
    "                print(\"{} ==> {}\".format(w1, w2))\n",
    "\n",
    "# 전체 값에 대해서 플로팅을 해보면 갑자기 확 주러드는 거라든가, 그런게 있지 않을까? \n",
    "# plottingg 해보면 어떻게 되나? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competence ==> competency\n",
      "corporate social responsibility csr ==> corporate social responsibility\n",
      "enterprise resource planning erp ==> enterprise resource planning\n",
      "globalisation ==> globalization\n",
      "internationalisation ==> internationalization\n",
      "sme internationalization ==> internationalization\n",
      "modelling ==> modeling\n",
      "product development ==> new product development\n",
      "organisational culture ==> organizational culture\n",
      "organisational performance ==> organizational performance\n",
      "shape memory effect sme ==> shape memory effect\n",
      "small and medium enterprise sme ==> small and medium sized enterprise\n",
      "small and medium enterprises sme ==> small and medium sized enterprise\n",
      "small and medium scale enterprise ==> small and medium sized enterprise\n",
      "small and medium size enterprise ==> small and medium sized enterprise\n",
      "small and medium enterprise ==> small and medium sized enterprise\n",
      "small medium enterprise ==> small and medium sized enterprise\n",
      "small to medium enterprise ==> small and medium sized enterprise\n",
      "small medium enterprise sme ==> small and medium sized enterprise\n",
      "small medium enterprises sme ==> small and medium sized enterprise\n",
      "small and medium sized enterprise sme ==> small and medium sized enterprise\n",
      "small and medium sized enterprises sme ==> small and medium sized enterprise\n",
      "small to medium sized enterprise ==> small and medium sized enterprise\n",
      "small to medium sized enterprises sme ==> small and medium sized enterprise\n",
      "sme performance ==> smes performance\n",
      "structural equation modeling ==> structural equation modelling\n",
      "superconducting magnetic energy storage ==> superconducting magnetic energy storage smes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'competence': 'competency',\n",
       " 'corporate social responsibility csr': 'corporate social responsibility',\n",
       " 'enterprise resource planning erp': 'enterprise resource planning',\n",
       " 'globalisation': 'globalization',\n",
       " 'internationalisation': 'internationalization',\n",
       " 'modelling': 'modeling',\n",
       " 'organisational culture': 'organizational culture',\n",
       " 'organisational performance': 'organizational performance',\n",
       " 'product development': 'new product development',\n",
       " 'shape memory effect sme': 'shape memory effect',\n",
       " 'small and medium enterprise': 'small and medium sized enterprise',\n",
       " 'small and medium enterprise sme': 'small and medium sized enterprise',\n",
       " 'small and medium enterprises sme': 'small and medium sized enterprise',\n",
       " 'small and medium scale enterprise': 'small and medium sized enterprise',\n",
       " 'small and medium size enterprise': 'small and medium sized enterprise',\n",
       " 'small and medium sized enterprise sme': 'small and medium sized enterprise',\n",
       " 'small and medium sized enterprises sme': 'small and medium sized enterprise',\n",
       " 'small medium enterprise': 'small and medium sized enterprise',\n",
       " 'small medium enterprise sme': 'small and medium sized enterprise',\n",
       " 'small medium enterprises sme': 'small and medium sized enterprise',\n",
       " 'small to medium enterprise': 'small and medium sized enterprise',\n",
       " 'small to medium sized enterprise': 'small and medium sized enterprise',\n",
       " 'small to medium sized enterprises sme': 'small and medium sized enterprise',\n",
       " 'sme internationalization': 'internationalization',\n",
       " 'sme performance': 'smes performance',\n",
       " 'structural equation modeling': 'structural equation modelling',\n",
       " 'superconducting magnetic energy storage': 'superconducting magnetic energy storage smes'}"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "키워드간를 변경할 때는 키워드의 수 또한 중요하게 고려해야 하지 않나, 더 많은 놈으로 변경한다. \n",
    "\"\"\"\n",
    "import nltk\n",
    "import difflib\n",
    "from collections import Counter\n",
    "import itertools \n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "sequence matcher ratio의 전체 분포를 한 번 보자 .\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "plot the word sequence similarity distribution \n",
    "\n",
    "a = (difflib.SequenceMatcher(None, w1, w2).ratio() for w1 in kwd_counter.keys() for w2 in kwd_counter.keys())\n",
    "# draw boxplot required \n",
    "a = filter(lambda x: True if x!=1.0 else False, a)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(sorted(a), 'ro')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "def make_kwd_change_dict(l_of_l, non_app_keys):\n",
    "    kwd_counter = itertools.chain.from_iterable(list(auth_kwd_df['Author Keywords']))\n",
    "    kwd_counter = Counter(total_c).most_common()\n",
    "    kwd_counter = {w:c for w, c in kwd_counter}\n",
    "\n",
    "    kwd_changed_dict = {}\n",
    "    for w1 in sorted(kwd_counter.keys()):\n",
    "        for w2 in sorted(kwd_counter.keys()):\n",
    "            if w1 < w2:# 중복으로 계산하는 것을 피하기 위함\n",
    "                sim_v = difflib.SequenceMatcher(None,w1, w2).ratio()\n",
    "                if sim_v >= 0.90:\n",
    "                    if kwd_counter[w1] >= kwd_counter[w2]:\n",
    "                        kwd_changed_dict[w2]=w1\n",
    "                        #print(\"{} ==> {}\".format(w2, w1))\n",
    "                    else:\n",
    "                        #print(\"{} ==> {}\".format(w1, w2))\n",
    "                        kwd_changed_dict[w1]=w2\n",
    "    \"\"\"\n",
    "    적합하지 않은 key는 제외함\n",
    "    \"\"\"\n",
    "    new_kwd_changed_dict = filter(lambda k: True if k[0] not in non_app_keys else False, kwd_changed_dict.items())\n",
    "    new_kwd_changed_dict = {k:v for k, v in new_kwd_changed_dict}\n",
    "    \"\"\"\n",
    "    아래와 같은 상황이 발생할 수 있다. 결국 D 또한 B로 변환되면 되는데, 변환되기 위해서는 \n",
    "    D ==> C, C==>A, A==B 의 세번 의 과정을 거쳐야 하는 것. 이러한 transivity를 dictionary에서 제외해준다. \n",
    "\n",
    "    A: small medium enterprise\n",
    "    B: small and medium enterprise\n",
    "    C: small medium enterprise sme\n",
    "    D: small medium enterprises sme\n",
    "\n",
    "    A ==> B\n",
    "    C ==> A\n",
    "    D ==> C\n",
    "    \"\"\"\n",
    "    non_transvitiy_kwd_dict = {}\n",
    "    for k, v in new_kwd_changed_dict.items():\n",
    "        while v in new_kwd_changed_dict.keys():\n",
    "            v = new_kwd_changed_dict[v]\n",
    "        non_transvitiy_kwd_dict[k] = v\n",
    "\n",
    "    for k, v in non_transvitiy_kwd_dict.items():\n",
    "        print(\"{} ==> {}\".format(k, v))\n",
    "    return non_transvitiy_kwd_dict\n",
    "make_kwd_change_dict(list(auth_kwd_df['Author Keywords']), non_app_keys=['lean production', 'coopetition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competence ==> competency\n",
      "corporate social responsibility csr ==> corporate social responsibility\n",
      "enterprise resource planning erp ==> enterprise resource planning\n",
      "globalisation ==> globalization\n",
      "internationalisation ==> internationalization\n",
      "sme internationalization ==> internationalization\n",
      "modelling ==> modeling\n",
      "product development ==> new product development\n",
      "organisational culture ==> organizational culture\n",
      "organisational performance ==> organizational performance\n",
      "shape memory effect sme ==> shape memory effect\n",
      "small and medium enterprise sme ==> small and medium sized enterprise\n",
      "small and medium enterprises sme ==> small and medium sized enterprise\n",
      "small and medium scale enterprise ==> small and medium sized enterprise\n",
      "small and medium size enterprise ==> small and medium sized enterprise\n",
      "small and medium enterprise ==> small and medium sized enterprise\n",
      "small medium enterprise ==> small and medium sized enterprise\n",
      "small to medium enterprise ==> small and medium sized enterprise\n",
      "small medium enterprise sme ==> small and medium sized enterprise\n",
      "small medium enterprises sme ==> small and medium sized enterprise\n",
      "small and medium sized enterprise sme ==> small and medium sized enterprise\n",
      "small and medium sized enterprises sme ==> small and medium sized enterprise\n",
      "small to medium sized enterprise ==> small and medium sized enterprise\n",
      "small to medium sized enterprises sme ==> small and medium sized enterprise\n",
      "sme performance ==> smes performance\n",
      "structural equation modeling ==> structural equation modelling\n",
      "superconducting magnetic energy storage ==> superconducting magnetic energy storage smes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n아래와 같은 상황이 발생할 수 있다. 결국 D 또한 B로 변환되면 되는데, 변환되기 위해서는 \\nD ==> C, C==>A, A==B 의 세번 의 과정을 거쳐야 하는 것. 이러한 transivity를 dictionary에서 제외해준다. \\n\\nA: small medium enterprise\\nB: small and medium enterprise\\nC: small medium enterprise sme\\nD: small medium enterprises sme\\n\\nA ==> B\\nC ==> A\\nD ==> C \\n'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3f5a2978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "a = sorted([1,2,3,4])\n",
    "plt.figure()\n",
    "plt.plot(a, range(0, len(a)), 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.9310344827586207\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import difflib\n",
    "w1 = \"small and medium enterprise\"\n",
    "w2 = \"small and medium enterprise sme\"\n",
    "print(nltk.edit_distance(w1, w2))\n",
    "\n",
    "seq = difflib.SequenceMatcher(None,w1, w2)\n",
    "print(seq.ratio()) \n",
    "seq = difflib.SequenceMatcher(None,'ab', 'abc')\n",
    "print(seq.ratio()) \n",
    "### OUTPUT: 87.323943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "abs_lst = list(df[df['Abstract']!=\"[No abstract available]\"]['Abstract'])\n",
    "sent_lst = itertools.chain.from_iterable(map(sent_tokenize, abs_lst))\n",
    "#sent_lst = list(sent_lst)\n",
    "sent_lst = map(lambda s: s.strip().lower(), sent_lst)\n",
    "def replace_sp_chr(input_s):\n",
    "    return \"\".join(map(lambda c: c if 'a'<=c and c<='z' else c if '0'<=c and c<='9'else \" \", input_s)).strip()\n",
    "def remove_double_space(input_s):\n",
    "    while \"  \" in input_s:\n",
    "        input_s = input_s.replace(\"  \", \" \")\n",
    "    return input_s\n",
    "sent_lst = map(replace_sp_chr, sent_lst)\n",
    "sent_lst = map(remove_double_space, sent_lst)\n",
    "sent_lst = list(sent_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105427"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sent의 갯수만 10만 개. \n",
    "word2vec에 학습이야 그냥 리스트 오브 리스트로 넘기면 되는 건데, 여기서 vocabulary를 구성할지에 대한 문제가 있는거겠지?\n",
    "일단 sentence를 어떻게 쪼갤 것인가의 문제. 그냥 스페이스로 쪼개면, 어느정도 비슷하게 학습이 되기는 하는데, 그게 뭐 의미가 있나?\n",
    "아니면 저번에 noun_phrase를 쪼개준 것처럼 phrase단위로 쪼개준 다음에 \n",
    "\"\"\"\n",
    "# \n",
    "#\n",
    "len(sent_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector of man:\n",
      "[ 0.0961192   0.00437185  0.03467375  0.26454127  0.02940031  0.02288906\n",
      "  0.02179242  0.02122602 -0.04023347  0.01882667 -0.02628091  0.03881754\n",
      " -0.06698899  0.14932805 -0.0474763   0.02154444 -0.14287125  0.21027721\n",
      " -0.02145271 -0.02302506  0.03378444  0.0153384   0.07713746  0.15382759\n",
      "  0.12019698 -0.1113359  -0.0593212  -0.19330981  0.00104285 -0.08455317\n",
      "  0.01631272  0.05338057 -0.05085417 -0.00311475  0.00962412  0.12170225\n",
      " -0.09533398  0.09420326 -0.03652238 -0.13428254 -0.13235193  0.12731819\n",
      "  0.06866247  0.01878783  0.07961938 -0.02594641  0.1502616  -0.09824947\n",
      "  0.11498301  0.03205505 -0.13049409 -0.11235102  0.18670972  0.20496006\n",
      "  0.11508316 -0.08224259  0.07353425  0.05081542  0.07457628 -0.06560689\n",
      "  0.06979878  0.00345675  0.15629113  0.11683072 -0.07166454 -0.08964456\n",
      "  0.0704189   0.01116451 -0.07762457 -0.15145063 -0.23548466  0.17004918\n",
      " -0.08410266  0.05517555 -0.12813565  0.08467376  0.15465082 -0.01966195\n",
      " -0.126249    0.05783306 -0.10403755 -0.01165899 -0.19907828  0.07758326\n",
      " -0.05870183 -0.04015943  0.01824598 -0.01141299  0.18254025 -0.01973889\n",
      " -0.06274197 -0.03590406 -0.10662002  0.10698724  0.0145605  -0.12283614\n",
      " -0.0986573   0.08289269  0.02273848 -0.09652561]\n",
      "similarity of actor and actress:\n",
      "0.871707394306\n",
      "similarity of man and woman:\n",
      "0.907711044147\n",
      "\n",
      "[('woman', 0.9077110886573792), ('girl', 0.8656805157661438), ('boy', 0.8102990984916687), ('child', 0.8080374598503113), ('killer', 0.8013476729393005), ('doctor', 0.7803214192390442), ('kid', 0.7172398567199707), ('secret', 0.7040114998817444), ('person', 0.7038406729698181), ('guy', 0.6979759931564331)]\n",
      "\n",
      "[('he', 0.3257312476634979)]\n",
      "[('she', 0.3937234878540039)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences, size=100) \n",
    "# 여기서 사이즈는 vector의 크기를 말합니다. nn의 최종 layer size라고 생각해도 됩니다. \n",
    "# 뉴럴넷의 사이즈를 좀 깊게 만들어보고 싶은데, gensim에서 자동으로 해주는지 모르겠네요. \n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 \n",
    "\n",
    "print(\"vector of {}:\".format('man'))\n",
    "print(model.wv['man'])\n",
    "for w1, w2 in [('actor', 'actress'), ('man', 'woman')]:\n",
    "    print(\"similarity of {} and {}:\".format(w1, w2))\n",
    "    print(model.wv.similarity(w1, w2))\n",
    "\n",
    "print()\n",
    "print(model.wv.most_similar('man'))\n",
    "print()\n",
    "# actor + she - actress \n",
    "print(model.wv.most_similar(positive=['actor', 'she'], negative='actress', topn=1))\n",
    "# actress + he - actor\n",
    "print(model.wv.most_similar(positive=['actress', 'he'], negative='actor', topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbipartite graph를 구성해서 거기서부터, similarity를 뽑아낼 수 있지 않을까? 사실 그게 일종의 워드 임베딩 아닌가. \\n원래 워드 임베딩은 개별 단어를 중심으로 하는 건데, 여기는 지금 그렇게 구성할 수 가 없을 것 같음. \\n\\nauthor kwd ==> index kwd \\nauthor kwd ==> abstrac noun or verb (or both)\\n\\n의 형태로 bipartite하게 구성할 수 있는데 이렇게 구성하여, structure equivalence를 만들 수 있지 않을까? \\n\\n아무튼 일단 워드 임베딩을 조금만 더 파서 정리는 마저 하는 것이 필요함. \\n'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bipartite graph를 구성해서 거기서부터, similarity를 뽑아낼 수 있지 않을까? 사실 그게 일종의 워드 임베딩 아닌가. \n",
    "원래 워드 임베딩은 개별 단어를 중심으로 하는 건데, 여기는 지금 그렇게 구성할 수 가 없을 것 같음. \n",
    "\n",
    "author kwd ==> index kwd \n",
    "author kwd ==> abstrac noun or verb (or both)\n",
    "\n",
    "의 형태로 bipartite하게 구성할 수 있는데 이렇게 구성하여, structure equivalence를 만들 수 있지 않을까? \n",
    "\n",
    "아무튼 일단 워드 임베딩을 조금만 더 파서 정리는 마저 하는 것이 필요함. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-fb019c283242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'am'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'good'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m bigram_transformer = gensim.models.phrases.Phraser([\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_transformer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, phrases_model)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'threshold'"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "ss = ['I', 'am', 'a', 'good', 'boy'] \n",
    "bigram_transformer = gensim.models.phrases.Phraser([\n",
    "    ss\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'small and medium enterprises' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-015b962efa43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 학습 완료 후, 필요없는 메모리 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"small and medium enterprises\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'small and medium enterprises' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(abs_lst)\n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 \n",
    "#model.most_similar(\"small and medium enterprises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_counter = Counter(itertools.chain.from_iterable(auth_kwd_lst)).most_common()\n",
    "count1_w = list(filter(lambda x: True if x[1]==5 else False, auth_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proximity\n",
      "[('trust,', 0.7394723892211914), ('experience,', 0.7393851280212402), ('formality', 0.7343860864639282), ('embeddedness', 0.7326754927635193), (\"owner's\", 0.72593092918396), ('human,', 0.7258467674255371), ('income,', 0.720426619052887), ('structure,', 0.7192338109016418), ('knowledge)', 0.7186062932014465), ('loyalty', 0.717450737953186)]\n",
      "\n",
      "fuzzy comprehensive evaluation\n",
      "[('prosthetics', 0.9823917150497437), ('information systems security', 0.9795824289321899), ('wastewater treatment', 0.9777668118476868), ('natural language processing', 0.9777394533157349), ('credit evaluations', 0.977641761302948), ('agile methods', 0.9775588512420654), ('power cables', 0.9772118926048279), ('cost saving', 0.9771889448165894), ('drug blood level', 0.9763650298118591), ('drug bioavailability', 0.9759867787361145)]\n",
      "\n",
      "spillovers\n",
      "[('portfolios', 0.7346036434173584), ('appropriability', 0.7307323217391968), ('cross-border', 0.7283568978309631), ('breadth', 0.7144168615341187), ('mind-set', 0.712887704372406), ('tenure', 0.7062869668006897), ('inflows', 0.701845109462738), ('reallocation', 0.6988070607185364), ('equity,', 0.6935837268829346), ('involvement.', 0.6921118497848511)]\n",
      "\n",
      "sensitivity analysis\n",
      "[('software testing', 0.992296576499939), ('data acquisition', 0.9920706748962402), ('user interfaces', 0.9906312823295593), ('computer software selection and evaluation', 0.9906026721000671), ('embedded systems', 0.9890177845954895), ('petroleum engineering', 0.9889569878578186), ('best practices', 0.9889198541641235), ('energy management systems', 0.9884681701660156), ('neural networks', 0.9881827235221863), ('computer simulation', 0.9881125092506409)]\n",
      "\n",
      "waste\n",
      "[('material', 0.7799482345581055), ('traffic', 0.7637003660202026), ('equipment', 0.7552963495254517), ('electricity', 0.7420525550842285), ('transport', 0.7340760231018066), ('raw', 0.7266721725463867), ('water', 0.723881185054779), ('air', 0.7221243381500244), ('charge', 0.7171646952629089), ('energy,', 0.7137024402618408)]\n",
      "\n",
      "technological innovation capabilities\n",
      "[('medium enterprises', 0.8217032551765442), ('wood processing', 0.7786175012588501), ('industrial enterprise', 0.7584035396575928), ('financial system', 0.7444849610328674), ('corporate social responsibilities (csr)', 0.744378387928009), ('firm size', 0.7370509505271912), ('industrial investment', 0.7339873909950256), ('business development', 0.7292322516441345), ('software and hardwares', 0.7284433245658875), ('brazil', 0.7282372117042542)]\n",
      "\n",
      "enablers\n",
      "[('drivers', 0.8133851885795593), ('barriers', 0.7590547204017639), ('facilitators', 0.7429195642471313), ('obstacles', 0.722565770149231), ('motivations', 0.6505460143089294), ('weaknesses', 0.650393009185791), ('tqm', 0.6491608619689941), ('motives', 0.6373159289360046), ('autonomy', 0.6284334659576416), ('oi', 0.6266810894012451)]\n",
      "\n",
      "vikor\n",
      "[('topsis', 0.9251444339752197), ('dematel', 0.896966278553009), ('ism', 0.8885839581489563), ('mcdm', 0.8861225247383118), ('information mining', 0.8844143152236938), ('colony', 0.8795072436332703), ('togaf', 0.8790115714073181), ('pso', 0.877009391784668), ('neuro-fuzzy', 0.8761899471282959), ('industrial electronics', 0.870215117931366)]\n",
      "\n",
      "renewable energy sources\n",
      "[('superconducting fault current limiters (sfcls)', 0.9842095375061035), ('reactive power compensation', 0.9841621518135071), ('high-temperature superconductor (hts)-cables', 0.984147846698761), ('superconducting faultcurrent limiters (sfcl)', 0.9800307750701904), ('time delay', 0.9798702001571655), ('flywheel energy storage system (fess)', 0.9796547889709473), ('superconducting magnetic energies', 0.9795759320259094), ('supercapacitor', 0.9793630242347717), ('laser treatment', 0.9785982966423035), ('petroleum reservoir evaluation', 0.9783592820167542)]\n",
      "\n",
      "energy consumption\n",
      "[('environmental pollutions', 0.9723266363143921), ('industrie 4.0', 0.9721337556838989), ('manufacturing execution system', 0.9709111452102661), ('remote monitoring', 0.9705083966255188), ('fleet operations', 0.9696674942970276), ('model and simulation', 0.9694508910179138), ('service platforms', 0.9688398241996765), ('programmable logic controllers', 0.9687053561210632), ('particle swarm optimization', 0.9684544205665588), ('industrial robotics', 0.968034565448761)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for w in count1_w[:10]:\n",
    "    print(w[0])\n",
    "    try:\n",
    "        print(model.most_similar(w[0]))\n",
    "    except:\n",
    "        continue\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plot',\n",
       "  ':',\n",
       "  'two',\n",
       "  'teen',\n",
       "  'couples',\n",
       "  'go',\n",
       "  'to',\n",
       "  'a',\n",
       "  'church',\n",
       "  'party',\n",
       "  ',',\n",
       "  'drink',\n",
       "  'and',\n",
       "  'then',\n",
       "  'drive',\n",
       "  '.'],\n",
       " ['they', 'get', 'into', 'an', 'accident', '.'],\n",
       " ['one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'guys',\n",
       "  'dies',\n",
       "  ',',\n",
       "  'but',\n",
       "  'his',\n",
       "  'girlfriend',\n",
       "  'continues',\n",
       "  'to',\n",
       "  'see',\n",
       "  'him',\n",
       "  'in',\n",
       "  'her',\n",
       "  'life',\n",
       "  ',',\n",
       "  'and',\n",
       "  'has',\n",
       "  'nightmares',\n",
       "  '.'],\n",
       " ['what', \"'\", 's', 'the', 'deal', '?'],\n",
       " ['watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.'],\n",
       " ['.'],\n",
       " ['.'],\n",
       " ['critique',\n",
       "  ':',\n",
       "  'a',\n",
       "  'mind',\n",
       "  '-',\n",
       "  'fuck',\n",
       "  'movie',\n",
       "  'for',\n",
       "  'the',\n",
       "  'teen',\n",
       "  'generation',\n",
       "  'that',\n",
       "  'touches',\n",
       "  'on',\n",
       "  'a',\n",
       "  'very',\n",
       "  'cool',\n",
       "  'idea',\n",
       "  ',',\n",
       "  'but',\n",
       "  'presents',\n",
       "  'it',\n",
       "  'in',\n",
       "  'a',\n",
       "  'very',\n",
       "  'bad',\n",
       "  'package',\n",
       "  '.'],\n",
       " ['which',\n",
       "  'is',\n",
       "  'what',\n",
       "  'makes',\n",
       "  'this',\n",
       "  'review',\n",
       "  'an',\n",
       "  'even',\n",
       "  'harder',\n",
       "  'one',\n",
       "  'to',\n",
       "  'write',\n",
       "  ',',\n",
       "  'since',\n",
       "  'i',\n",
       "  'generally',\n",
       "  'applaud',\n",
       "  'films',\n",
       "  'which',\n",
       "  'attempt',\n",
       "  'to',\n",
       "  'break',\n",
       "  'the',\n",
       "  'mold',\n",
       "  ',',\n",
       "  'mess',\n",
       "  'with',\n",
       "  'your',\n",
       "  'head',\n",
       "  'and',\n",
       "  'such',\n",
       "  '(',\n",
       "  'lost',\n",
       "  'highway',\n",
       "  '&',\n",
       "  'memento',\n",
       "  ')',\n",
       "  ',',\n",
       "  'but',\n",
       "  'there',\n",
       "  'are',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'ways',\n",
       "  'of',\n",
       "  'making',\n",
       "  'all',\n",
       "  'types',\n",
       "  'of',\n",
       "  'films',\n",
       "  ',',\n",
       "  'and',\n",
       "  'these',\n",
       "  'folks',\n",
       "  'just',\n",
       "  'didn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'snag',\n",
       "  'this',\n",
       "  'one',\n",
       "  'correctly',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'have',\n",
       "  'taken',\n",
       "  'this',\n",
       "  'pretty',\n",
       "  'neat',\n",
       "  'concept',\n",
       "  ',',\n",
       "  'but',\n",
       "  'executed',\n",
       "  'it',\n",
       "  'terribly',\n",
       "  '.']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences)\n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-5f9fcd98af16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m ]\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 학습 완료 후, 필요없는 메모리 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    [\"a\", 'is', 'not', 'b'], ['b', 'is', 'c']\n",
    "]\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences)\n",
    "model.init_sims(replace=True)# 학습 완료 후, 필요없는 메모리 삭제 \n",
    "model.most_similar('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
