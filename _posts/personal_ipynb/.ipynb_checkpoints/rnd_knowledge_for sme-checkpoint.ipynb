{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make table \n",
    "\n",
    "- 아래 코드는 키워드 네트워크를 구성하고, 중요한 키워드를 테이블로 만들어서 엑셀로 만들어주는 부분\n",
    "- 그림 그리는 부분은 다음 셀에서 진행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도 완료\n",
      "전체 centrality 완료\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# 필터링을 진행한 경우 \n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from inflection import singularize \n",
    "from textblob import TextBlob\n",
    "\n",
    "\"\"\"\n",
    "centrality를 계산하는 함수들입니다. \n",
    "사실 networkx에 있는 것과 큰 차이 없는데, 그래도 제가 편하려고 몇 개는 고치고, \n",
    "이름을 return 으로 시작하는 것으로 동일화해서 저장해두었습니다. \n",
    "\"\"\"\n",
    "def return_weighted_degree_centrality(input_g, normalized=True):\n",
    "    w_d_centrality = {n:0.0 for n in input_g.nodes()}\n",
    "    for u, v, d in input_g.edges(data=True):\n",
    "        w_d_centrality[u]+=d['weight']\n",
    "        w_d_centrality[v]+=d['weight']\n",
    "    if normalized==True:\n",
    "        weighted_sum = sum(w_d_centrality.values())\n",
    "        return {k:v/weighted_sum for k, v in w_d_centrality.items()}\n",
    "    else:\n",
    "        return w_d_centrality\n",
    "def return_closeness_centrality(input_g):\n",
    "    new_g_with_distance = input_g.copy()\n",
    "    for u,v,d in new_g_with_distance.edges(data=True):\n",
    "        if 'distance' not in d:\n",
    "            d['distance'] = 1.0/d['weight']\n",
    "    return nx.closeness_centrality(new_g_with_distance, distance='distance')\n",
    "def return_betweenness_centrality(input_g):\n",
    "    return nx.betweenness_centrality(input_g, weight='weight')\n",
    "def return_pagerank(input_g):\n",
    "    return nx.pagerank(input_g, weight='weight')\n",
    "\n",
    "\"\"\"\n",
    "일종의 main 함수입니다. \n",
    "raw_df를 넘기는데, 가능하면 해당 argument에서 복사해서 넘겨주는 게 좋을 것 같습니다. 혹시나 싶어서요. \n",
    "\"\"\"\n",
    "def scopus_analysis(raw_df, outputExcelname):\n",
    "    new_df = raw_df[['Author Keywords', 'Year', 'Abstract']].dropna()\n",
    "    new_df['Author Keywords'] = new_df['Author Keywords'].apply(lambda s: s.split(\";\"))\n",
    "    new_df['Author Keywords'] = new_df['Author Keywords'].apply(lambda ks: [singularize(k).strip().lower() for k in ks])\n",
    "    new_df['Noun Phrases'] = new_df['Abstract'].apply(lambda s: TextBlob(s).noun_phrases)\n",
    "    new_df['Noun Phrases'] = new_df['Noun Phrases'].apply(lambda ks: [singularize(k).lower().strip() for k in ks])\n",
    "    # edge를 만들때 중복을 방지하기 위해서 sorting해둔다. \n",
    "    new_df['Author Keywords'] = new_df['Author Keywords'].apply(lambda l: sorted(list(set(l))))\n",
    "    new_df['Noun Phrases'] = new_df['Noun Phrases'].apply(lambda l: sorted(list(set(l))))\n",
    "    \"\"\"\n",
    "    여기 부분에서 토탈 키워드를 세었을 때, 최소한 2개 혹은 n개가 넘는 경우에 대해서만 해당 노드가 의미가 있다고 생각하고. \n",
    "    나머지 키워드는 무시하고 진행해야 노드의 수가 확연히 줄어들 수 있지 않을까? \n",
    "    \"\"\"\n",
    "    def total_count(input_df, column_name='Author Keywords'):\n",
    "        # 'Author Keywords' or 'Noun Phrases'\n",
    "        r = itertools.chain.from_iterable(input_df[column_name])\n",
    "        r = Counter(r).most_common()\n",
    "        return pd.DataFrame(r, columns=[column_name, 'count'])\n",
    "    def filtering_auth_kwds(input_df,column_name='Author Keywords', above_n=3):\n",
    "        \"\"\"\n",
    "        개별 node가 전체에서 1번 밖에 등장하지 않는 경우도 많은데, 이를 모두 고려해서 분석을 하면, 효율적이지 못한 계산이 된다. \n",
    "        따라서, 빈도가 일정 이상을 넘는 경우에 대해서만 고려하여 new_df를 수정하는 것이 필요하다. \n",
    "        \"\"\"\n",
    "        filtered_kwds = total_count(input_df)\n",
    "        filtered_kwds = set(filtered_kwds[filtered_kwds['count']>=above_n][column_name])\n",
    "        #return filtered_kwds\n",
    "        input_df[column_name] = input_df[column_name].apply(lambda ks: list(filter(lambda k: True if k in filtered_kwds else False, ks)))\n",
    "        return input_df\n",
    "    def yearly_rank(input_df, column_name='Author Keywords', until_rank_n=50):\n",
    "        r_dict = {}\n",
    "        for year, year_df in input_df.groupby('Year'):\n",
    "            r_dict[year] = list(total_count(year_df, column_name=column_name)[column_name])[:until_rank_n]\n",
    "            if len(r_dict[year])<until_rank_n:\n",
    "                for i in range(0, until_rank_n - len(r_dict[year])):\n",
    "                    r_dict[year].append(\"\")\n",
    "        return pd.DataFrame(r_dict)\n",
    "    #print(total_count('Noun Phrases'))\n",
    "    \"\"\"\n",
    "    df로부터 그래프를 만ㄷ르어서 리턴해주는 함수입니다.\n",
    "    \"\"\"\n",
    "    def make_graph(input_df, column_name='Author Keywords'):\n",
    "        # make edges: edge가 중복으로 생기지 않게 하려면, \n",
    "        def make_edges_from_lst(lst):\n",
    "            if len(lst)>1:\n",
    "                return [(lst[i], lst[j]) for i in range(0, len(lst)-1) for j in range(i+1, len(lst))]\n",
    "            else:\n",
    "                return []\n",
    "        edges = itertools.chain.from_iterable(input_df[column_name].apply(make_edges_from_lst))\n",
    "        edges = ((uv[0], uv[1], w) for uv, w in Counter(edges).most_common())\n",
    "        G = nx.Graph()\n",
    "        G.add_weighted_edges_from(edges)\n",
    "        # graph에 대한 데이터 필터링이 필요할 수 있는데. 여기서. \n",
    "        return G\n",
    "    def total_centrality(input_df, centrality_func):\n",
    "        inputG = make_graph(input_df)\n",
    "        r = sorted(centrality_func(inputG).items(), key=lambda e: e[1], reverse=True)\n",
    "        return pd.DataFrame(r, columns=['kwd', 'centrality'])\n",
    "    def yearly_centrality_rank(input_df, cent_func, column_name = 'Author Keywords', until_rank_n=50):\n",
    "        r_dict={}\n",
    "        for year, year_df in input_df.groupby(\"Year\"):\n",
    "            r_dict[year] = list(total_centrality(year_df, cent_func)['kwd'][:until_rank_n])\n",
    "            if len(r_dict[year])<until_rank_n:\n",
    "                for i in range(0, until_rank_n - len(r_dict[year])):\n",
    "                    r_dict[year].append(\"\")\n",
    "        return pd.DataFrame(r_dict)\n",
    "    \"\"\"\n",
    "    그래프 그리기 \n",
    "    \"\"\"\n",
    "    def drop_low_weighted_edge(inputG, above_weight=3):\n",
    "        rG = nx.Graph()\n",
    "        rG.add_nodes_from(inputG.nodes(data=True))\n",
    "        edges = filter(lambda e: True if e[2]['weight']>=above_weight else False, inputG.edges(data=True))\n",
    "        rG.add_edges_from(edges)\n",
    "        for n in inputG.nodes(): # neighbor가 없는 isolated node를 모두 지운다. \n",
    "            if len(list(nx.all_neighbors(rG, n)))==0:\n",
    "                rG.remove_node(n)\n",
    "            #print(n, list(nx.all_neighbors(rG, n)))\n",
    "        return rG\n",
    "\n",
    "    def draw_whole_graph(inputG, outputFileName):\n",
    "        f = plt.figure(figsize=(13,7))\n",
    "        plt.margins(x=0.1, y=0.1) # text 가 잘리는 경우가 있어서, margins을 넣음\n",
    "        pos = nx.spring_layout(inputG)\n",
    "        \"\"\"\n",
    "        - weight에 따라서 값을 0.1에서 1.0으로 스케일링 하는데, 그냥 minmax scaling 하는 것은 적합하지 않을 것 같고 \n",
    "        - 해당 데이터들이 특정한 분포를 가지고 있다고 가정하고, 그 분포에 의거해서 그림을 그려주는 게 좋을 것 같다는 생각이 드는데. \n",
    "        - 흐음. \n",
    "        \"\"\"\n",
    "        node_weight_lst = map(np.log, [n[1]['weight'] for n in inputG.nodes(data=True)])\n",
    "        edge_weight_lst = [e[2]['weight'] for e in inputG.edges(data=True)] \n",
    "        nx.draw_networkx_nodes(inputG, pos, \n",
    "                         node_size = [ n[1]['weight']*1000 for n in inputG.nodes(data=True)],\n",
    "                         alpha=1.0 )\n",
    "        \"\"\"\n",
    "        - label의 경우는 특정 node만 그릴 수 없음. 그리면 모두 그려야함. \n",
    "        \"\"\"\n",
    "        nx.draw_networkx_labels(inputG, pos, font_weight='bold', \n",
    "                                font_family='sans-serif', \n",
    "                                font_color='black', font_size=15\n",
    "                               )\n",
    "        nx.draw_networkx_edges(inputG, pos, \n",
    "                               width = [e[2]['weight'] for e in inputG.edges(data=True)], \n",
    "                               edge_color='b', alpha=0.5\n",
    "                              )\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    new_df = filtering_auth_kwds(new_df) # 빈도 n 개 이하의 키워드 삭제 \n",
    "    writer = pd.ExcelWriter(outputExcelname)\n",
    "    total_count(new_df, column_name='Author Keywords').to_excel(writer, '1. 전체 저자 키워드 빈도 상위 키워드')\n",
    "    total_count(new_df, column_name='Noun Phrases').to_excel(writer, '2. 전체 noun phrase 빈도 상위')\n",
    "    yearly_rank(new_df, column_name='Author Keywords').to_excel(writer, '3. 연도별 저자 키워드 순위 변화')\n",
    "    yearly_rank(new_df, column_name='Noun Phrases').to_excel(writer, '4. 연도별 noun phrase 순위 변화')\n",
    "    print(\"빈도 완료\")\n",
    "    total_centrality(new_df, return_weighted_degree_centrality).to_excel(writer, '5. 전체 저자 키워드 w. deg cent')\n",
    "    total_centrality(new_df, return_closeness_centrality).to_excel(writer, '6. 전체 저자 키워드 closeness cent')\n",
    "    total_centrality(new_df, return_betweenness_centrality).to_excel(writer, '7. 전체 저자 키워드 betweeness cent')\n",
    "    print(\"전체 centrality 완료\")\n",
    "    yearly_centrality_rank(new_df, return_weighted_degree_centrality).to_excel(writer, '8. 저자 키워드 연도별 w. deg cent 순위 변화')\n",
    "    yearly_centrality_rank(new_df, return_closeness_centrality).to_excel(writer, '9. 저자 키워드 연도별 close cent 순위 변화')\n",
    "    yearly_centrality_rank(new_df, return_betweenness_centrality).to_excel(writer, '91. 저자 키워드 연도별 betw cent 순위 변화')\n",
    "    yearly_centrality_rank(new_df, return_pagerank).to_excel(writer, '92. 저자 키워드 연도별 pagerank 순위 변화')\n",
    "    writer.save()\n",
    "\n",
    "excel_path_and_filename = \"../../../Downloads/SMEs_Scopus_2013-2017.xlsx\"\n",
    "df = pd.read_excel(excel_path_and_filename)\n",
    "df = df[['Author Keywords', 'Year', 'Abstract']]\n",
    "\n",
    "a = scopus_analysis(df[:500].copy(), 'simple_report_for_SME.xlsx')\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## draw network \n",
    "\n",
    "- 그림을 예쁘게 그려 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('entire_network_180508', '', 'entire_network_180508.png'), ('entire_network_180508', '', 'ego_network_sme_180508.png'), ('entire_network_180508', '', 'ego_network_innovation_180508.png'), ('entire_network_180508', '', 'ego_network_small and medium enterprise_180508.png'), ('entire_network_180508', '', 'ego_network_small and medium-sized enterprise_180508.png'), ('entire_network_180508', '', 'ego_network_entrepreneurship_180508.png'), ('entire_network_180508', '', 'ego_network_performance_180508.png'), ('entire_network_180508', '', 'ego_network_cloud computing_180508.png'), ('entire_network_180508', '', 'ego_network_knowledge management_180508.png'), ('entire_network_180508', '', 'ego_network_internationalization_180508.png'), ('entire_network_180508', '', 'ego_network_case study_180508.png'), ('entire_network_180508', '', 'ego_network_entrepreneurial orientation_180508.png'), ('entire_network_180508', '', 'ego_network_small and medium enterprises (smes)_180508.png'), ('entire_network_180508', '', 'ego_network_sustainability_180508.png'), ('entire_network_180508', '', 'ego_network_open innovation_180508.png'), ('entire_network_180508', '', 'ego_network_malaysium_180508.png'), ('entire_network_180508', '', 'ego_network_network_180508.png'), ('entire_network_180508', '', 'ego_network_manufacturing_180508.png'), ('entire_network_180508', '', 'ego_network_small- and medium-sized enterprise_180508.png'), ('entire_network_180508', '', 'ego_network_china_180508.png'), ('entire_network_180508', '', 'ego_network_firm performance_180508.png')]\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# 필터링을 진행한 경우 \n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from inflection import singularize \n",
    "from textblob import TextBlob\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "\n",
    "import graphviz\n",
    "\"\"\"\n",
    "일종의 main 함수입니다. \n",
    "raw_df를 넘기는데, 가능하면 해당 argument에서 복사해서 넘겨주는 게 좋을 것 같습니다. 혹시나 싶어서요. \n",
    "\"\"\"\n",
    "def draw_and_export_ppt(raw_df, outputPPTname):\n",
    "    def total_count(input_df, column_name='Author Keywords'):\n",
    "        # 'Author Keywords' or 'Noun Phrases'\n",
    "        r = itertools.chain.from_iterable(input_df[column_name])\n",
    "        r = Counter(r).most_common()\n",
    "        return pd.DataFrame(r, columns=[column_name, 'count'])\n",
    "    def filtering_auth_kwds(input_df,column_name='Author Keywords', above_n=3):\n",
    "        \"\"\"\n",
    "        개별 node가 전체에서 1번 밖에 등장하지 않는 경우도 많은데, 이를 모두 고려해서 분석을 하면, 효율적이지 못한 계산이 된다. \n",
    "        따라서, 빈도가 일정 이상을 넘는 경우에 대해서만 고려하여 new_df를 수정하는 것이 필요하다. \n",
    "        \"\"\"\n",
    "        filtered_kwds = total_count(input_df)\n",
    "        filtered_kwds = set(filtered_kwds[filtered_kwds['count']>=above_n][column_name])\n",
    "        #return filtered_kwds\n",
    "        input_df[column_name] = input_df[column_name].apply(lambda ks: list(filter(lambda k: True if k in filtered_kwds else False, ks)))\n",
    "        return input_df\n",
    "    def make_graph(input_df, column_name='Author Keywords'):\n",
    "        # make edges: edge가 중복으로 생기지 않게 하려면, \n",
    "        def make_edges_from_lst(lst):\n",
    "            if len(lst)>1:\n",
    "                return [(lst[i], lst[j]) for i in range(0, len(lst)-1) for j in range(i+1, len(lst))]\n",
    "            else:\n",
    "                return []\n",
    "        nodes = total_count(input_df)\n",
    "        new_nodes = []\n",
    "        for i in range(0, len(nodes)):\n",
    "            name = nodes[column_name].iloc()[i]\n",
    "            w = nodes['count'].iloc()[i]\n",
    "            new_nodes.append( (name, {'weight':w}) )\n",
    "        nodes = new_nodes\n",
    "        edges = itertools.chain.from_iterable(input_df[column_name].apply(make_edges_from_lst))\n",
    "        edges = ((uv[0], uv[1], w) for uv, w in Counter(edges).most_common())\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(nodes)\n",
    "        G.add_weighted_edges_from(edges)\n",
    "        # graph에 대한 데이터 필터링이 필요할 수 있는데. 여기서. \n",
    "        return G\n",
    "    \n",
    "    def make_ego_graph_from_inputG(inputG, target_n):\n",
    "        newG = nx.Graph()\n",
    "        edges = filter(lambda s: True if s[0]==target_n or s[1]==target_n else False, (e for e in inputG.edges(data=True)))\n",
    "        edges = list(edges)\n",
    "        nodes = itertools.chain.from_iterable([[e[0], e[1]] for e in edges])\n",
    "        \n",
    "    def nxG_to_dotG(nx_G):\n",
    "        dot_G = graphviz.Graph(comment=\"this is what?\")\n",
    "        dot_G.attr(rankdir='LR')\n",
    "        max_n_weight = max((n[1]['weight'] for n in nx_G.nodes(data=True)))\n",
    "        min_n_weight = min((n[1]['weight'] for n in nx_G.nodes(data=True)))\n",
    "        for node in nx_G.nodes(data=True):\n",
    "            weight = (node[1]['weight'] - min_n_weight)/max_n_weight * 3 + 2\n",
    "            dot_G.node(node[0], shape='circle', color='blue', height='{}'.format(weight), width='{}'.format(weight), \n",
    "                       fixedsize='true', fontcolor='blue', fontsize='{}'.format(20*weight))\n",
    "        # edge normalization: 10이면 많이 굵은 편이고 1이면 얇은 편. 1-10 으로 표준화하자.\n",
    "        max_e_weight = max((e[2]['weight'] for e in nx_G.edges(data=True)))\n",
    "        min_e_weight = min((e[2]['weight'] for e in nx_G.edges(data=True)))\n",
    "        for edge in nx_G.edges(data=True):\n",
    "            weight = (edge[2]['weight'] - min_e_weight)/(max_e_weight - min_e_weight) * 20 + 5\n",
    "            dot_G.edge(edge[0], edge[1], style='setlinewidth({})'.format(weight))\n",
    "        return dot_G\n",
    "        \n",
    "    def save_graph(dot_string, output_file_name, file_format='svg'):\n",
    "        if type(dot_string) is str:\n",
    "            g = graphviz.Source(dot_string)\n",
    "        elif isinstance(dot_string, (graphviz.dot.Digraph, graphviz.dot.Graph)):\n",
    "            g = dot_string\n",
    "        g.format=file_format\n",
    "        g.filename = output_file_name\n",
    "        g.directory = '../../assets/images/markdown_img/'\n",
    "        g.render(view=False)\n",
    "        return g\n",
    "    \"\"\"\n",
    "    여기서는 우선 author keywords에 대해서만 진행함. \n",
    "    \"\"\"\n",
    "    new_df = raw_df[['Author Keywords', 'Year', 'Abstract']].dropna()\n",
    "    new_df['Author Keywords'] = new_df['Author Keywords'].apply(lambda s: s.split(\";\"))\n",
    "    new_df['Author Keywords'] = new_df['Author Keywords'].apply(lambda ks: [singularize(k).strip().lower() for k in ks])\n",
    "    new_df['Author Keywords'] = new_df['Author Keywords'].apply(lambda l: sorted(list(set(l))))\n",
    "    \n",
    "    \"\"\"\n",
    "    전체 키워드의 경우는 상위 20개의 키워드에 대해서만 그린다. 따라서 50개에 대해서 필터링 해줌. \n",
    "    \"\"\"\n",
    "    new_df = filtering_auth_kwds(new_df, above_n = total_count(new_df)['count'][20])\n",
    "    \"\"\"\n",
    "    전체 그래프 그리기 \n",
    "    \"\"\"\n",
    "    content_lst = []\n",
    "    G = make_graph(new_df)\n",
    "    title_name = 'entire_network_180508'\n",
    "    save_graph(nxG_to_dotG(G), title_name, 'png')\n",
    "    content_lst.append((title_name, \"\", title_name+\".png\"))\n",
    "    \"\"\"\n",
    "    ego-network 그리기. \n",
    "    \"\"\"\n",
    "    for k in list(total_count(new_df)['Author Keywords'])[:20]:\n",
    "        egoG = nx.ego_graph(G, k)\n",
    "        pic_title_name= 'ego_network_{}_180508'.format(k)\n",
    "        save_graph(nxG_to_dotG(egoG), pic_title_name, 'png')\n",
    "        content_lst.append((pic_title_name, \"\", pic_title_name+\".png\"))\n",
    "    \n",
    "    this_prs = Presentation()\n",
    "    slide_layout = this_prs.slide_layouts[1] \n",
    "    for title, content, img_file_name in content_lst:\n",
    "        this_slide = this_prs.slides.add_slide(slide_layout)\n",
    "        shapes = this_slide.shapes\n",
    "        shapes.title.text = title\n",
    "        shapes.placeholders[1].text = content\n",
    "        # placeholders는 개별 slide에 있는 모든 개체를 가져온다고 보면 됨. \n",
    "        #shapes.add_picture(img_stream, left, top, height=height)\n",
    "        #shapes.add_picture(img_file_name, left=Inches(5), top=Inches(10))\n",
    "        img_path = '../../assets/images/markdown_img/'\n",
    "        shapes.add_picture(img_path+img_file_name, Inches(2.5), Inches(3.2))\n",
    "        # 변환하지 않고 숫자로 넘기면 잘 되지 않는다. \n",
    "    this_prs.save(outputPPTname)\n",
    "    #ego = nx.ego_graph(G, 'sme')\n",
    "    #print(ego.nodes(data=True))\n",
    "    #return G\n",
    "\n",
    "excel_path_and_filename = \"../../../Downloads/SMEs_Scopus_2013-2017.xlsx\"\n",
    "df = pd.read_excel(excel_path_and_filename)\n",
    "df = df[['Author Keywords', 'Year', 'Abstract']]\n",
    "\n",
    "draw_and_export_ppt(df.copy(), \"sme_network.pptx\")\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
