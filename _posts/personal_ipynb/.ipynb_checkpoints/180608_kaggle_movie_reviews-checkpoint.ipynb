{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----data cleaning complete----\n",
      "----word count vectorization complete----\n",
      "----fitting complete----\n",
      "----test data count vectorization----\n",
      "----all complete upload it on kaggle----\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_url = '/Users/frhyme/Downloads/labeledTrainData.tsv'\n",
    "train_df = pd.read_csv(train_url, delimiter='\\t', header=0, quoting=3)\n",
    "#train_df = train_df[:1000] # 생각보다 계산이 오래 걸려서 일단 이걸 넣어줌\n",
    "\n",
    "def cleaning_each_review(raw_review):\n",
    "    cleaned_review = BeautifulSoup(raw_review, 'lxml').get_text()# 태그를 없애줍니다. \n",
    "    cleaned_review = re.sub(\"[^a-zA-Z]\",\" \", cleaned_review ).lower().strip()\n",
    "    while \"  \" in cleaned_review: # 공백을 없애줍니다. \n",
    "        cleaned_review = cleaned_review.replace(\"  \", \" \")\n",
    "    words_in_cleaned_review = cleaned_review.split(\" \")# 단어를 잘라줍니다. \n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    # nltk에 정의된 무의미한 단어를 삭제해줍니다. \n",
    "    words_in_cleaned_review = filter(lambda w: True if w not in stop_words else False, words_in_cleaned_review)    \n",
    "    return \" \".join(list(words_in_cleaned_review))\n",
    "train_df['cleaned_movie_review'] = train_df['review'].apply(cleaning_each_review)\n",
    "print(\"----data cleaning complete----\")\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                             preprocessor = None, stop_words = None, \n",
    "                             max_features = 5000) # max_feature의 술ㄹ 조절하자\n",
    "\n",
    "# CountVectorizer의 결과는 sparse matrix로 리턴되는데, 따라서 이를 np.array로 변형해주는 것이 필요함. \n",
    "train_word_count_df = pd.DataFrame(vectorizer.fit_transform(train_df['cleaned_movie_review']).toarray(), \n",
    "                             columns=vectorizer.get_feature_names())\n",
    "print(\"----word count vectorization complete----\")\n",
    "\"\"\"\n",
    "값들이 표준화되지도 않았고, n-gram을 사용한 것도 아니고, \n",
    "하지만 어쨌든 간에 개별 review에 대해서 feature vector를 만들었습니다. \n",
    "이걸 사용해서, 학습을 해보려고 합니다. 믿음의 랜덤포뤠스트!!!\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)# n_estimators를 늘리면서 해봐야 할것 같아요. \n",
    "rf.fit(train_word_count_df.values, train_df['sentiment'])\n",
    "print(\"----fitting complete----\")\n",
    "########\n",
    "\n",
    "# test_data에 적용해봅니다. \n",
    "test_url = '/Users/frhyme/Downloads/testData.tsv'\n",
    "test_df = pd.read_csv(test_url, delimiter='\\t', header=0, quoting=3)\n",
    "test_df['cleaned_movie_review'] = test_df['review'].apply(cleaning_each_review)\n",
    "\n",
    "print(\"----test data count vectorization----\")\n",
    "test_word_count_df = pd.DataFrame(vectorizer.transform(test_df['cleaned_movie_review']).toarray(), \n",
    "                             columns=vectorizer.get_feature_names())\n",
    "output = pd.DataFrame( data={\"id\":test_df[\"id\"], \"sentiment\":rf.predict(test_word_count_df.values)} )\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )\n",
    "print(\"----all complete upload it on kaggle----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 complete\n",
      "5000 complete\n",
      "10000 complete\n",
      "15000 complete\n",
      "20000 complete\n",
      "25000 complete\n",
      "30000 complete\n",
      "35000 complete\n",
      "40000 complete\n",
      "45000 complete\n",
      "50000 complete\n",
      "55000 complete\n",
      "60000 complete\n",
      "65000 complete\n",
      "70000 complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## part 2 \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\"\"\"\n",
    "- 여기서는 word-embedding을 이용합니다. \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import nltk.data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import itertools\n",
    "\n",
    "train_url = '/Users/frhyme/Downloads/labeledTrainData.tsv'\n",
    "train_df = pd.read_csv(train_url, delimiter='\\t', header=0, quoting=3)\n",
    "\n",
    "unlabeled_train_url = '/Users/frhyme/Downloads/unlabeledTrainData.tsv'\n",
    "unlabeled_train_df = pd.read_csv(unlabeled_train_url, delimiter='\\t', header=0, quoting=3)\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # sentence tokenizer가 밑에서 선언되는데, 그냥 여기서 디폴트로 만들어주는 게 더 좋을 수 있다. \n",
    "    # 흠...이 tokenizer는 점도 없는데 어떻게 이렇게 잘 잘라주는거지. \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist(raw_sentence) )\n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "for i, r in enumerate(list(train_df['review'])+list(unlabeled_train_df['review'])):\n",
    "    sentences += review_to_sentences(r, tokenizer=nltk.data.load('tokenizers/punkt/english.pickle'))\n",
    "    if i % 5000==0:\n",
    "        print(\"{} complete\".format(i))\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 13:48:39,310 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-06-09 13:48:39,357 : INFO : collecting all words and their counts\n",
      "2018-06-09 13:48:39,361 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-09 13:48:39,506 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 13:48:39,709 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2018-06-09 13:48:39,871 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2018-06-09 13:48:39,990 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2018-06-09 13:48:40,098 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2018-06-09 13:48:40,207 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2018-06-09 13:48:40,323 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2018-06-09 13:48:40,433 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2018-06-09 13:48:40,539 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2018-06-09 13:48:40,667 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2018-06-09 13:48:40,802 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2018-06-09 13:48:40,939 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2018-06-09 13:48:41,080 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2018-06-09 13:48:41,264 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2018-06-09 13:48:41,440 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2018-06-09 13:48:41,561 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2018-06-09 13:48:41,752 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2018-06-09 13:48:41,875 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2018-06-09 13:48:42,044 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2018-06-09 13:48:42,184 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2018-06-09 13:48:42,292 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2018-06-09 13:48:42,438 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2018-06-09 13:48:42,541 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2018-06-09 13:48:42,665 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2018-06-09 13:48:42,774 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2018-06-09 13:48:42,892 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2018-06-09 13:48:43,041 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
      "2018-06-09 13:48:43,390 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
      "2018-06-09 13:48:43,548 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
      "2018-06-09 13:48:43,683 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
      "2018-06-09 13:48:43,807 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
      "2018-06-09 13:48:43,936 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
      "2018-06-09 13:48:44,074 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
      "2018-06-09 13:48:44,190 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
      "2018-06-09 13:48:44,331 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
      "2018-06-09 13:48:44,465 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types\n",
      "2018-06-09 13:48:44,645 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types\n",
      "2018-06-09 13:48:44,827 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types\n",
      "2018-06-09 13:48:44,960 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types\n",
      "2018-06-09 13:48:45,077 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types\n",
      "2018-06-09 13:48:45,190 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types\n",
      "2018-06-09 13:48:45,300 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types\n",
      "2018-06-09 13:48:45,424 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types\n",
      "2018-06-09 13:48:45,613 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types\n",
      "2018-06-09 13:48:45,805 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types\n",
      "2018-06-09 13:48:45,937 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types\n",
      "2018-06-09 13:48:46,081 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types\n",
      "2018-06-09 13:48:46,199 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types\n",
      "2018-06-09 13:48:46,325 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types\n",
      "2018-06-09 13:48:46,572 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types\n",
      "2018-06-09 13:48:46,805 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types\n",
      "2018-06-09 13:48:46,974 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types\n",
      "2018-06-09 13:48:47,104 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types\n",
      "2018-06-09 13:48:47,239 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types\n",
      "2018-06-09 13:48:47,365 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types\n",
      "2018-06-09 13:48:47,505 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types\n",
      "2018-06-09 13:48:47,632 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types\n",
      "2018-06-09 13:48:47,783 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types\n",
      "2018-06-09 13:48:47,964 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types\n",
      "2018-06-09 13:48:48,219 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types\n",
      "2018-06-09 13:48:48,409 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types\n",
      "2018-06-09 13:48:48,580 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types\n",
      "2018-06-09 13:48:48,745 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types\n",
      "2018-06-09 13:48:48,914 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types\n",
      "2018-06-09 13:48:49,077 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types\n",
      "2018-06-09 13:48:49,217 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types\n",
      "2018-06-09 13:48:49,578 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types\n",
      "2018-06-09 13:48:49,751 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types\n",
      "2018-06-09 13:48:49,891 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types\n",
      "2018-06-09 13:48:50,046 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types\n",
      "2018-06-09 13:48:50,175 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types\n",
      "2018-06-09 13:48:50,309 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types\n",
      "2018-06-09 13:48:50,434 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 13:48:50,589 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types\n",
      "2018-06-09 13:48:50,728 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types\n",
      "2018-06-09 13:48:50,870 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types\n",
      "2018-06-09 13:48:51,000 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types\n",
      "2018-06-09 13:48:51,240 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types\n",
      "2018-06-09 13:48:51,411 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types\n",
      "2018-06-09 13:48:51,505 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences\n",
      "2018-06-09 13:48:51,509 : INFO : Loading a fresh vocabulary\n",
      "2018-06-09 13:48:52,484 : INFO : min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2018-06-09 13:48:52,490 : INFO : min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
      "2018-06-09 13:48:52,861 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2018-06-09 13:48:52,964 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-06-09 13:48:52,969 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
      "2018-06-09 13:48:53,184 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2018-06-09 13:48:53,188 : INFO : resetting layer weights\n",
      "2018-06-09 13:48:53,754 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-06-09 13:48:54,858 : INFO : EPOCH 1 - PROGRESS: at 2.56% examples, 313663 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:48:55,864 : INFO : EPOCH 1 - PROGRESS: at 5.42% examples, 337653 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:48:56,885 : INFO : EPOCH 1 - PROGRESS: at 8.06% examples, 332264 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:48:57,906 : INFO : EPOCH 1 - PROGRESS: at 11.32% examples, 350805 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:48:58,916 : INFO : EPOCH 1 - PROGRESS: at 14.62% examples, 362811 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:48:59,923 : INFO : EPOCH 1 - PROGRESS: at 16.61% examples, 344003 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:00,928 : INFO : EPOCH 1 - PROGRESS: at 19.26% examples, 342622 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:01,939 : INFO : EPOCH 1 - PROGRESS: at 22.87% examples, 356353 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:03,015 : INFO : EPOCH 1 - PROGRESS: at 24.67% examples, 339552 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:04,058 : INFO : EPOCH 1 - PROGRESS: at 26.18% examples, 323832 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:05,110 : INFO : EPOCH 1 - PROGRESS: at 28.43% examples, 318980 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:06,129 : INFO : EPOCH 1 - PROGRESS: at 31.26% examples, 321565 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:07,148 : INFO : EPOCH 1 - PROGRESS: at 34.54% examples, 328039 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:08,153 : INFO : EPOCH 1 - PROGRESS: at 37.11% examples, 328000 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:09,156 : INFO : EPOCH 1 - PROGRESS: at 39.54% examples, 326596 words/s, in_qsize 6, out_qsize 0\n",
      "2018-06-09 13:49:10,174 : INFO : EPOCH 1 - PROGRESS: at 42.41% examples, 329052 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:11,203 : INFO : EPOCH 1 - PROGRESS: at 45.12% examples, 329333 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:49:12,204 : INFO : EPOCH 1 - PROGRESS: at 48.77% examples, 337072 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:13,207 : INFO : EPOCH 1 - PROGRESS: at 52.54% examples, 344313 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:14,215 : INFO : EPOCH 1 - PROGRESS: at 56.55% examples, 352521 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:15,234 : INFO : EPOCH 1 - PROGRESS: at 60.31% examples, 358440 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:49:16,253 : INFO : EPOCH 1 - PROGRESS: at 64.11% examples, 363828 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:17,274 : INFO : EPOCH 1 - PROGRESS: at 67.63% examples, 367184 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:18,278 : INFO : EPOCH 1 - PROGRESS: at 71.22% examples, 370843 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:19,285 : INFO : EPOCH 1 - PROGRESS: at 75.13% examples, 375830 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:20,299 : INFO : EPOCH 1 - PROGRESS: at 78.20% examples, 376302 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:21,327 : INFO : EPOCH 1 - PROGRESS: at 81.10% examples, 375493 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:22,327 : INFO : EPOCH 1 - PROGRESS: at 84.67% examples, 378368 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:23,358 : INFO : EPOCH 1 - PROGRESS: at 88.28% examples, 380914 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:24,358 : INFO : EPOCH 1 - PROGRESS: at 90.65% examples, 378277 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:25,375 : INFO : EPOCH 1 - PROGRESS: at 93.18% examples, 376298 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:26,396 : INFO : EPOCH 1 - PROGRESS: at 95.97% examples, 375260 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:27,411 : INFO : EPOCH 1 - PROGRESS: at 99.11% examples, 376075 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:27,582 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-09 13:49:27,597 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-09 13:49:27,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-09 13:49:27,623 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-09 13:49:27,626 : INFO : EPOCH - 1 : training on 17798082 raw words (12750207 effective words) took 33.8s, 377052 effective words/s\n",
      "2018-06-09 13:49:28,653 : INFO : EPOCH 2 - PROGRESS: at 3.54% examples, 445476 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:29,677 : INFO : EPOCH 2 - PROGRESS: at 6.62% examples, 414396 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:30,677 : INFO : EPOCH 2 - PROGRESS: at 9.98% examples, 416811 words/s, in_qsize 7, out_qsize 1\n",
      "2018-06-09 13:49:31,684 : INFO : EPOCH 2 - PROGRESS: at 13.82% examples, 433501 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:32,710 : INFO : EPOCH 2 - PROGRESS: at 17.30% examples, 431696 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:33,719 : INFO : EPOCH 2 - PROGRESS: at 20.90% examples, 435199 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:34,723 : INFO : EPOCH 2 - PROGRESS: at 24.44% examples, 437165 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:35,724 : INFO : EPOCH 2 - PROGRESS: at 28.20% examples, 442254 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:36,725 : INFO : EPOCH 2 - PROGRESS: at 31.32% examples, 436777 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:37,740 : INFO : EPOCH 2 - PROGRESS: at 34.94% examples, 438094 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:38,755 : INFO : EPOCH 2 - PROGRESS: at 38.45% examples, 438637 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:39,768 : INFO : EPOCH 2 - PROGRESS: at 42.02% examples, 439759 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:40,771 : INFO : EPOCH 2 - PROGRESS: at 45.00% examples, 435104 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:49:41,797 : INFO : EPOCH 2 - PROGRESS: at 48.00% examples, 430821 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:42,799 : INFO : EPOCH 2 - PROGRESS: at 51.01% examples, 427836 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:43,814 : INFO : EPOCH 2 - PROGRESS: at 54.10% examples, 425312 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:44,819 : INFO : EPOCH 2 - PROGRESS: at 57.84% examples, 428674 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:49:45,829 : INFO : EPOCH 2 - PROGRESS: at 61.81% examples, 432807 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:46,830 : INFO : EPOCH 2 - PROGRESS: at 65.45% examples, 434479 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:47,839 : INFO : EPOCH 2 - PROGRESS: at 69.14% examples, 436168 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:48,860 : INFO : EPOCH 2 - PROGRESS: at 72.90% examples, 437779 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 13:49:49,864 : INFO : EPOCH 2 - PROGRESS: at 75.76% examples, 434396 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:49:50,896 : INFO : EPOCH 2 - PROGRESS: at 78.61% examples, 430818 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:49:51,928 : INFO : EPOCH 2 - PROGRESS: at 82.15% examples, 431075 words/s, in_qsize 7, out_qsize 1\n",
      "2018-06-09 13:49:52,929 : INFO : EPOCH 2 - PROGRESS: at 86.25% examples, 434668 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:53,942 : INFO : EPOCH 2 - PROGRESS: at 90.59% examples, 439122 words/s, in_qsize 7, out_qsize 1\n",
      "2018-06-09 13:49:54,960 : INFO : EPOCH 2 - PROGRESS: at 95.17% examples, 443950 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:55,987 : INFO : EPOCH 2 - PROGRESS: at 99.66% examples, 448311 words/s, in_qsize 5, out_qsize 1\n",
      "2018-06-09 13:49:56,007 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-09 13:49:56,034 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-09 13:49:56,038 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-09 13:49:56,047 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-09 13:49:56,049 : INFO : EPOCH - 2 : training on 17798082 raw words (12749880 effective words) took 28.4s, 448823 effective words/s\n",
      "2018-06-09 13:49:57,067 : INFO : EPOCH 3 - PROGRESS: at 4.37% examples, 553675 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:49:58,081 : INFO : EPOCH 3 - PROGRESS: at 9.00% examples, 565500 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:49:59,082 : INFO : EPOCH 3 - PROGRESS: at 13.48% examples, 565686 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:00,103 : INFO : EPOCH 3 - PROGRESS: at 17.96% examples, 562359 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:01,103 : INFO : EPOCH 3 - PROGRESS: at 21.46% examples, 538828 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:50:02,124 : INFO : EPOCH 3 - PROGRESS: at 25.05% examples, 523668 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:03,145 : INFO : EPOCH 3 - PROGRESS: at 29.09% examples, 520739 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:04,146 : INFO : EPOCH 3 - PROGRESS: at 32.29% examples, 505679 words/s, in_qsize 5, out_qsize 2\n",
      "2018-06-09 13:50:05,185 : INFO : EPOCH 3 - PROGRESS: at 35.50% examples, 492717 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:06,192 : INFO : EPOCH 3 - PROGRESS: at 38.63% examples, 483316 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:50:07,201 : INFO : EPOCH 3 - PROGRESS: at 42.30% examples, 481905 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:08,203 : INFO : EPOCH 3 - PROGRESS: at 46.72% examples, 488614 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:09,213 : INFO : EPOCH 3 - PROGRESS: at 50.74% examples, 490251 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:10,216 : INFO : EPOCH 3 - PROGRESS: at 53.88% examples, 483784 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:50:11,233 : INFO : EPOCH 3 - PROGRESS: at 58.01% examples, 486699 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:12,259 : INFO : EPOCH 3 - PROGRESS: at 61.30% examples, 481924 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:13,270 : INFO : EPOCH 3 - PROGRESS: at 65.12% examples, 481882 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:14,272 : INFO : EPOCH 3 - PROGRESS: at 68.03% examples, 475791 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:15,282 : INFO : EPOCH 3 - PROGRESS: at 71.93% examples, 476844 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:16,291 : INFO : EPOCH 3 - PROGRESS: at 75.64% examples, 476410 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:17,303 : INFO : EPOCH 3 - PROGRESS: at 79.69% examples, 477929 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:18,319 : INFO : EPOCH 3 - PROGRESS: at 83.84% examples, 479893 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:19,320 : INFO : EPOCH 3 - PROGRESS: at 87.31% examples, 478324 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:20,320 : INFO : EPOCH 3 - PROGRESS: at 90.82% examples, 477152 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:21,331 : INFO : EPOCH 3 - PROGRESS: at 95.06% examples, 479292 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:22,337 : INFO : EPOCH 3 - PROGRESS: at 99.16% examples, 481098 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:22,481 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-09 13:50:22,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-09 13:50:22,510 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-09 13:50:22,520 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-09 13:50:22,522 : INFO : EPOCH - 3 : training on 17798082 raw words (12748234 effective words) took 26.5s, 481764 effective words/s\n",
      "2018-06-09 13:50:23,539 : INFO : EPOCH 4 - PROGRESS: at 4.05% examples, 511602 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:24,547 : INFO : EPOCH 4 - PROGRESS: at 8.50% examples, 535966 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:25,551 : INFO : EPOCH 4 - PROGRESS: at 12.39% examples, 521480 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:26,553 : INFO : EPOCH 4 - PROGRESS: at 16.01% examples, 505366 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:27,562 : INFO : EPOCH 4 - PROGRESS: at 20.16% examples, 507625 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:28,572 : INFO : EPOCH 4 - PROGRESS: at 23.98% examples, 503347 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:29,578 : INFO : EPOCH 4 - PROGRESS: at 28.08% examples, 505503 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:30,618 : INFO : EPOCH 4 - PROGRESS: at 32.13% examples, 503255 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:31,627 : INFO : EPOCH 4 - PROGRESS: at 36.06% examples, 502424 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:50:32,638 : INFO : EPOCH 4 - PROGRESS: at 40.02% examples, 502476 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:33,657 : INFO : EPOCH 4 - PROGRESS: at 43.92% examples, 501478 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:34,674 : INFO : EPOCH 4 - PROGRESS: at 47.95% examples, 501858 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:35,684 : INFO : EPOCH 4 - PROGRESS: at 51.87% examples, 501447 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:36,691 : INFO : EPOCH 4 - PROGRESS: at 55.83% examples, 501650 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:37,695 : INFO : EPOCH 4 - PROGRESS: at 59.95% examples, 503769 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:38,723 : INFO : EPOCH 4 - PROGRESS: at 64.22% examples, 505383 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:39,726 : INFO : EPOCH 4 - PROGRESS: at 68.38% examples, 506704 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:40,736 : INFO : EPOCH 4 - PROGRESS: at 72.66% examples, 508833 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:50:41,744 : INFO : EPOCH 4 - PROGRESS: at 76.93% examples, 510446 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:42,748 : INFO : EPOCH 4 - PROGRESS: at 81.15% examples, 511629 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:43,772 : INFO : EPOCH 4 - PROGRESS: at 84.88% examples, 509517 words/s, in_qsize 7, out_qsize 1\n",
      "2018-06-09 13:50:44,780 : INFO : EPOCH 4 - PROGRESS: at 88.92% examples, 509588 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:45,786 : INFO : EPOCH 4 - PROGRESS: at 92.62% examples, 507846 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:46,795 : INFO : EPOCH 4 - PROGRESS: at 96.08% examples, 504716 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:47,798 : INFO : EPOCH 4 - PROGRESS: at 99.49% examples, 502229 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:47,890 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-09 13:50:47,899 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-09 13:50:47,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-09 13:50:47,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-09 13:50:47,920 : INFO : EPOCH - 4 : training on 17798082 raw words (12752483 effective words) took 25.4s, 502325 effective words/s\n",
      "2018-06-09 13:50:48,965 : INFO : EPOCH 5 - PROGRESS: at 3.76% examples, 470161 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:49,976 : INFO : EPOCH 5 - PROGRESS: at 7.77% examples, 486137 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 13:50:50,980 : INFO : EPOCH 5 - PROGRESS: at 12.12% examples, 506854 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:51,983 : INFO : EPOCH 5 - PROGRESS: at 16.35% examples, 513948 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:53,006 : INFO : EPOCH 5 - PROGRESS: at 20.51% examples, 513177 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:54,070 : INFO : EPOCH 5 - PROGRESS: at 23.70% examples, 490699 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:55,092 : INFO : EPOCH 5 - PROGRESS: at 25.45% examples, 451469 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:56,139 : INFO : EPOCH 5 - PROGRESS: at 28.26% examples, 437371 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:57,172 : INFO : EPOCH 5 - PROGRESS: at 31.32% examples, 430219 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:50:58,177 : INFO : EPOCH 5 - PROGRESS: at 34.39% examples, 425658 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:50:59,189 : INFO : EPOCH 5 - PROGRESS: at 37.68% examples, 424884 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:00,200 : INFO : EPOCH 5 - PROGRESS: at 40.63% examples, 420773 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:01,225 : INFO : EPOCH 5 - PROGRESS: at 43.75% examples, 418468 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:02,271 : INFO : EPOCH 5 - PROGRESS: at 46.72% examples, 414363 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:51:03,282 : INFO : EPOCH 5 - PROGRESS: at 50.34% examples, 417346 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:04,284 : INFO : EPOCH 5 - PROGRESS: at 53.49% examples, 416250 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:05,302 : INFO : EPOCH 5 - PROGRESS: at 57.15% examples, 419030 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:06,303 : INFO : EPOCH 5 - PROGRESS: at 60.18% examples, 417578 words/s, in_qsize 6, out_qsize 2\n",
      "2018-06-09 13:51:07,308 : INFO : EPOCH 5 - PROGRESS: at 63.05% examples, 414750 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:51:08,342 : INFO : EPOCH 5 - PROGRESS: at 65.85% examples, 411264 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:09,351 : INFO : EPOCH 5 - PROGRESS: at 68.50% examples, 407580 words/s, in_qsize 8, out_qsize 1\n",
      "2018-06-09 13:51:10,353 : INFO : EPOCH 5 - PROGRESS: at 71.54% examples, 406886 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:11,356 : INFO : EPOCH 5 - PROGRESS: at 73.90% examples, 402285 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:51:12,375 : INFO : EPOCH 5 - PROGRESS: at 77.43% examples, 403941 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:51:13,388 : INFO : EPOCH 5 - PROGRESS: at 80.70% examples, 404128 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:14,419 : INFO : EPOCH 5 - PROGRESS: at 83.28% examples, 400808 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:15,429 : INFO : EPOCH 5 - PROGRESS: at 86.14% examples, 399362 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:16,437 : INFO : EPOCH 5 - PROGRESS: at 88.92% examples, 397785 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-09 13:51:17,448 : INFO : EPOCH 5 - PROGRESS: at 91.87% examples, 396998 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:18,462 : INFO : EPOCH 5 - PROGRESS: at 95.24% examples, 397603 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-09 13:51:19,468 : INFO : EPOCH 5 - PROGRESS: at 98.71% examples, 399236 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-09 13:51:19,749 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-09 13:51:19,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-09 13:51:19,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-09 13:51:19,781 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-09 13:51:19,783 : INFO : EPOCH - 5 : training on 17798082 raw words (12748392 effective words) took 31.8s, 400440 effective words/s\n",
      "2018-06-09 13:51:19,789 : INFO : training on a 88990410 raw words (63749196 effective words) took 146.0s, 436583 effective words/s\n",
      "2018-06-09 13:51:19,828 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-06-09 13:51:20,089 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-06-09 13:51:20,116 : INFO : not storing attribute vectors_norm\n",
      "2018-06-09 13:51:20,137 : INFO : not storing attribute cum_table\n",
      "2018-06-09 13:51:21,285 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "# logging 귀찮아서안하기로 함. 대충 3-4분 걸림. \n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = 4,# Number of threads to run in parallel\n",
    "                          size = 300, # Word vector dimensionality                      \n",
    "                          min_count = 40, # Minimum word count\n",
    "                          window = 10, # Context window size\n",
    "                          sample = 1e-3, # Downsample setting for frequent words\n",
    "                         )\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man woman child kitchen => kitchen\n",
      "france england germany berlin => berlin\n",
      "paris berlin london austria => paris\n",
      "movie actor actress director => movie\n",
      "movie actor actress => movie\n",
      "movie cinema film art => art\n",
      "actor actress director => director\n",
      "---doesnt match over---\n",
      "'man' is most similar with (woman, lady, monk, lad, farmer)\n",
      "'movie' is most similar with (film, flick, movies, it, sequel)\n",
      "'soldier' is most similar with (army, warrior, navy, marine, dictator)\n",
      "---most similar over---\n"
     ]
    }
   ],
   "source": [
    "doesnt_match_sentences = [\n",
    "    \"man woman child kitchen\", \n",
    "    \"france england germany berlin\", \n",
    "    \"paris berlin london austria\",\n",
    "    'movie actor actress director', \n",
    "    'movie actor actress', \n",
    "    'movie cinema film art',\n",
    "    'actor actress director', \n",
    "]\n",
    "for s in doesnt_match_sentences:\n",
    "    print(\"{} => {}\".format(s, model.doesnt_match(s.split())))\n",
    "print(\"---doesnt match over---\")\n",
    "for w in ['man', 'movie', 'soldier']:\n",
    "    similar_words = \", \".join([s_w[0] for s_w in model.most_similar(w)][:5])\n",
    "    print(\"'{}' is most similar with ({})\".format(w, similar_words))\n",
    "print(\"---most similar over---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 14:08:06,792 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2018-06-09 14:08:07,570 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2018-06-09 14:08:07,574 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-09 14:08:07,576 : INFO : loading vocabulary recursively from 300features_40minwords_10context.vocabulary.* with mmap=None\n",
      "2018-06-09 14:08:07,578 : INFO : loading trainables recursively from 300features_40minwords_10context.trainables.* with mmap=None\n",
      "2018-06-09 14:08:07,580 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-09 14:08:07,583 : INFO : loaded 300features_40minwords_10context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of each word vector: (300,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- 지난번에 만든 model을 save해두었기 때문에, 이후에도 이 model을 그대로 사용할 수 있습니다. 개꿀!\n",
    "- 흠, 이런 식이면 keras에서도 모델을 저장해두고, 나중에 그대로 사용할 수 있는 것 아닐까? 그러면 좀 편할것 같은데 흐음. \n",
    "    - training이 오래 걸리지, prediction은 오래 걸리지 않기 때문에\n",
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "print(\"the shape of each word vector: {}\".format(model['man'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "---train data over---\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "---test data over---\n",
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "## after making train_df and test_df \n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    - word list의 개별 semantic vector의 평균을 계산하여 돌려주는 함수\n",
    "    - method가 조금 달라서 고쳐준 부분이 있음 \n",
    "    \"\"\"\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.0\n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.0\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    return np.divide(featureVec,nwords) # Divide the result by the number of words to get the average\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    - makeFeatureVec가 개별 word list로부터 semantic vector의 평균을 계산하여 돌려줬다면\n",
    "    - 이 함수의 경우는 review list로부터 semantice vector의 평균 list를 돌려준다. \n",
    "    - 이전에 word2vec 학습할때 num_feature의 경우 300으로 했으므로 아마도 300이어야 할듯\n",
    "    - 이건 굳이 argument로 넣을 필요 없이, 만든 model의 num_feature를 내부에서 돌리는게 더 효율적이지 않나? \n",
    "    \"\"\"\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # Loop through the reviews\n",
    "    for i, review in enumerate(reviews):\n",
    "        if i%5000. == 0.:\n",
    "            print(\"Review {:.0f} of {:.0f}\".format(i, len(reviews)))\n",
    "        reviewFeatureVecs[i] = makeFeatureVec(review, model, num_features)\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train_df[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features=300 )\n",
    "print(\"---train data over---\")\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test_df[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features=300)\n",
    "print(\"---test data over---\")\n",
    "\n",
    "#######\n",
    "### random forest fitting \n",
    "#######\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier( n_estimators = 500)\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "rf = rf.fit( trainDataVecs, train_df[\"sentiment\"] )\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\": test_df[\"id\"], \"sentiment\": rf.predict( testDataVecs )} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n",
    "print('complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using clustering after word-embedding \n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "word_vectors = model.wv.syn0\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = word_vectors.shape[0]//5 )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "# Get the end time and print how long the process took\n",
    "print(\"Time taken for K Means clustering: {} seconds\".format(time.time()- start))\n",
    "##### clustering over \n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))\n",
    "# cluster를 key로 두고, 포함되는 모든 word를 리스트로 value로 넣은 dict\n",
    "word_centroid_map_grouped_dict = {cluster: [] for cluster in range(min(word_centroid_map.values()), \n",
    "                                                              max(word_centroid_map.values())+1)}\n",
    "for w, k in word_centroid_map.items():\n",
    "    word_centroid_map_grouped_dict[k].append(w)\n",
    "\n",
    "# 상위 10개의 클러스트터를 출력해서 한번 봅닌다. \n",
    "for k in word_centroid_map_grouped_dict.keys():\n",
    "    if k >10:\n",
    "        break\n",
    "    else:\n",
    "        print(\"cluster {}:\".format(k))\n",
    "        print(word_centroid_map_grouped_dict[k])\n",
    "        print(\"-------\")\n",
    "### \n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( max( word_centroid_map.values() ) + 1, dtype=\"float32\" )\n",
    "    # cluster에 속하면 count를 늘림. 단순함. \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map.keys():\n",
    "            bag_of_centroids[word_centroid_map[word]] += 1\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train_df[\"review\"].size, word_vectors.shape[0]//5), dtype=\"float32\" )\n",
    "for i, review in enumerate(clean_train_reviews):\n",
    "    train_centroids[i] = create_bag_of_centroids( review, word_centroid_map )\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test_df[\"review\"].size, word_vectors.shape[0]//5), dtype=\"float32\" )\n",
    "for i, review in enumerate(clean_test_reviews):\n",
    "    test_centroids[i] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    \n",
    "# Fit a random forest and extract predictions \n",
    "rf = RandomForestClassifier(n_estimators = 200)\n",
    "# Fitting the forest may take a few minutes\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "rf.fit(train_centroids,train_df[\"sentiment\"])\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\": test_df[\"id\"], \"sentiment\": rf.predict(test_centroids)})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3)\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "complete\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
