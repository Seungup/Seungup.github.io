{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지난번에 코딩할때, 제대로 모듈화도 하지 않고, 너무 막 코딩하여 이 노트북에서는 좀 정리를 해놓으려고 합니다. \n",
    "    - 아직 추가로 하지 못한 부분이 많아서, 해야할게 많거든요. 아무튼. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop 45 row\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Author Keywords  Year  \\\n",
      "0  [CSFs,  Factor analysis,  Indian manufacturing...  2017   \n",
      "1  [Energy recovery,  Muzzle arc,  Pulsed supply,...  2017   \n",
      "3  [internationalization,  market research,  orga...  2017   \n",
      "5  [Contractors,  Entrepreneurship,  Indonesia,  ...  2017   \n",
      "6  [BPGM-SME,  Improved UKF,  Multi-target tracki...  2017   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  This research paper is to evaluate and present...   \n",
      "1  According to the application requirements of t...   \n",
      "3  This study shows a low-priced information coll...   \n",
      "5  The success of entrepreneurship as an importan...   \n",
      "6  The problem of multi-sensor detecting and mult...   \n",
      "\n",
      "                                        Noun Phrases  \n",
      "0  [research paper, online survey, medium enterpr...  \n",
      "1  [according, application requirements, novel ci...  \n",
      "3  [study shows, information collection process, ...  \n",
      "5  [important driving force, business success, co...  \n",
      "6  [firstly, double satellite, state equations, m...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frhyme/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도 시트 완료\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "required library \n",
    "\"\"\"\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from inflection import singularize \n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\"\"\"\n",
    "desc: centrality를 계산하는 함수들입니다. \n",
    "input: graph(undirected)\n",
    "output: dictionary\n",
    "\"\"\"\n",
    "def return_weighted_degree_centrality(input_g, normalized=True):\n",
    "    w_d_centrality = {n:0.0 for n in input_g.nodes()}\n",
    "    for u, v, d in input_g.edges(data=True):\n",
    "        w_d_centrality[u]+=d['weight']\n",
    "        w_d_centrality[v]+=d['weight']\n",
    "    if normalized==True:\n",
    "        weighted_sum = sum(w_d_centrality.values())\n",
    "        return {k:v/weighted_sum for k, v in w_d_centrality.items()}\n",
    "    else:\n",
    "        return w_d_centrality\n",
    "def return_closeness_centrality(input_g):\n",
    "    new_g_with_distance = input_g.copy()\n",
    "    for u,v,d in new_g_with_distance.edges(data=True):\n",
    "        if 'distance' not in d:\n",
    "            d['distance'] = 1.0/d['weight']\n",
    "    return nx.closeness_centrality(new_g_with_distance, distance='distance')\n",
    "def return_betweenness_centrality(input_g):\n",
    "    return nx.betweenness_centrality(input_g, weight='weight')\n",
    "def return_pagerank(input_g):\n",
    "    return nx.pagerank(input_g, weight='weight')\n",
    "\n",
    "\"\"\"\n",
    "desc: pd.DataFrame의 특정 칼럼(each element is list)의 모든 리스트를 합친 다음 Counter => pd.DataFrame\n",
    "input: pd.DataFrame\n",
    "output: pd.DataFrame\n",
    "\"\"\"\n",
    "def total_count_with(input_df, column_name='Author Keywords'):\n",
    "    # 'Author Keywords' or 'Noun Phrases'\n",
    "    r = itertools.chain.from_iterable(input_df[column_name])\n",
    "    r = Counter(r).most_common()\n",
    "    return pd.DataFrame(r, columns=[column_name, 'count'])\n",
    "\"\"\"\n",
    "desc: 전체적으로 listfmf filtering하여 리턴한다. \n",
    "input: pd.DataFrame(each element is list of kwd)\n",
    "output: pd.DataFrame\n",
    "\"\"\"\n",
    "def filtering_auth_kwds(input_df,column_name='Author Keywords', above_n=3):\n",
    "    \"\"\"\n",
    "    개별 node가 전체에서 1번 밖에 등장하지 않는 경우도 많은데, 이를 모두 고려해서 분석을 하면, 효율적이지 못한 계산이 된다. \n",
    "    따라서, 빈도가 일정 이상을 넘는 경우에 대해서만 고려하여 new_df를 수정하는 것이 필요하다. \n",
    "    \"\"\"\n",
    "    # singularize \n",
    "    input_df[column_name] = input_df[column_name].apply(lambda ks: [singularize(k).strip().lower() for k in ks])\n",
    "    # drop low count kwd \n",
    "    filtered_kwds = total_count(input_df, column_name=column_name)\n",
    "    filtered_kwds = set(filtered_kwds[filtered_kwds['count']>=above_n][column_name])\n",
    "    input_df[column_name] = input_df[column_name].apply(lambda ks: list(filter(lambda k: True if k in filtered_kwds else False, ks)))\n",
    "    # edge를 만들때 중복을 방지하기 위해서 sorting해둔다. \n",
    "    input_df[column_name] = input_df[column_name].apply(lambda l: sorted(list(set(l))))\n",
    "    \"\"\"\n",
    "    word embeddingd 등 다른 데이터 전처리가 필요하다면 여기서 처리하는 것이 좋음. \n",
    "    \"\"\"\n",
    "    return input_df# 사실 굳이 return을 쓸 필요가 없음. 이미 내부에서 다 바꿔줌. \n",
    "\"\"\"\n",
    "desc: 연도별로 상위 빈도 키워드를 50개씩 칼럼별로 보여줌\n",
    "input: pd.DataFrame\n",
    "output: pd.DataFrame(column 이름은 각 연도)\n",
    "\"\"\"\n",
    "def yearly_count_rank(input_df, column_name='Author Keywords', until_rank_n=50):\n",
    "    r_dict = {}\n",
    "    for year, year_df in input_df.groupby('Year'):\n",
    "        r_dict[year] = list(total_count(year_df, column_name=column_name)[column_name])[:until_rank_n]\n",
    "        if len(r_dict[year])<until_rank_n:\n",
    "            for i in range(0, until_rank_n - len(r_dict[year])):\n",
    "                r_dict[year].append(\"\")\n",
    "    return pd.DataFrame(r_dict)\n",
    "\"\"\"\n",
    "desc: df로부터 그래프를 생성하는 함수\n",
    "input: pd.DataFrame\n",
    "output: nx.Graph\n",
    "\"\"\"\n",
    "def make_graph(input_df, column_name='Author Keywords'):\n",
    "    # make edges: edge가 중복으로 생기지 않게 하려면, \n",
    "    def make_edges_from_lst(lst):\n",
    "        if len(lst)>1:\n",
    "            return [(lst[i], lst[j]) for i in range(0, len(lst)-1) for j in range(i+1, len(lst))]\n",
    "        else:\n",
    "            return []\n",
    "    nodes = total_count(input_df)\n",
    "    new_nodes = []\n",
    "    for i in range(0, len(nodes)):\n",
    "        name = nodes[column_name].iloc()[i]\n",
    "        w = nodes['count'].iloc()[i]\n",
    "        new_nodes.append( (name, {'weight':w}) )\n",
    "    nodes = new_nodes\n",
    "    edges = itertools.chain.from_iterable(input_df[column_name].apply(make_edges_from_lst))\n",
    "    edges = ((uv[0], uv[1], w) for uv, w in Counter(edges).most_common())\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_weighted_edges_from(edges)\n",
    "    # graph에 대한 데이터 필터링이 필요할 수 있는데. 여기서. \n",
    "    return G\n",
    "\"\"\"\n",
    "일종의 main 함수입니다. \n",
    "raw_df를 넘기는데, 가능하면 해당 argument에서 복사해서 넘겨주는 게 좋을 것 같습니다. 혹시나 싶어서요. \n",
    "\"\"\"\n",
    "def scopus_analysis(raw_df, outExcelname, outPPTname):\n",
    "    r_df = raw_df.dropna()\n",
    "    print(\"drop {} row\".format(len(raw_df) - len(r_df)))\n",
    "    r_df['Author Keywords'] = r_df['Author Keywords'].apply(lambda s: s.strip().split(\";\"))\n",
    "    r_df['Noun Phrases'] = r_df['Abstract'].apply(lambda s: TextBlob(s).noun_phrases)\n",
    "    print(r_df.head())\n",
    "    r_df = filtering_auth_kwds(r_df, 'Author Keywords', 1)\n",
    "    r_df = filtering_auth_kwds(r_df, 'Noun Phrases', 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    여기서부터는 엑셀에 시트별로 내용을 넣어주는 부분 \n",
    "    \"\"\"\n",
    "    writer = pd.ExcelWriter(outExcelname)\n",
    "    total_count(r_df, column_name='Author Keywords').to_excel(writer, '01. 전체 저자 키워드 빈도 상위 키워드')\n",
    "    total_count(r_df, column_name='Noun Phrases').to_excel(writer, '02. 전체 noun phrase 빈도 상위')\n",
    "    yearly_count_rank(r_df, column_name='Author Keywords').to_excel(writer, '03. 연도별 저자 키워드 순위 변화')\n",
    "    yearly_count_rank(r_df, column_name='Noun Phrases').to_excel(writer, '04. 연도별 noun phrase 순위 변화')\n",
    "    print(\"빈도 시트 완료\")\n",
    "    \n",
    "    kwdG = make_graph(r_df, 'Author Keywords')\n",
    "    def make_centrality_df(inputG, cent_func):\n",
    "        deg_cent = cent_func(inputG)\n",
    "        deg_cent = sorted([(k, v) for k, v in deg_cent.items()], key=lambda x: x[1], reverse=True)\n",
    "        return pd.DataFrame(deg_cent, columns=['kwd', 'centrality'])\n",
    "    make_centrality_df(kwdG, return_weighted_degree_centrality).to_excel(writer, '05. 키워드 전체 네트워크 w.deg cent')\n",
    "    make_centrality_df(kwdG, return_closeness_centrality).to_excel(writer, '06. 키워드 전체 네트워크 close cent')\n",
    "    make_centrality_df(kwdG, return_betweenness_centrality).to_excel(writer, '07. 키워드 전체 네트워크 bet. cent')\n",
    "        \n",
    "    writer.save()\n",
    "    \n",
    "\n",
    "excel_path_and_filename = \"../../../Downloads/SMEs_Scopus_2013-2017.xlsx\"\n",
    "df = pd.read_excel(excel_path_and_filename)\n",
    "df = df[['Author Keywords', 'Year', 'Abstract']]\n",
    "\n",
    "scopus_analysis(df.iloc()[:200], 'simple_report_for_SME.xlsx', 'simple_figure_for_SME.pptx')\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
