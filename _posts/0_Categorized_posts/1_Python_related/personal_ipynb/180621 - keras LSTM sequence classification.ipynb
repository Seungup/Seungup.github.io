{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(7)# fix random seed for reproducibility\n",
    "\n",
    "\"\"\"\n",
    "개별 movie review에 있는, 모든 단어를 고려하는 것은 무의미하기 때문에 \n",
    "top_words, 즉 상위 5000개의 단어에 대해서만, 추려냄. 나머지는 필터링\n",
    "그리고 단어는 index로 표시됨 .\n",
    "\"\"\"\n",
    "max_review_length, top_words = 100, 500 # 원래 500, 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\"\"\"\n",
    "sequence의 길이를 똑같이 맞춤. \n",
    "길이가 500보다 큰 경우에는 그냥 일괄적으로 앞부분을 잘라내버림. \n",
    "\"\"\"\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print(\"reading data complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 100, 32)           16000     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 39,053\n",
      "Trainable params: 39,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 171s - loss: 0.5373 - acc: 0.7154 - val_loss: 0.4850 - val_acc: 0.7600\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 163s - loss: 0.4209 - acc: 0.8074 - val_loss: 0.4257 - val_acc: 0.8086\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 174s - loss: 0.4004 - acc: 0.8214 - val_loss: 0.4004 - val_acc: 0.8158\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 165s - loss: 0.3826 - acc: 0.8292 - val_loss: 0.3879 - val_acc: 0.8248\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 173s - loss: 0.3687 - acc: 0.8381 - val_loss: 0.3833 - val_acc: 0.8255\n",
      "training complete\n",
      "Accuracy: 82.55\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "layer의 구성은 \n",
    "- Embedding: 단어를 벡터화하고, 이 결과 값을 LSTM에 집어넣어줌. \n",
    "    - input_dim에 top_words를 넣어주는데, 아마도 내부에서 자동으로 one-hot vector를 만들어주는 것 같음\n",
    "    - 현재는 one-hot vector가 아니라, 0, 1, 등 word vocab의 index가 넘어감. \n",
    "- Conv1D: 구조적인 특성을 파악하기 위해 여러 filter로 찍어줌.\n",
    "- MaxPooling1D: convolution으로 찍어낸 정보를 좀 더 특징화함. \n",
    "- LSTM: sequential한 정보를 활용\n",
    "- Dense: classification이므로 output layer을 1칸짜리로 넣어줌. \n",
    "\"\"\"\n",
    "embedding_vector_length = 32\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=top_words, # 5000\n",
    "              output_dim=embedding_vector_length, # 32\n",
    "              input_length=max_review_length), \n",
    "    Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'), \n",
    "    MaxPooling1D(pool_size=2),\n",
    "    LSTM(50), # 원래는 100, \n",
    "    Dropout(0.2), \n",
    "    Dense(25, activation='sigmoid'), \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
    "print(\"training complete\")\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: {:.2f}\".format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.43\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
